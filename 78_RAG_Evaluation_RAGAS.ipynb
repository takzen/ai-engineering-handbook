{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a859376d",
   "metadata": {},
   "source": [
    "# ‚öñÔ∏è RAG Evaluation: Sƒôdzia AI (LLM-as-a-Judge)\n",
    "\n",
    "W klasycznym ML mamy `Accuracy` (czy 0=0?). W generatywnym AI odpowied≈∫ jest tekstem. Nie da siƒô jej por√≥wnaƒá \"znak po znaku\".\n",
    "\n",
    "**Paradygmat LLM-as-a-Judge:**\n",
    "U≈ºywamy jednego modelu AI (Sƒôdziego), ≈ºeby ocenia≈Ç drugi model (Studenta).\n",
    "\n",
    "Zbudujemy system oceny **Faithfulness (Wierno≈õci)**:\n",
    "1.  **Krok 1:** WyciƒÖgnij z odpowiedzi Studenta wszystkie twierdzenia (fakty).\n",
    "2.  **Krok 2:** Sprawd≈∫ ka≈ºdy fakt w Kontek≈õcie (≈∫r√≥d≈Çach).\n",
    "3.  **Krok 3:** Je≈õli fakt nie ma pokrycia w ≈∫r√≥d≈Çach -> **Halucynacja**.\n",
    "4.  **Wynik:** Liczba fakt√≥w potwierdzonych / Liczba wszystkich fakt√≥w.\n",
    "\n",
    "To jest dok≈Çadnie to, co robi biblioteka **RAGAS** pod maskƒÖ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a936f343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LOGI Z RAG ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Case_A_Good</td>\n",
       "      <td>Jaka jest stolica Francji?</td>\n",
       "      <td>Francja to kraj w Europie. Jej stolicƒÖ jest Pa...</td>\n",
       "      <td>StolicƒÖ Francji jest Pary≈º.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Case_B_Hallucination</td>\n",
       "      <td>Kto wygra≈Ç mundial w 2022?</td>\n",
       "      <td>Mundial 2022 odby≈Ç siƒô w Katarze. Bra≈Ça w nim ...</td>\n",
       "      <td>Mundial w 2022 wygra≈Ça Argentyna.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Case_C_Irrelevant</td>\n",
       "      <td>Jak zresetowaƒá has≈Ço?</td>\n",
       "      <td>Aby zresetowaƒá has≈Ço, kliknij w link w mailu.</td>\n",
       "      <td>Has≈Ça sƒÖ wa≈ºne dla bezpiecze≈Ñstwa.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                    question  \\\n",
       "0           Case_A_Good  Jaka jest stolica Francji?   \n",
       "1  Case_B_Hallucination  Kto wygra≈Ç mundial w 2022?   \n",
       "2     Case_C_Irrelevant       Jak zresetowaƒá has≈Ço?   \n",
       "\n",
       "                                             context  \\\n",
       "0  Francja to kraj w Europie. Jej stolicƒÖ jest Pa...   \n",
       "1  Mundial 2022 odby≈Ç siƒô w Katarze. Bra≈Ça w nim ...   \n",
       "2      Aby zresetowaƒá has≈Ço, kliknij w link w mailu.   \n",
       "\n",
       "                               answer  \n",
       "0         StolicƒÖ Francji jest Pary≈º.  \n",
       "1   Mundial w 2022 wygra≈Ça Argentyna.  \n",
       "2  Has≈Ça sƒÖ wa≈ºne dla bezpiecze≈Ñstwa.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 1. DANE DO OCENY (Symulacja log√≥w z Twojego RAG-a)\n",
    "# Mamy 3 przypadki:\n",
    "# A: Idealna odpowied≈∫.\n",
    "# B: Halucynacja (zmy≈õla fakty spoza kontekstu).\n",
    "# C: Nie na temat (Unikanie odpowiedzi).\n",
    "\n",
    "rag_logs = [\n",
    "    {\n",
    "        \"id\": \"Case_A_Good\",\n",
    "        \"question\": \"Jaka jest stolica Francji?\",\n",
    "        \"context\": \"Francja to kraj w Europie. Jej stolicƒÖ jest Pary≈º.\",\n",
    "        \"answer\": \"StolicƒÖ Francji jest Pary≈º.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"Case_B_Hallucination\",\n",
    "        \"question\": \"Kto wygra≈Ç mundial w 2022?\",\n",
    "        \"context\": \"Mundial 2022 odby≈Ç siƒô w Katarze. Bra≈Ça w nim udzia≈Ç Polska.\",\n",
    "        \"answer\": \"Mundial w 2022 wygra≈Ça Argentyna.\" \n",
    "        # To prawda w ≈ºyciu, ale FA≈ÅSZ w kontek≈õcie RAG! \n",
    "        # Model u≈ºy≈Ç wiedzy z g≈Çowy, a nie z dokumentu. To b≈ÇƒÖd Faithfulness.\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"Case_C_Irrelevant\",\n",
    "        \"question\": \"Jak zresetowaƒá has≈Ço?\",\n",
    "        \"context\": \"Aby zresetowaƒá has≈Ço, kliknij w link w mailu.\",\n",
    "        \"answer\": \"Has≈Ça sƒÖ wa≈ºne dla bezpiecze≈Ñstwa.\"\n",
    "        # Prawda, ale nie odpowiada na pytanie. B≈ÇƒÖd Relevance.\n",
    "    }\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(rag_logs)\n",
    "print(\"--- LOGI Z RAG ---\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78844a2",
   "metadata": {},
   "source": [
    "## Budowa Sƒôdziego (The Judge)\n",
    "\n",
    "Nie bƒôdziemy tu podinaƒá p≈Çatnego API OpenAI.\n",
    "Zapiszemy **Prompty Sƒôdziowskie**, kt√≥re normalnie wys≈Ça≈Çby≈õ do GPT-4.\n",
    "Zasymulujemy dzia≈Çanie Sƒôdziego, ≈ºeby≈õ zrozumia≈Ç algorytm oceniania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b69e1f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sƒôdzia gotowy. Mo≈ºemy generowaƒá instrukcje dla GPT-4.\n"
     ]
    }
   ],
   "source": [
    "class RAGEvaluator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def create_faithfulness_prompt(self, context, answer):\n",
    "        \"\"\"\n",
    "        Prompt sprawdzajƒÖcy, czy model nie k≈Çamie.\n",
    "        \"\"\"\n",
    "        return f\"\"\"\n",
    "        Jeste≈õ SƒôdziƒÖ weryfikujƒÖcym fakty.\n",
    "        \n",
    "        ZADANIE:\n",
    "        1. Rozbij 'Odpowied≈∫' na listƒô pojedynczych twierdze≈Ñ.\n",
    "        2. Dla ka≈ºdego twierdzenia sprawd≈∫, czy wynika ono logicznie z 'Kontekstu'.\n",
    "        3. Je≈õli wynika -> TAK. Je≈õli nie ma go w kontek≈õcie -> NIE.\n",
    "        \n",
    "        Kontekst: \"{context}\"\n",
    "        Odpowied≈∫: \"{answer}\"\n",
    "        \n",
    "        Format wyj≈õciowy (JSON):\n",
    "        {{\n",
    "            \"statements\": [\"twierdzenie 1\", \"twierdzenie 2\"],\n",
    "            \"verdict\": [\"TAK\", \"NIE\"],\n",
    "            \"score\": (liczba TAK / wszystkie)\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "    def create_relevance_prompt(self, question, answer):\n",
    "        \"\"\"\n",
    "        Prompt sprawdzajƒÖcy, czy model odpowiedzia≈Ç na temat.\n",
    "        \"\"\"\n",
    "        return f\"\"\"\n",
    "        Jeste≈õ SƒôdziƒÖ oceniajƒÖcym jako≈õƒá rozmowy.\n",
    "        \n",
    "        ZADANIE:\n",
    "        Oce≈Ñ, czy 'Odpowied≈∫' jest u≈ºyteczna i bezpo≈õrednio odnosi siƒô do 'Pytania'.\n",
    "        Ignoruj poprawno≈õƒá fakt√≥w (to sprawdzamy gdzie indziej). Skup siƒô na intencji.\n",
    "        \n",
    "        Pytanie: \"{question}\"\n",
    "        Odpowied≈∫: \"{answer}\"\n",
    "        \n",
    "        Wynik: 0 (Kompletnie nie na temat) do 1 (Idealna odpowied≈∫).\n",
    "        \"\"\"\n",
    "\n",
    "evaluator = RAGEvaluator()\n",
    "print(\"Sƒôdzia gotowy. Mo≈ºemy generowaƒá instrukcje dla GPT-4.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958fe7df",
   "metadata": {},
   "source": [
    "## Symulacja Oceny (Case B: Halucynacja)\n",
    "\n",
    "Sp√≥jrzmy na przypadek B:\n",
    "*   Context: \"Mundial by≈Ç w Katarze.\"\n",
    "*   Answer: \"Wygra≈Ça Argentyna.\"\n",
    "\n",
    "Dla cz≈Çowieka to prawda.\n",
    "Dla systemu RAG to **B≈ÇƒÖd Krytyczny**. Je≈õli w dokumentach firmowych nie ma info o Argentynie, model nie ma prawa o tym pisaƒá. To siƒô nazywa **Data Leakage from Pre-training**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad4908b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ANALIZA PRZYPADKU: Case_B_Hallucination ---\n",
      "Pytanie: Kto wygra≈Ç mundial w 2022?\n",
      "Kontekst: Mundial 2022 odby≈Ç siƒô w Katarze. Bra≈Ça w nim udzia≈Ç Polska.\n",
      "Odpowied≈∫ Modelu: Mundial w 2022 wygra≈Ça Argentyna.\n",
      "\n",
      "--- CO WIDZI SƒòDZIA (PROMPT) ---\n",
      "\n",
      "        Jeste≈õ SƒôdziƒÖ weryfikujƒÖcym fakty.\n",
      "\n",
      "        ZADANIE:\n",
      "        1. Rozbij 'Odpowied≈∫' na listƒô pojedynczych twierdze≈Ñ.\n",
      "        2. Dla ka≈ºdego twierdzenia sprawd≈∫, czy wynika ono logicznie z 'Kontekstu'.\n",
      "        3. Je≈õli wynika -> TAK. Je≈õli nie ma go w kontek≈õcie -> NIE.\n",
      "\n",
      "        Kontekst: \"Mundial 2022 odby≈Ç siƒô w Katarze. Bra≈Ça w nim udzia≈Ç Polska.\"\n",
      "        Odpowied≈∫: \"Mundial w 2022 wygra≈Ça Argentyna.\"\n",
      "\n",
      "        Format wyj≈õciowy (JSON):\n",
      "        {\n",
      "            \"statements\": [\"twierdzenie 1\", \"twierdzenie 2\"],\n",
      "            \"verdict\": [\"TAK\", \"NIE\"],\n",
      "            \"score\": (liczba TAK / wszystkie)\n",
      "        }\n",
      "        \n",
      "\n",
      "--- SYMULOWANA ODPOWIED≈π SƒòDZIEGO (GPT-4) ---\n",
      "{\n",
      "  \"statements\": [\n",
      "    \"Mundial w 2022 wygra≈Ça Argentyna\"\n",
      "  ],\n",
      "  \"verdict\": [\n",
      "    \"NIE\"\n",
      "  ],\n",
      "  \"reasoning\": \"W podanym kontek≈õcie nie ma informacji o zwyciƒôzcy. Jest tylko info o lokalizacji.\",\n",
      "  \"score\": 0.0\n",
      "}\n",
      "------------------------------\n",
      "üö® FAITHFULNESS FAIL: Model u≈ºy≈Ç wiedzy spoza dokumentu!\n"
     ]
    }
   ],
   "source": [
    "case = rag_logs[1] # Case B\n",
    "\n",
    "print(f\"--- ANALIZA PRZYPADKU: {case['id']} ---\")\n",
    "print(f\"Pytanie: {case['question']}\")\n",
    "print(f\"Kontekst: {case['context']}\")\n",
    "print(f\"Odpowied≈∫ Modelu: {case['answer']}\")\n",
    "\n",
    "print(\"\\n--- CO WIDZI SƒòDZIA (PROMPT) ---\")\n",
    "prompt = evaluator.create_faithfulness_prompt(case['context'], case['answer'])\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n--- SYMULOWANA ODPOWIED≈π SƒòDZIEGO (GPT-4) ---\")\n",
    "# To jest to, co zwr√≥ci≈Çby model:\n",
    "simulated_verdict = {\n",
    "    \"statements\": [\"Mundial w 2022 wygra≈Ça Argentyna\"],\n",
    "    \"verdict\": [\"NIE\"],\n",
    "    \"reasoning\": \"W podanym kontek≈õcie nie ma informacji o zwyciƒôzcy. Jest tylko info o lokalizacji.\",\n",
    "    \"score\": 0.0\n",
    "}\n",
    "print(json.dumps(simulated_verdict, indent=2, ensure_ascii=False))\n",
    "\n",
    "print(\"-\" * 30)\n",
    "if simulated_verdict['score'] < 1.0:\n",
    "    print(\"üö® FAITHFULNESS FAIL: Model u≈ºy≈Ç wiedzy spoza dokumentu!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c82ee12",
   "metadata": {},
   "source": [
    "## Symulacja Oceny (Case C: Relevance)\n",
    "\n",
    "Sp√≥jrzmy na przypadek C:\n",
    "*   Pytanie: \"Jak zresetowaƒá has≈Ço?\"\n",
    "*   Odpowied≈∫: \"Has≈Ça sƒÖ wa≈ºne.\"\n",
    "\n",
    "Model powiedzia≈Ç prawdƒô, opar≈Ç siƒô na swojej wiedzy og√≥lnej, ale... nie pom√≥g≈Ç u≈ºytkownikowi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d1a061d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ANALIZA PRZYPADKU: Case_C_Irrelevant ---\n",
      "\n",
      "        Jeste≈õ SƒôdziƒÖ oceniajƒÖcym jako≈õƒá rozmowy.\n",
      "\n",
      "        ZADANIE:\n",
      "        Oce≈Ñ, czy 'Odpowied≈∫' jest u≈ºyteczna i bezpo≈õrednio odnosi siƒô do 'Pytania'.\n",
      "        Ignoruj poprawno≈õƒá fakt√≥w (to sprawdzamy gdzie indziej). Skup siƒô na intencji.\n",
      "\n",
      "        Pytanie: \"Jak zresetowaƒá has≈Ço?\"\n",
      "        Odpowied≈∫: \"Has≈Ça sƒÖ wa≈ºne dla bezpiecze≈Ñstwa.\"\n",
      "\n",
      "        Wynik: 0 (Kompletnie nie na temat) do 1 (Idealna odpowied≈∫).\n",
      "        \n",
      "\n",
      "--- SYMULOWANA ODPOWIED≈π SƒòDZIEGO ---\n",
      "{\n",
      "  \"score\": 0.2,\n",
      "  \"reasoning\": \"U≈ºytkownik pyta≈Ç o instrukcjƒô (JAK), model poda≈Ç opiniƒô/fakt og√≥lny. Nie rozwiƒÖzuje problemu.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "case = rag_logs[2] # Case C\n",
    "\n",
    "print(f\"--- ANALIZA PRZYPADKU: {case['id']} ---\")\n",
    "prompt = evaluator.create_relevance_prompt(case['question'], case['answer'])\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n--- SYMULOWANA ODPOWIED≈π SƒòDZIEGO ---\")\n",
    "simulated_verdict_relevance = {\n",
    "    \"score\": 0.2,\n",
    "    \"reasoning\": \"U≈ºytkownik pyta≈Ç o instrukcjƒô (JAK), model poda≈Ç opiniƒô/fakt og√≥lny. Nie rozwiƒÖzuje problemu.\"\n",
    "}\n",
    "print(json.dumps(simulated_verdict_relevance, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aad98e",
   "metadata": {},
   "source": [
    "## üß† Podsumowanie: Trust but Verify\n",
    "\n",
    "W in≈ºynierii LLM nie ufamy modelom na s≈Çowo. Budujemy **Testy Jednostkowe** oparte na innym LLM.\n",
    "\n",
    "**Jak to wdro≈ºyƒá w firmie?**\n",
    "1.  Zbierasz 50 par (Pytanie, Idealna Odpowied≈∫) ‚Äì tzw. **Golden Dataset**.\n",
    "2.  Zmieniasz kod swojego RAG-a (np. zmieniasz Chunk Size).\n",
    "3.  Puszczasz te 50 pyta≈Ñ przez nowy system.\n",
    "4.  U≈ºywasz GPT-4 jako Sƒôdziego do oceny Faithfulness i Relevance.\n",
    "5.  Je≈õli ≈õredni wynik spad≈Ç ‚Äì nie wdra≈ºasz zmian.\n",
    "\n",
    "Bez tego krƒôcisz siƒô w k√≥≈Çko (\"Wydaje mi siƒô, ≈ºe teraz dzia≈Ça lepiej\")."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-handbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
