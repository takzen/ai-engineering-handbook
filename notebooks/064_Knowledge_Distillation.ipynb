{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/takzen/ai-engineering-handbook/blob/main/notebooks/064_Knowledge_Distillation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939e6822",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/takzen/ai-engineering-handbook/blob/main/64_Knowledge_Distillation.ipynb\" target=\"_parent\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aebb050",
   "metadata": {},
   "source": [
    "# \ud83e\uddea Knowledge Distillation: Nauczyciel i Ucze\u0144\n",
    "\n",
    "Jak zmie\u015bci\u0107 inteligencj\u0119 GPT-4 w ma\u0142ym modelu na telefon?\n",
    "Nie wystarczy zmniejszy\u0107 sieci (kwantyzacja). Trzeba j\u0105 \"nauczy\u0107 sprytu\".\n",
    "\n",
    "**Ideologia:**\n",
    "Du\u017cy model (Teacher) widzi wi\u0119cej ni\u017c tylko wynik.\n",
    "Patrz\u0105c na zdj\u0119cie psa, Teacher my\u015bli: *\"To jest Pies na 90%, ale na 9% wygl\u0105da jak Kot (bo ma uszy), a na 1% jak Auto\"*.\n",
    "Zwyk\u0142a etykieta m\u00f3wi tylko: *\"To jest Pies\"*.\n",
    "\n",
    "Ucz\u0105c ma\u0142y model (Student) tylko z etykiet, tracimy informacj\u0119 o tym podobie\u0144stwie do Kota.\n",
    "Dlatego ka\u017cemy Studentowi na\u015bladowa\u0107 **Logity Nauczyciela** (zmi\u0119kczone Temperatur\u0105).\n",
    "\n",
    "Wz\u00f3r na strat\u0119 (Loss):\n",
    "$$ Loss = \\alpha \\cdot L_{soft}(Student, Teacher) + (1-\\alpha) \\cdot L_{hard}(Student, Labels) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "790bf892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urz\u0105dzenie: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "# Konfiguracja\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Dane (MNIST)\n",
    "train_loader = DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "    batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=1000, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Urz\u0105dzenie: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0439081",
   "metadata": {},
   "source": [
    "## Definicja Modeli\n",
    "\n",
    "1.  **Teacher:** Du\u017ca, g\u0142\u0119boka sie\u0107 (du\u017co neuron\u00f3w, Dropout). Powinna osi\u0105gn\u0105\u0107 \u015bwietny wynik.\n",
    "2.  **Student:** Malutka sie\u0107. Ma ma\u0142o parametr\u00f3w, wi\u0119c sama z siebie uczy si\u0119 s\u0142abo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63c599ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher parameters: 2,395,210\n",
      "Student parameters: 15,910\n",
      "Student jest 150.5x mniejszy!\n"
     ]
    }
   ],
   "source": [
    "class TeacherNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 1200)\n",
    "        self.fc2 = nn.Linear(1200, 1200)\n",
    "        self.fc3 = nn.Linear(1200, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.dropout(self.fc1(x)))\n",
    "        x = F.relu(self.dropout(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class StudentNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Student jest du\u017co mniejszy! (Mniej warstw, mniej neuron\u00f3w)\n",
    "        self.fc1 = nn.Linear(784, 20) \n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Inicjalizacja\n",
    "teacher = TeacherNet().to(DEVICE)\n",
    "student = StudentNet().to(DEVICE)\n",
    "\n",
    "# Policzmy parametry\n",
    "teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "student_params = sum(p.numel() for p in student.parameters())\n",
    "\n",
    "print(f\"Teacher parameters: {teacher_params:,}\")\n",
    "print(f\"Student parameters: {student_params:,}\")\n",
    "print(f\"Student jest {teacher_params/student_params:.1f}x mniejszy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043df3ce",
   "metadata": {},
   "source": [
    "## Krok 1: Trening Nauczyciela (Normalny)\n",
    "\n",
    "Najpierw musimy mie\u0107 m\u0105drego nauczyciela. Trenujemy go standardowo (Cross Entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba4d3b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83c\udf93 Trenuj\u0119 Nauczyciela...\n",
      "Nauczyciel Accuracy: 97.51%\n"
     ]
    }
   ],
   "source": [
    "def train(model, optimizer, loss_fn, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    return 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "print(\"\ud83c\udf93 Trenuj\u0119 Nauczyciela...\")\n",
    "optimizer_t = optim.Adam(teacher.parameters(), lr=0.001)\n",
    "train(teacher, optimizer_t, nn.CrossEntropyLoss(), epochs=3)\n",
    "acc_teacher = evaluate(teacher)\n",
    "print(f\"Nauczyciel Accuracy: {acc_teacher:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2f33e4",
   "metadata": {},
   "source": [
    "## Krok 2: Destylacja (Loss Function)\n",
    "\n",
    "To jest serce algorytmu.\n",
    "Funkcja kosztu sk\u0142ada si\u0119 z dw\u00f3ch cz\u0119\u015bci:\n",
    "1.  **Soft Loss:** Por\u00f3wnujemy *zmi\u0119kczone* wyniki Studenta ze *zmi\u0119kczonymi* wynikami Nauczyciela (KL Divergence).\n",
    "    *   U\u017cywamy `Temperature` (T), \u017ceby \"rozmy\u0107\" pewno\u015b\u0107 nauczyciela i pokaza\u0107 niuanse.\n",
    "2.  **Hard Loss:** Por\u00f3wnujemy wynik Studenta z prawdziw\u0105 etykiet\u0105 (Cross Entropy).\n",
    "\n",
    "Parametr `alpha` (np. 0.7) decyduje, jak bardzo Student ma s\u0142ucha\u0107 Nauczyciela, a jak bardzo patrze\u0107 na prawd\u0119."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87286885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gotowi do lekcji.\n"
     ]
    }
   ],
   "source": [
    "def distillation_loss(student_logits, teacher_logits, labels, T=5.0, alpha=0.7):\n",
    "    # 1. Hard Loss (Student vs Prawda)\n",
    "    hard_loss = F.cross_entropy(student_logits, labels)\n",
    "    \n",
    "    # 2. Soft Loss (Student vs Teacher)\n",
    "    # KL Divergence oczekuje log-probabilities na wej\u015bciu i probabilities na celu\n",
    "    soft_loss = nn.KLDivLoss(reduction=\"batchmean\")(\n",
    "        F.log_softmax(student_logits / T, dim=1),\n",
    "        F.softmax(teacher_logits / T, dim=1)\n",
    "    )\n",
    "    \n",
    "    # Wa\u017cne: Mno\u017cymy soft_loss przez T^2, \u017ceby wyr\u00f3wna\u0107 gradienty\n",
    "    return alpha * (soft_loss * (T * T)) + (1. - alpha) * hard_loss\n",
    "\n",
    "# Funkcja treningowa z destylacj\u0105\n",
    "def train_distill(student, teacher, optimizer, epochs=3, T=5.0, alpha=0.7):\n",
    "    student.train()\n",
    "    teacher.eval() # Nauczyciel si\u0119 nie uczy! Tylko podpowiada.\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass obu modeli\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(data)\n",
    "            student_logits = student(data)\n",
    "            \n",
    "            # Specjalny Loss\n",
    "            loss = distillation_loss(student_logits, teacher_logits, target, T, alpha)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "print(\"Gotowi do lekcji.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12fc7c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udc76 Student uczy si\u0119 sam...\n",
      "Czas: 16.1s\n",
      "\n",
      "\ud83d\udc68\u200d\ud83c\udfeb Student uczy si\u0119 z Nauczycielem...\n",
      "Czas: 17.2s\n",
      "------------------------------\n",
      "Teacher Accuracy:   97.51%\n",
      "Student (Sam):      93.81%\n",
      "Student (Distilled): 91.88%\n"
     ]
    }
   ],
   "source": [
    "# Eksperyment A: Student uczy si\u0119 sam (od zera, bez pomocy)\n",
    "student_alone = StudentNet().to(DEVICE)\n",
    "optimizer_s = optim.Adam(student_alone.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\ud83d\udc76 Student uczy si\u0119 sam...\")\n",
    "start = time.time()\n",
    "train(student_alone, optimizer_s, nn.CrossEntropyLoss(), epochs=3)\n",
    "print(f\"Czas: {time.time()-start:.1f}s\")\n",
    "acc_alone = evaluate(student_alone)\n",
    "\n",
    "\n",
    "# Eksperyment B: Student uczy si\u0119 od Nauczyciela (Destylacja)\n",
    "# Resetujemy studenta\n",
    "student_distilled = StudentNet().to(DEVICE)\n",
    "optimizer_d = optim.Adam(student_distilled.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\n\ud83d\udc68\u200d\ud83c\udfeb Student uczy si\u0119 z Nauczycielem...\")\n",
    "start = time.time()\n",
    "train_distill(student_distilled, teacher, optimizer_d, epochs=3, T=7.0, alpha=0.8)\n",
    "print(f\"Czas: {time.time()-start:.1f}s\")\n",
    "acc_distilled = evaluate(student_distilled)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Teacher Accuracy:   {acc_teacher:.2f}%\")\n",
    "print(f\"Student (Sam):      {acc_alone:.2f}%\")\n",
    "print(f\"Student (Distilled): {acc_distilled:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4d9925",
   "metadata": {},
   "source": [
    "## \ud83e\udde0 Podsumowanie: Dark Knowledge\n",
    "\n",
    "Powiniene\u015b zobaczy\u0107, \u017ce **Student (Distilled)** ma lepszy wynik ni\u017c **Student (Sam)**, mimo \u017ce maj\u0105 **dok\u0142adnie tak\u0105 sam\u0105, ma\u0142\u0105 architektur\u0119**.\n",
    "\n",
    "Dlaczego?\n",
    "Bo Nauczyciel przekaza\u0142 mu \"Ciemn\u0105 Wiedz\u0119\" (Dark Knowledge).\n",
    "*   Gdy Nauczyciel widzi \"7\", m\u00f3wi: *\"To 7, ale troch\u0119 wygl\u0105da jak 1\"*.\n",
    "*   Student (Sam) widzi tylko: *\"To jest 7\"*. Je\u015bli zobaczy krzyw\u0105 si\u00f3demk\u0119, zg\u0142upieje.\n",
    "*   Student (Distilled) wie: *\"Aha, si\u00f3demki mog\u0105 przypomina\u0107 jedynki\"*. Nauczy\u0142 si\u0119 **relacji mi\u0119dzy klasami**.\n",
    "\n",
    "To technika obowi\u0105zkowa przy wdra\u017caniu modeli na urz\u0105dzenia mobilne (Edge AI)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-handbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}