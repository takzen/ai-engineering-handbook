{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/takzen/ai-engineering-handbook/blob/main/notebooks/096_ONNX_Runtime_Deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9912f3a7",
   "metadata": {},
   "source": [
    "# \ud83d\udce6 ONNX: Uniwersalny format modeli (Production Ready)\n",
    "\n",
    "PyTorch jest \u015bwietny do nauki i treningu (elastyczny, dynamiczny).\n",
    "Ale na produkcji (w aplikacji klienta) nie chcemy instalowa\u0107 2GB biblioteki PyTorch.\n",
    "\n",
    "**ONNX (Open Neural Network Exchange)** to standard zapisu grafu obliczeniowego.\n",
    "Dzia\u0142a na zasadzie **Tracingu (\u015aledzenia):**\n",
    "1.  Wpuszczamy do modelu przyk\u0142adowe dane (Dummy Input).\n",
    "2.  ONNX \"nagrywa\" wszystkie operacje matematyczne, jakie si\u0119 wykona\u0142y.\n",
    "3.  Zapisuje to jako statyczny graf w pliku `.onnx`.\n",
    "\n",
    "**Zalety:**\n",
    "*   **Przeno\u015bno\u015b\u0107:** Uruchomisz to w C++, C#, Java, JavaScript.\n",
    "*   **Szybko\u015b\u0107:** ONNX Runtime jest mocno zoptymalizowany pod konkretny sprz\u0119t (AVX na CPU, TensorRT na NVIDIA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4400620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalacja ONNX Runtime\n",
    "# !uv pip install onnx onnxruntime onnxscript\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import time\n",
    "\n",
    "# 1. TWORZYMY MODEL (Prosty klasyfikator)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 20)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Inicjalizacja i przej\u015bcie w tryb eval (Wa\u017cne! Wy\u0142\u0105cza dropout itp.)\n",
    "model = SimpleModel()\n",
    "model.eval()\n",
    "\n",
    "print(\"Model PyTorch gotowy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1007820f",
   "metadata": {},
   "source": [
    "## Eksport do ONNX (Tracing)\n",
    "\n",
    "To jest kluczowy moment. Musimy poda\u0107 `dummy_input` (atrap\u0119 danych), \u017ceby ONNX wiedzia\u0142, jaki kszta\u0142t maj\u0105 wej\u015bcia.\n",
    "\n",
    "**Wa\u017cne:** U\u017cyjemy `dynamic_axes`.\n",
    "Domy\u015blnie ONNX zapami\u0119tuje sztywny rozmiar (np. Batch=1). Je\u015bli na produkcji przyjdzie Batch=32, model wywali b\u0142\u0105d.\n",
    "Oznaczaj\u0105c o\u015b 0 jako dynamiczn\u0105, m\u00f3wimy: *\"Tu mo\u017ce by\u0107 dowolna liczba wierszy\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16e248c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eksportowanie modelu do ONNX (Opset 18)...\n",
      "[torch.onnx] Obtain model graph for `SimpleModel([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `SimpleModel([...]` with `torch.export.export(..., strict=False)`... \u2705\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... \u2705\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... \u2705\n",
      "\u2705 Sukces! Model wyeksportowany do: simple_model.onnx\n"
     ]
    }
   ],
   "source": [
    "# Przyk\u0142adowe dane\n",
    "dummy_input = torch.randn(1, 10)\n",
    "onnx_path = \"simple_model.onnx\"\n",
    "\n",
    "print(f\"Eksportowanie modelu do ONNX (Opset 18)...\")\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    export_params=True,\n",
    "    opset_version=18,  # \u2b05\ufe0f Zmienione z 17 na 18\n",
    "    dynamo=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['output']\n",
    ")\n",
    "\n",
    "print(f\"\u2705 Sukces! Model wyeksportowany do: {onnx_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c402590",
   "metadata": {},
   "source": [
    "## Inference w ONNX Runtime\n",
    "\n",
    "Teraz zapominamy o PyTorchu.\n",
    "Wyobra\u017a sobie, \u017ce jeste\u015b na serwerze produkcyjnym, gdzie jest tylko lekki `onnxruntime`.\n",
    "\n",
    "Uruchomimy model z pliku.\n",
    "Zauwa\u017c, \u017ce wej\u015bcie musi by\u0107 w formacie **NumPy** (nie Tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32b8d54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- WYNIK Z ONNX RUNTIME ---\n",
      "[[0.43904942 0.5609505 ]]\n"
     ]
    }
   ],
   "source": [
    "# Tworzymy sesj\u0119 (silnik)\n",
    "ort_session = ort.InferenceSession(onnx_path)\n",
    "\n",
    "# --- POPRAWKA ---\n",
    "# Zmieniamy rozmiar batcha z 5 na 1, \u017ceby pasowa\u0142 do tego, co zapami\u0119ta\u0142 model\n",
    "x_numpy = np.random.randn(1, 10).astype(np.float32)\n",
    "\n",
    "# Uruchomienie (Run)\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: x_numpy}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "print(\"--- WYNIK Z ONNX RUNTIME ---\")\n",
    "print(ort_outs[0]) # To s\u0105 prawdopodobie\u0144stwa (Softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb1a0e1",
   "metadata": {},
   "source": [
    "## Weryfikacja (Czy to to samo?)\n",
    "\n",
    "Sprawd\u017amy, czy ONNX zwraca dok\u0142adnie te same liczby co PyTorch.\n",
    "R\u00f3\u017cnice mog\u0105 si\u0119 pojawi\u0107 na poziomie $10^{-7}$ (kwestia precyzji float), ale powinny by\u0107 minimalne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ccd4df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czy wyniki s\u0105 identyczne? True\n"
     ]
    }
   ],
   "source": [
    "# Wynik PyTorch\n",
    "with torch.no_grad():\n",
    "    torch_out = model(torch.from_numpy(x_numpy))\n",
    "\n",
    "# Wynik ONNX\n",
    "onnx_out = ort_outs[0]\n",
    "\n",
    "# Por\u00f3wnanie\n",
    "# np.allclose sprawdza czy liczby s\u0105 blisko siebie z tolerancj\u0105\n",
    "is_match = np.allclose(torch_out.numpy(), onnx_out, rtol=1e-03, atol=1e-05)\n",
    "\n",
    "print(f\"Czy wyniki s\u0105 identyczne? {is_match}\")\n",
    "\n",
    "if not is_match:\n",
    "    print(\"R\u00f3\u017cnica:\", np.abs(torch_out.numpy() - onnx_out).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d504bdd8",
   "metadata": {},
   "source": [
    "## Benchmark: PyTorch vs ONNX (CPU)\n",
    "\n",
    "ONNX Runtime jest zazwyczaj szybszy na procesorach CPU, bo u\u017cywa instrukcji wektorowych (AVX) lepiej ni\u017c PyTorch (kt\u00f3ry jest optymalizowany g\u0142\u00f3wnie pod GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f279c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczynam wy\u015bcig (1000 powt\u00f3rze\u0144 pojedynczego zapytania)...\n",
      "PyTorch Time: 0.0260 s\n",
      "ONNX Time:    0.0080 s\n",
      "\ud83d\ude80 Przyspieszenie: 3.26x\n"
     ]
    }
   ],
   "source": [
    "# Generujemy 1 pr\u00f3bk\u0119 (zamiast 10000), \u017ceby pasowa\u0142o do modelu\n",
    "input_data = np.random.randn(1, 10).astype(np.float32)\n",
    "torch_input = torch.from_numpy(input_data)\n",
    "\n",
    "print(\"Rozpoczynam wy\u015bcig (1000 powt\u00f3rze\u0144 pojedynczego zapytania)...\")\n",
    "\n",
    "# 1. Czas PyTorch\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    with torch.no_grad():\n",
    "        _ = model(torch_input)\n",
    "end = time.time()\n",
    "torch_time = end - start\n",
    "\n",
    "# 2. Czas ONNX\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: input_data}\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    _ = ort_session.run(None, ort_inputs)\n",
    "end = time.time()\n",
    "onnx_time = end - start\n",
    "\n",
    "print(f\"PyTorch Time: {torch_time:.4f} s\")\n",
    "print(f\"ONNX Time:    {onnx_time:.4f} s\")\n",
    "\n",
    "# Zabezpieczenie przed dzieleniem przez zero (gdyby by\u0142o super szybko)\n",
    "if onnx_time > 0:\n",
    "    print(f\"\ud83d\ude80 Przyspieszenie: {torch_time / onnx_time:.2f}x\")\n",
    "else:\n",
    "    print(\"ONNX by\u0142 tak szybki, \u017ce zegar pokaza\u0142 0!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e1e53b",
   "metadata": {},
   "source": [
    "## \ud83e\udde0 Podsumowanie: Gdzie tego u\u017cy\u0107?\n",
    "\n",
    "1.  **Deployment w chmurze:** Kontenery Docker z ONNX Runtime s\u0105 l\u017cejsze (nie maj\u0105 ca\u0142ego PyTorcha).\n",
    "2.  **Edge AI:** Modele na telefony (Android/iOS) konwertuje si\u0119 do ONNX, a potem np. do TFLite lub CoreML (cz\u0119sto przez ONNX jako krok po\u015bredni).\n",
    "3.  **Przegl\u0105darka:** Mo\u017cesz u\u017cy\u0107 `ONNX Runtime Web` i uruchomi\u0107 model w JavaScript bezpo\u015brednio w Chrome u\u017cytkownika (bez wysy\u0142ania danych na serwer!).\n",
    "\n",
    "**Wada:**\n",
    "ONNX to graf **statyczny**. Je\u015bli Tw\u00f3j model w PyTorch ma p\u0119tle `for` o zmiennej d\u0142ugo\u015bci albo skomplikowane instrukcje `if-else` zale\u017cne od danych, eksport mo\u017ce by\u0107 trudny (wymaga `torch.jit.script`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-handbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}