{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n<a href=\"https://colab.research.google.com/github/takzen/ai-engineering-handbook/blob/main/notebooks/050_UNet_Image_Segmentation.ipynb\" target=\"_parent\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n<a href=\"https://colab.research.google.com/github/takzen/ai-engineering-handbook/blob/main/50_UNet_Image_Segmentation.ipynb\" target=\"_parent\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86828fc",
   "metadata": {},
   "source": [
    "# \ud83e\ude7a U-Net: Precyzyjna Segmentacja Obrazu\n",
    "\n",
    "Klasyfikacja m\u00f3wi: \"Tu jest kot\".\n",
    "Detekcja m\u00f3wi: \"Kot jest w tym pude\u0142ku\".\n",
    "**Segmentacja** m\u00f3wi: \"Te piksele to kot, a tamte to t\u0142o\".\n",
    "\n",
    "Architektura U-Net to standard w medycynie. Sk\u0142ada si\u0119 z:\n",
    "1.  **Zje\u017cd\u017calni w d\u00f3\u0142 (Encoder):** U\u017cywa `Conv2d` i `MaxPool`, \u017ceby zrozumie\u0107 *co jest na zdj\u0119ciu* (ale traci informacj\u0119 o lokalizacji, bo obrazek robi si\u0119 ma\u0142y).\n",
    "2.  **Wspinaczki w g\u00f3r\u0119 (Decoder):** U\u017cywa `ConvTranspose2d` (odwrotny splot), \u017ceby powi\u0119kszy\u0107 obrazek z powrotem do oryginalnego rozmiaru.\n",
    "3.  **Skr\u00f3t\u00f3w (Skip Connections):** Kopiujemy obrazek z lewej strony na praw\u0105. Dzi\u0119ki temu sie\u0107 pami\u0119ta, gdzie by\u0142y kraw\u0119dzie.\n",
    "\n",
    "Zbudujemy t\u0119 architektur\u0119 od zera w PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dabf4a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Podstawowy klocek gotowy.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# 1. BLOK PODSTAWOWY (Double Conv)\n",
    "# W U-Net zawsze robimy dwie konwolucje po sobie.\n",
    "# Conv3x3 -> ReLU -> Conv3x3 -> ReLU\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels), # Stabilizacja\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "print(\"Podstawowy klocek gotowy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16acca1",
   "metadata": {},
   "source": [
    "## Budowa U-Net\n",
    "\n",
    "To jest serce modelu.\n",
    "*   **Downs:** Lista warstw id\u0105cych w d\u00f3\u0142 (zwi\u0119kszamy liczb\u0119 filtr\u00f3w: 64 -> 128 -> 256...).\n",
    "*   **Ups:** Lista warstw id\u0105cych w g\u00f3r\u0119 (zmniejszamy liczb\u0119 filtr\u00f3w: ...256 -> 128 -> 64).\n",
    "*   **Bottleneck:** Najni\u017cszy punkt litery U.\n",
    "\n",
    "Najwa\u017cniejsza linijka w kodzie to `torch.cat`. To ona \u0142\u0105czy (skleja) obrazek z Encodera z obrazkiem z Decodera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a8b9c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architektura U-Net zdefiniowana.\n"
     ]
    }
   ],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 1. Budujemy Zje\u017cd\u017calni\u0119 (Down)\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # 2. Budujemy Wspinaczk\u0119 (Up)\n",
    "        for feature in reversed(features):\n",
    "            # ConvTranspose2d powi\u0119ksza obrazek (2x2 -> 4x4)\n",
    "            self.ups.append(nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2))\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "        # 3. Najni\u017cszy punkt (Bottleneck)\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        \n",
    "        # 4. Ostatnia warstwa (Mapowanie na wynik 1x1)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        # --- W D\u00d3\u0141 ---\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x) # Zapisujemy obrazek na p\u00f3\u017aniej (do skr\u00f3tu)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        # Dno sieci\n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        # Odwracamy list\u0119 skr\u00f3t\u00f3w, \u017ceby bra\u0107 je w dobrej kolejno\u015bci\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        # --- W G\u00d3R\u0118 ---\n",
    "        # Iterujemy co 2 kroki (bo mamy Transpose + DoubleConv)\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x) # Powi\u0119kszamy\n",
    "            \n",
    "            skip_connection = skip_connections[idx//2] # Bierzemy pasuj\u0105cy skr\u00f3t\n",
    "            \n",
    "            # --- MAGIA: Sklejamy (Concatenate) ---\n",
    "            # Je\u015bli wymiary si\u0119 nie zgadzaj\u0105 (np. przez nieparzyste dzielenie), trzeba przyci\u0105\u0107.\n",
    "            # Tutaj dla uproszczenia zak\u0142adamy idealne wymiary (pot\u0119gi 2).\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1) # Sklejamy wzd\u0142u\u017c kana\u0142\u00f3w\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "print(\"Architektura U-Net zdefiniowana.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f8d2e1",
   "metadata": {},
   "source": [
    "## Test Wymiar\u00f3w (Smoke Test)\n",
    "\n",
    "Zanim zaczniemy trenowa\u0107 na prawdziwych zdj\u0119ciach (co trwa godziny), sprawd\u017amy, czy matematyka si\u0119 zgadza.\n",
    "Wrzucimy losowy szum o wymiarach `160x160`.\n",
    "Oczekujemy, \u017ce sie\u0107 zwr\u00f3ci `160x160` (map\u0119 segmentacji). Je\u015bli zwr\u00f3ci inny rozmiar -> mamy b\u0142\u0105d w kodzie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ac9efed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wej\u015bcie: torch.Size([3, 1, 160, 160])\n",
      "Wyj\u015bcie: torch.Size([3, 1, 160, 160])\n",
      "\u2705 SUKCES! Wymiary wej\u015bcia i wyj\u015bcia s\u0105 identyczne.\n"
     ]
    }
   ],
   "source": [
    "def test_unet():\n",
    "    # Losowy obrazek: Batch=3, Kana\u0142y=1 (czarno-bia\u0142y), 160x160 pikseli\n",
    "    x = torch.randn((3, 1, 160, 160))\n",
    "    \n",
    "    # Tworzymy model\n",
    "    model = UNET(in_channels=1, out_channels=1)\n",
    "    \n",
    "    # Przepuszczamy dane\n",
    "    preds = model(x)\n",
    "    \n",
    "    print(f\"Wej\u015bcie: {x.shape}\")\n",
    "    print(f\"Wyj\u015bcie: {preds.shape}\")\n",
    "    \n",
    "    assert preds.shape == x.shape\n",
    "    print(\"\u2705 SUKCES! Wymiary wej\u015bcia i wyj\u015bcia s\u0105 identyczne.\")\n",
    "\n",
    "test_unet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc54751",
   "metadata": {},
   "source": [
    "## \ud83e\udde0 Podsumowanie: Po co te skr\u00f3ty (Skip Connections)?\n",
    "\n",
    "Dlaczego U-Net jest lepszy od zwyk\u0142ego Autoenkodera?\n",
    "\n",
    "**Tu jest haczyk.**\n",
    "Kiedy Autoenkoder zmniejsza zdj\u0119cie do malutkiego wektora (Bottleneck), traci informacj\u0119 o **precyzyjnych kraw\u0119dziach**. Wie, \u017ce na zdj\u0119ciu jest \"p\u0142uco\", ale nie wie dok\u0142adnie, gdzie ko\u0144czy si\u0119 jego granica (piksel 120 czy 121?).\n",
    "\n",
    "**Skip Connections** dzia\u0142aj\u0105 jak kalka techniczna.\n",
    "Decoder pr\u00f3buje narysowa\u0107 p\u0142uco z pami\u0119ci (z Bottlenecku), ale dostaje te\u017c \"\u015bci\u0105g\u0119\" z Encodera \u2013 oryginalny obraz w wysokiej rozdzielczo\u015bci z danej warstwy.\n",
    "\u0141\u0105czy te dwie informacje:\n",
    "*   Kontekst (\"To jest p\u0142uco\") z do\u0142u.\n",
    "*   Lokalizacj\u0119 (\"Kraw\u0119d\u017a jest tutaj\") ze skr\u00f3tu.\n",
    "\n",
    "Dlatego U-Net daje ostre jak brzytwa maski segmentacji."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}