{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n<a href=\"https://colab.research.google.com/github/takzen/ai-engineering-handbook/blob/main/26_RAG_Chunking_Strategies.ipynb\" target=\"_parent\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c214d",
   "metadata": {},
   "source": [
    "# \u2702\ufe0f RAG Chunking: Sztuka krojenia tekstu\n",
    "\n",
    "Kiedy budujesz system RAG, nie wrzucasz ca\u0142ej ksi\u0105\u017cki do bazy wektorowej. Dzielisz j\u0105 na fragmenty.\n",
    "\n",
    "**Dlaczego to trudne?**\n",
    "Wyobra\u017a sobie zdanie: *\"Kluczem do sejfu jest... [CI\u0118CIE] ...1234.\"*\n",
    "*   Chunk 1: \"Kluczem do sejfu jest...\" (AI nie wie co).\n",
    "*   Chunk 2: \"...1234.\" (AI nie wie, do czego to kod).\n",
    "\n",
    "Oba kawa\u0142ki staj\u0105 si\u0119 bezu\u017cyteczne.\n",
    "\n",
    "**Strategie:**\n",
    "1.  **Fixed Size:** Tniemy r\u00f3wno co 100 znak\u00f3w (Ryzykowne).\n",
    "2.  **Sliding Window (Overlap):** Tniemy co 100 znak\u00f3w, ale cofamy si\u0119 o 20. (Tworzymy zak\u0142adk\u0119).\n",
    "3.  **Recursive Split:** Najpierw tniemy akapity. Jak za du\u017ce -> tniemy zdania. Jak za du\u017ce -> tniemy s\u0142owa. (Standard w LangChain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ef104e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D\u0142ugo\u015b\u0107 tekstu: 629 znak\u00f3w.\n"
     ]
    }
   ],
   "source": [
    "# 1. PRZYGOTOWANIE DANYCH\n",
    "# D\u0142ugi tekst (symulacja artyku\u0142u)\n",
    "\n",
    "long_text = \"\"\"\n",
    "ROZDZIA\u0141 1: WST\u0118P DO AI\n",
    "Sztuczna inteligencja zmienia \u015bwiat w tempie wyk\u0142adniczym. Modele j\u0119zykowe, takie jak GPT-4, potrafi\u0105 pisa\u0107 kod, wiersze i analizowa\u0107 prawo.\n",
    "Jednak\u017ce, maj\u0105 one swoje ograniczenia, zwane halucynacjami.\n",
    "\n",
    "ROZDZIA\u0141 2: PROBLEM PAMI\u0118CI\n",
    "Modele nie maj\u0105 pami\u0119ci d\u0142ugotrwa\u0142ej. Kiedy zamykasz okno czatu, model zapomina, o czym rozmawiali\u015bcie.\n",
    "Dlatego stosujemy techniki takie jak RAG (Retrieval Augmented Generation), aby dostarczy\u0107 im kontekst.\n",
    "\n",
    "ROZDZIA\u0141 3: PRZYSZ\u0141O\u015a\u0106\n",
    "W przysz\u0142o\u015bci agenci AI b\u0119d\u0105 dzia\u0142a\u0107 autonomicznie. B\u0119d\u0105 rezerwowa\u0107 loty i robi\u0107 zakupy.\n",
    "Wa\u017cne jest jednak bezpiecze\u0144stwo i etyka tych system\u00f3w.\n",
    "\"\"\"\n",
    "\n",
    "# Usuwamy zb\u0119dne puste linie dla czytelno\u015bci\n",
    "long_text = long_text.strip()\n",
    "print(f\"D\u0142ugo\u015b\u0107 tekstu: {len(long_text)} znak\u00f3w.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f562d",
   "metadata": {},
   "source": [
    "## Metoda 1: Naive Chunking (Sztywne ci\u0119cie)\n",
    "\n",
    "Tniemy tekst r\u00f3wno co `chunk_size` znak\u00f3w.\n",
    "To najprostsza metoda, ale niszczy sens, je\u015bli ci\u0119cie wypadnie w \u015brodku wa\u017cnego s\u0142owa lub zdania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d648eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METODA NAIWNA ---\n",
      "CHUNK 0: [ROZDZIA\u0141 1: WST\u0118P DO AI\n",
      "Sztuczna inteligencja zmienia \u015bwiat w tempie wyk\u0142adniczym. Modele j\u0119zykowe, takie jak GPT-4, pot]\n",
      "CHUNK 1: [rafi\u0105 pisa\u0107 kod, wiersze i analizowa\u0107 prawo.\n",
      "Jednak\u017ce, maj\u0105 one swoje ograniczenia, zwane halucynacjami.\n",
      "\n",
      "ROZDZIA\u0141 2: PR]\n",
      "CHUNK 2: [OBLEM PAMI\u0118CI\n",
      "Modele nie maj\u0105 pami\u0119ci d\u0142ugotrwa\u0142ej. Kiedy zamykasz okno czatu, model zapomina, o czym rozmawiali\u015bcie.\n",
      "Dl]\n",
      "CHUNK 3: [atego stosujemy techniki takie jak RAG (Retrieval Augmented Generation), aby dostarczy\u0107 im kontekst.\n",
      "\n",
      "ROZDZIA\u0141 3: PRZYSZ]\n",
      "CHUNK 4: [\u0141O\u015a\u0106\n",
      "W przysz\u0142o\u015bci agenci AI b\u0119d\u0105 dzia\u0142a\u0107 autonomicznie. B\u0119d\u0105 rezerwowa\u0107 loty i robi\u0107 zakupy.\n",
      "Wa\u017cne jest jednak bezpiecz]\n",
      "CHUNK 5: [e\u0144stwo i etyka tych system\u00f3w.]\n",
      "\n",
      "ZAUWA\u017b PROBLEM:\n",
      "Chunk 0 ko\u0144czy si\u0119 w po\u0142owie zdania. Chunk 1 zaczyna si\u0119 od \u015brodka.\n",
      "Je\u015bli zapytasz AI o 'halucynacje' (koniec Chunk 0), mo\u017ce nie zrozumie\u0107 kontekstu z pocz\u0105tku Chunk 1.\n"
     ]
    }
   ],
   "source": [
    "def naive_chunking(text, chunk_size=100):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunk = text[i : i + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# Testujemy\n",
    "chunks_naive = naive_chunking(long_text, chunk_size=120)\n",
    "\n",
    "print(\"--- METODA NAIWNA ---\")\n",
    "for i, chunk in enumerate(chunks_naive):\n",
    "    print(f\"CHUNK {i}: [{chunk}]\")\n",
    "\n",
    "print(\"\\nZAUWA\u017b PROBLEM:\")\n",
    "print(\"Chunk 0 ko\u0144czy si\u0119 w po\u0142owie zdania. Chunk 1 zaczyna si\u0119 od \u015brodka.\")\n",
    "print(\"Je\u015bli zapytasz AI o 'halucynacje' (koniec Chunk 0), mo\u017ce nie zrozumie\u0107 kontekstu z pocz\u0105tku Chunk 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bab15b",
   "metadata": {},
   "source": [
    "## Metoda 2: Overlap (Zak\u0142adka)\n",
    "\n",
    "Naprawiamy problem.\n",
    "Tniemy kawa\u0142ek o d\u0142ugo\u015bci 120, ale nast\u0119pny zaczynamy nie od 121, tylko cofamy si\u0119 o 20 znak\u00f3w (Overlap).\n",
    "Dzi\u0119ki temu informacje z granicy ci\u0119cia pojawiaj\u0105 si\u0119 w **obu** kawa\u0142kach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e147df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METODA Z ZAK\u0141ADK\u0104 (OVERLAP) ---\n",
      "CHUNK 0: [ROZDZIA\u0141 1: WST\u0118P DO AI\n",
      "Sztuczna inteligencja zmienia \u015bwiat w tempie wyk\u0142adniczym. Modele j\u0119zykowe, takie jak GPT-4, pot]\n",
      "CHUNK 1: [j\u0119zykowe, takie jak GPT-4, potrafi\u0105 pisa\u0107 kod, wiersze i analizowa\u0107 prawo.\n",
      "Jednak\u017ce, maj\u0105 one swoje ograniczenia, zwane ]\n",
      "CHUNK 2: [one swoje ograniczenia, zwane halucynacjami.\n",
      "\n",
      "ROZDZIA\u0141 2: PROBLEM PAMI\u0118CI\n",
      "Modele nie maj\u0105 pami\u0119ci d\u0142ugotrwa\u0142ej. Kiedy za]\n",
      "CHUNK 3: [pami\u0119ci d\u0142ugotrwa\u0142ej. Kiedy zamykasz okno czatu, model zapomina, o czym rozmawiali\u015bcie.\n",
      "Dlatego stosujemy techniki takie]\n",
      "CHUNK 4: [atego stosujemy techniki takie jak RAG (Retrieval Augmented Generation), aby dostarczy\u0107 im kontekst.\n",
      "\n",
      "ROZDZIA\u0141 3: PRZYSZ]\n",
      "CHUNK 5: [ kontekst.\n",
      "\n",
      "ROZDZIA\u0141 3: PRZYSZ\u0141O\u015a\u0106\n",
      "W przysz\u0142o\u015bci agenci AI b\u0119d\u0105 dzia\u0142a\u0107 autonomicznie. B\u0119d\u0105 rezerwowa\u0107 loty i robi\u0107 zaku]\n",
      "CHUNK 6: [\u0105 rezerwowa\u0107 loty i robi\u0107 zakupy.\n",
      "Wa\u017cne jest jednak bezpiecze\u0144stwo i etyka tych system\u00f3w.]\n",
      "\n",
      "ZALETA:\n",
      "Sp\u00f3jrz na koniec Chunk 0 i pocz\u0105tek Chunk 1.\n",
      "Zdanie o halucynacjach jest powt\u00f3rzone. Dzi\u0119ki temu nie tracimy w\u0105tku!\n"
     ]
    }
   ],
   "source": [
    "def overlapping_chunking(text, chunk_size=120, overlap=30):\n",
    "    chunks = []\n",
    "    step = chunk_size - overlap  # O tyle si\u0119 przesuwamy\n",
    "    \n",
    "    for i in range(0, len(text), step):\n",
    "        chunk = text[i : i + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "        # Je\u015bli doszli\u015bmy do ko\u0144ca, przerywamy\n",
    "        if i + chunk_size >= len(text):\n",
    "            break\n",
    "            \n",
    "    return chunks\n",
    "\n",
    "chunks_overlap = overlapping_chunking(long_text, chunk_size=120, overlap=30)\n",
    "\n",
    "print(\"--- METODA Z ZAK\u0141ADK\u0104 (OVERLAP) ---\")\n",
    "for i, chunk in enumerate(chunks_overlap):\n",
    "    print(f\"CHUNK {i}: [{chunk}]\")\n",
    "    \n",
    "print(\"\\nZALETA:\")\n",
    "print(\"Sp\u00f3jrz na koniec Chunk 0 i pocz\u0105tek Chunk 1.\")\n",
    "print(\"Zdanie o halucynacjach jest powt\u00f3rzone. Dzi\u0119ki temu nie tracimy w\u0105tku!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0028aabb",
   "metadata": {},
   "source": [
    "## Metoda 3: Recursive Splitting (Metoda Profesjonalna)\n",
    "\n",
    "Tak dzia\u0142aj\u0105 biblioteki typu **LangChain** czy **LlamaIndex**.\n",
    "Algorytm nie tnie na \u015blepo po znakach.\n",
    "Pr\u00f3buje ci\u0105\u0107 w \"logicznych\" miejscach:\n",
    "1.  Najpierw szuka podw\u00f3jnych enter\u00f3w `\\n\\n` (Akapity).\n",
    "2.  Jak akapit jest wci\u0105\u017c za du\u017cy, szuka pojedynczych enter\u00f3w `\\n` (Linie).\n",
    "3.  Jak linia jest za du\u017ca, szuka spacji ` ` (S\u0142owa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9dc5699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METODA REKURENCYJNA (Logiczna) ---\n",
      "CHUNK 0:\n",
      "ROZDZIA\u0141 1: WST\u0118P DO AI\n",
      "Sztuczna inteligencja zmienia \u015bwiat w tempie wyk\u0142adniczym. Modele j\u0119zykowe, takie jak GPT-4, potrafi\u0105 pisa\u0107 kod, wiersze i analizowa\u0107 prawo.\n",
      "Jednak\u017ce, maj\u0105 one swoje ograniczenia, zwane halucynacjami.\n",
      "--------------------\n",
      "CHUNK 1:\n",
      "ROZDZIA\u0141 2: PROBLEM PAMI\u0118CI\n",
      "Modele nie maj\u0105 pami\u0119ci d\u0142ugotrwa\u0142ej. Kiedy zamykasz okno czatu, model zapomina, o czym rozmawiali\u015bcie.\n",
      "Dlatego stosujemy techniki takie jak RAG (Retrieval Augmented Generation), aby dostarczy\u0107 im kontekst.\n",
      "--------------------\n",
      "CHUNK 2:\n",
      "ROZDZIA\u0141 3: PRZYSZ\u0141O\u015a\u0106\n",
      "W przysz\u0142o\u015bci agenci AI b\u0119d\u0105 dzia\u0142a\u0107 autonomicznie. B\u0119d\u0105 rezerwowa\u0107 loty i robi\u0107 zakupy.\n",
      "Wa\u017cne jest jednak bezpiecze\u0144stwo i etyka tych system\u00f3w.\n",
      "--------------------\n",
      "WYNIK:\n",
      "Ka\u017cdy chunk to pe\u0142ny, logiczny rozdzia\u0142. Nie uci\u0119li\u015bmy \u017cadnego zdania w po\u0142owie!\n"
     ]
    }
   ],
   "source": [
    "def recursive_split(text, max_chunk_size=120):\n",
    "    chunks = []\n",
    "    \n",
    "    # KROK 1: Podziel na akapity (Logiczne ca\u0142o\u015bci)\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    \n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        # Sprawdzamy, czy jak dodamy ten akapit, to przekroczymy limit?\n",
    "        if len(current_chunk) + len(para) <= max_chunk_size:\n",
    "            # Mie\u015bci si\u0119 -> Doklejamy\n",
    "            current_chunk += para + \"\\n\\n\"\n",
    "        else:\n",
    "            # Nie mie\u015bci si\u0119 -> Zapisujemy stary chunk i zaczynamy nowy\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = para + \"\\n\\n\"\n",
    "            \n",
    "            # (Tutaj w prawdziwym LangChain nast\u0105pi\u0142oby dalsze ci\u0119cie akapitu na zdania,\n",
    "            #  je\u015bli sam akapit by\u0142by wi\u0119kszy ni\u017c max_chunk_size. Dla uproszczenia to pomijamy).\n",
    "    \n",
    "    # Dodajemy ostatni kawa\u0142ek\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "chunks_recursive = recursive_split(long_text, max_chunk_size=150)\n",
    "\n",
    "print(\"--- METODA REKURENCYJNA (Logiczna) ---\")\n",
    "for i, chunk in enumerate(chunks_recursive):\n",
    "    print(f\"CHUNK {i}:\\n{chunk}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "print(\"WYNIK:\")\n",
    "print(\"Ka\u017cdy chunk to pe\u0142ny, logiczny rozdzia\u0142. Nie uci\u0119li\u015bmy \u017cadnego zdania w po\u0142owie!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efd2583",
   "metadata": {},
   "source": [
    "## \ud83e\udde0 Podsumowanie: Jak nie zepsu\u0107 RAG-a?\n",
    "\n",
    "Wyb\u00f3r strategii zale\u017cy od danych:\n",
    "\n",
    "1.  **Kod \u017ar\u00f3d\u0142owy (Python, JS):** Tniemy w oparciu o klasy i funkcje. (Overlap ma\u0142o wa\u017cny).\n",
    "2.  **Dokumenty prawne/umowy:** Bardzo du\u017cy **Overlap** (nawet 50%), bo kontekst jednego paragrafu mo\u017ce zale\u017ce\u0107 od poprzedniego zdania.\n",
    "3.  **Wiki/Artyku\u0142y:** Recursive Split (zachowanie akapit\u00f3w) jest najlepszy.\n",
    "\n",
    "**Tu jest haczyk (Embedding Model).**\n",
    "Pami\u0119taj, \u017ce ka\u017cdy model Embeddings ma limit (np. 512 lub 8192 token\u00f3w). Tw\u00f3j Chunk **musi** by\u0107 mniejszy ni\u017c ten limit. Inaczej model utnie ko\u0144c\u00f3wk\u0119 i nawet o tym nie b\u0119dziesz wiedzia\u0142."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}