{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/takzen/ai-engineering-handbook/blob/main/notebooks/044_RL_Q_Learning_FrozenLake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9970424",
   "metadata": {},
   "source": [
    "# \ud83e\udd16 Q-Learning: Jak nauczy\u0107 robota chodzi\u0107 po lodzie?\n",
    "\n",
    "Reinforcement Learning (RL) to nauka metod\u0105 pr\u00f3b i b\u0142\u0119d\u00f3w.\n",
    "Mamy:\n",
    "1.  **Agenta:** Nasz robot.\n",
    "2.  **\u015arodowisko:** Gra (np. FrozenLake - \u015bliskie jezioro z dziurami).\n",
    "3.  **Akcje:** G\u00f3ra, D\u00f3\u0142, Lewo, Prawo.\n",
    "4.  **Nagrody:** +1 za dotarcie do prezentu, 0 za reszt\u0119 (i \u015bmier\u0107 w dziurze).\n",
    "\n",
    "**Q-Table (Tabela Jako\u015bci):**\n",
    "Agent nie ma m\u00f3zgu (sieci neuronowej). Ma **\u015bci\u0105g\u0119** (Tabel\u0119 Excela).\n",
    "*   Wiersze = Gdzie jestem? (Stan).\n",
    "*   Kolumny = Co zrobi\u0107? (Akcja).\n",
    "*   Warto\u015b\u0107 = Jak bardzo to si\u0119 op\u0142aca? (Q-Value).\n",
    "\n",
    "Na pocz\u0105tku tabela jest pusta. Agent chodzi losowo. Gdy wpadnie do dziury -> zapisuje \"To by\u0142o g\u0142upie\". Gdy znajdzie prezent -> zapisuje \"To by\u0142o super\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b430c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u015arodowisko gotowe.\n",
      "Liczba stan\u00f3w (p\u00f3l na planszy): 16\n",
      "Liczba akcji (ruchy): 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.2 environment at: venv\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m5 packages\u001b[0m \u001b[2min 590ms\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 2.41s\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 57ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfarama-notifications\u001b[0m\u001b[2m==0.0.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgymnasium\u001b[0m\u001b[2m==1.2.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Instalacja \u015brodowiska Gym (standard w RL)\n",
    "!uv pip install gymnasium\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Tworzymy \u015brodowisko FrozenLake\n",
    "# is_slippery=False u\u0142atwia spraw\u0119 (l\u00f3d nie jest \u015bliski, robot idzie tam gdzie chce)\n",
    "# render_mode=\"rgb_array\" pozwala nam podgl\u0105da\u0107, co si\u0119 dzieje\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")\n",
    "\n",
    "print(\"\u015arodowisko gotowe.\")\n",
    "print(f\"Liczba stan\u00f3w (p\u00f3l na planszy): {env.observation_space.n}\")\n",
    "print(f\"Liczba akcji (ruchy): {env.action_space.n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5244e8b1",
   "metadata": {},
   "source": [
    "## Inicjalizacja Q-Table\n",
    "\n",
    "Tworzymy tabel\u0119 rozmiaru `16 x 4` (16 p\u00f3l na planszy, 4 mo\u017cliwe ruchy).\n",
    "Wype\u0142niamy j\u0105 zerami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "497e79fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pusta Q-Table ---\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "\n",
    "# Q-Table: [16 wierszy, 4 kolumny]\n",
    "q_table = np.zeros((state_space, action_space))\n",
    "\n",
    "print(\"--- Pusta Q-Table ---\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caa46d3",
   "metadata": {},
   "source": [
    "## Algorytm Q-Learning (R\u00f3wnanie Bellmana)\n",
    "\n",
    "To jest serce RL. Wz\u00f3r na aktualizacj\u0119 wiedzy:\n",
    "\n",
    "$$ Q(s, a) = Q(s, a) + \\alpha [R + \\gamma \\max Q(s', a') - Q(s, a)] $$\n",
    "\n",
    "Po ludzku:\n",
    "Nowa Wiedza = Stara Wiedza + Nauka * (Nagroda + Przewidywana Przysz\u0142o\u015b\u0107 - Stara Wiedza).\n",
    "\n",
    "Parametry:\n",
    "*   **Alpha ($\\alpha$):** Szybko\u015b\u0107 uczenia (0.1 - 0.9).\n",
    "*   **Gamma ($\\gamma$):** Jak bardzo dbamy o przysz\u0142o\u015b\u0107? (0.9 = bardzo, 0.1 = \u017cyjemy chwil\u0105).\n",
    "*   **Epsilon ($\\epsilon$):** Ch\u0119\u0107 eksploracji. Na pocz\u0105tku (1.0) robimy losowe ruchy, \u017ceby pozna\u0107 \u015bwiat. Z czasem zmniejszamy epsilon, \u017ceby wykorzystywa\u0107 wiedz\u0119."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "171630cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Trening zako\u0144czony!\n"
     ]
    }
   ],
   "source": [
    "# Hiperparametry\n",
    "num_episodes = 2000       # Ile razy gramy?\n",
    "max_steps = 100           # Maksymalna liczba krok\u00f3w w jednej grze\n",
    "learning_rate = 0.8       # Alpha\n",
    "gamma = 0.95              # Discount factor (przysz\u0142o\u015b\u0107 jest wa\u017cna)\n",
    "\n",
    "# Parametry Eksploracji (Epsilon Greedy)\n",
    "epsilon = 1.0             # Na pocz\u0105tku 100% losowo\u015bci\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.005        # Jak szybko przestajemy losowa\u0107\n",
    "\n",
    "rewards_all_episodes = []\n",
    "\n",
    "# --- G\u0141\u00d3WNA P\u0118TLA TRENINGOWA ---\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 1. Decyzja: Eksploracja (Los) czy Eksploatacja (Wiedza)?\n",
    "        tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        if tradeoff > epsilon:\n",
    "            action = np.argmax(q_table[state, :]) # Wybierz najlepszy znany ruch\n",
    "        else:\n",
    "            action = env.action_space.sample() # Wybierz losowy ruch\n",
    "            \n",
    "        # 2. Wykonaj ruch\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # 3. Aktualizacja Q-Table (R\u00f3wnanie Bellmana)\n",
    "        # Q(s,a) = Q(s,a) + lr * [R + gamma * max(Q(s',a')) - Q(s,a)]\n",
    "        q_table[state, action] = q_table[state, action] + learning_rate * (\n",
    "            reward + gamma * np.max(q_table[new_state, :]) - q_table[state, action]\n",
    "        )\n",
    "        \n",
    "        state = new_state\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    # Zmniejszamy Epsilon (coraz mniej losowo\u015bci)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "print(\"\u2705 Trening zako\u0144czony!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2105ed3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- \u015arednia nagroda na 100 gier ---\n",
      "100: 0.00\n",
      "200: 0.00\n",
      "300: 0.00\n",
      "400: 0.00\n",
      "500: 0.00\n",
      "600: 0.00\n",
      "700: 0.00\n",
      "800: 0.00\n",
      "900: 0.00\n",
      "1000: 0.00\n",
      "1100: 0.00\n",
      "1200: 0.00\n",
      "1300: 0.00\n",
      "1400: 0.00\n",
      "1500: 0.00\n",
      "1600: 0.00\n",
      "1700: 0.00\n",
      "1800: 0.00\n",
      "1900: 0.00\n",
      "2000: 0.00\n",
      "\n",
      "--- NAUCZONA Q-TABLE ---\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "(Widzisz liczby inne ni\u017c zero? To \u015bcie\u017cka do nagrody!)\n"
     ]
    }
   ],
   "source": [
    "# Sprawd\u017amy wyniki\n",
    "# Dzielimy trening na paczki po 100 gier i liczymy \u015bredni\u0105 nagrod\u0119\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/100)\n",
    "count = 100\n",
    "\n",
    "print(\"--- \u015arednia nagroda na 100 gier ---\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(f\"{count}: {sum(r)/100 :.2f}\")\n",
    "    count += 100\n",
    "\n",
    "print(\"\\n--- NAUCZONA Q-TABLE ---\")\n",
    "print(q_table)\n",
    "print(\"(Widzisz liczby inne ni\u017c zero? To \u015bcie\u017cka do nagrody!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d53841",
   "metadata": {},
   "source": [
    "## Gra w Czasie Rzeczywistym\n",
    "\n",
    "Teraz, gdy agent ma wype\u0142nion\u0105 tabel\u0119, pu\u015bcimy go na plansz\u0119 bez losowo\u015bci (`epsilon=0`).\n",
    "Powinien i\u015b\u0107 prosto do celu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec1b096f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start gry!\n",
      "Liczba krok\u00f3w: 20\n"
     ]
    }
   ],
   "source": [
    "# Funkcja do wizualizacji gry\n",
    "def play_game(env, q_table):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    print(\"Start gry!\")\n",
    "    path = []\n",
    "    \n",
    "    for step in range(20):\n",
    "        # Zawsze wybieramy najlepszy ruch (argmax)\n",
    "        action = np.argmax(q_table[state, :])\n",
    "        \n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        path.append(state)\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            if reward == 1:\n",
    "                print(\"\ud83c\udfc6 SUKCES! Agent dotar\u0142 do prezentu.\")\n",
    "            else:\n",
    "                print(\"\u2620\ufe0f PORA\u017bKA! Agent wpad\u0142 do dziury.\")\n",
    "            break\n",
    "            \n",
    "    print(f\"Liczba krok\u00f3w: {step+1}\")\n",
    "\n",
    "# Zagrajmy!\n",
    "play_game(env, q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b4864",
   "metadata": {},
   "source": [
    "## \ud83e\udde0 Podsumowanie: Gdzie jest haczyk?\n",
    "\n",
    "To zadzia\u0142a\u0142o \u015bwietnie, bo plansza ma 16 p\u00f3l.\n",
    "Tabela Q ma rozmiar $16 \\times 4 = 64$ kom\u00f3rki. To ma\u0142o.\n",
    "\n",
    "**Problem:**\n",
    "Wyobra\u017a sobie gr\u0119 w szachy albo StarCrafta. Liczba stan\u00f3w jest wi\u0119ksza ni\u017c liczba atom\u00f3w we wszech\u015bwiecie.\n",
    "Nie da si\u0119 stworzy\u0107 Q-Tabeli dla szach\u00f3w. Zjad\u0142aby ca\u0142\u0105 pami\u0119\u0107 RAM \u015bwiata.\n",
    "\n",
    "**Rozwi\u0105zanie:**\n",
    "Zamiast Tabeli (Excela), u\u017cyjemy **Sieci Neuronowej** (Funkcji), kt\u00f3ra b\u0119dzie *zgadywa\u0107* warto\u015bci Q dla ka\u017cdego stanu.\n",
    "To si\u0119 nazywa **Deep Q-Network (DQN)** i tym zajmiemy si\u0119 w nast\u0119pnym notatniku."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}