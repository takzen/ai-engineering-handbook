{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n<a href=\"https://colab.research.google.com/github/takzen/ai-engineering-handbook/blob/main/notebooks/020_RAG_Architecture_Simulation.ipynb\" target=\"_parent\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n<a href=\"https://colab.research.google.com/github/takzen/ai-engineering-handbook/blob/main/20_RAG_Architecture_Simulation.ipynb\" target=\"_parent\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e18b40",
   "metadata": {},
   "source": [
    "# \ud83e\udd16 RAG: Jak rozmawia\u0107 z w\u0142asnymi PDF-ami?\n",
    "\n",
    "ChatGPT jest \u015bwietny, ale ma jedn\u0105 wad\u0119: **nie zna Twoich sekret\u00f3w**. Nie wie, jaki jest regulamin urlopowy w Twojej firmie, ani co wczoraj zjad\u0142e\u015b na \u015bniadanie.\n",
    "\n",
    "Mamy dwa wyj\u015bcia:\n",
    "1.  **Fine-Tuning:** Douczamy model (bardzo drogie i trudne).\n",
    "2.  **RAG (Retrieval Augmented Generation):** Oszukujemy system.\n",
    "\n",
    "**Jak dzia\u0142a RAG?**\n",
    "Zamiast uczy\u0107 studenta (model) ca\u0142ej encyklopedii na pami\u0119\u0107, dajemy mu **otwart\u0105 ksi\u0105\u017ck\u0119** podczas egzaminu.\n",
    "\n",
    "1.  **Retrieval (Wyszukanie):** U\u017cytkownik zadaje pytanie. My przeszukujemy nasz\u0105 baz\u0119 i znajdujemy odpowiedni fragment tekstu (korzystaj\u0105c z Cosine Similarity).\n",
    "2.  **Augmentation (Rozszerzenie):** Doklejamy ten fragment do pytania jako \"Kontekst\".\n",
    "3.  **Generation (Generowanie):** Wysy\u0142amy ca\u0142o\u015b\u0107 do LLM.\n",
    "\n",
    "W tym notatniku zbudujemy ten proces od zera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dcc5029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baza wiedzy zindeksowana! Mamy 5 dokument\u00f3w.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. TWOJA PRYWATNA BAZA WIEDZY\n",
    "# Wyobra\u017a sobie, \u017ce to s\u0105 fragmenty wyci\u0105gni\u0119te z firmowych PDF-\u00f3w.\n",
    "knowledge_base = [\n",
    "    \"ID_01: Polityka urlopowa: Ka\u017cdemu pracownikowi przys\u0142uguje 26 dni urlopu rocznie.\",\n",
    "    \"ID_02: Godziny pracy: Biuro jest czynne od 8:00 do 16:00, ale pi\u0105tki s\u0105 zdalne.\",\n",
    "    \"ID_03: Has\u0142o do Wi-Fi: Sie\u0107 nazywa si\u0119 'Firma_Guest', has\u0142o to 'SuperTajne123'.\",\n",
    "    \"ID_04: Ekspres do kawy: Aby zrobi\u0107 latte, naci\u015bnij dwa razy guzik z ziarnem.\",\n",
    "    \"ID_05: Kontakt IT: W razie awarii dzwo\u0144 do Micha\u0142a pod numer 555-000-111.\"\n",
    "]\n",
    "\n",
    "# 2. ZAMIANA NA WEKTORY (TF-IDF)\n",
    "# U\u017cywamy TF-IDF zamiast zwyk\u0142ego liczenia s\u0142\u00f3w, bo jest m\u0105drzejsze\n",
    "# (ignoruje s\u0142owa typu \"i\", \"w\", a skupia si\u0119 na unikalnych jak \"Wi-Fi\").\n",
    "vectorizer = TfidfVectorizer()\n",
    "base_vectors = vectorizer.fit_transform(knowledge_base)\n",
    "\n",
    "print(f\"Baza wiedzy zindeksowana! Mamy {len(knowledge_base)} dokument\u00f3w.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3623f6",
   "metadata": {},
   "source": [
    "## Krok 1: Retrieval (Wyszukiwarka)\n",
    "\n",
    "To jest element, kt\u00f3ry decyduje o inteligencji systemu. Je\u015bli tutaj zawiedziemy, model dostanie z\u0142e dane i zacznie k\u0142ama\u0107 (\"Garbage In, Garbage Out\").\n",
    "\n",
    "Napiszemy funkcj\u0119 `retrieve_context`, kt\u00f3ra:\n",
    "1.  Bierze pytanie u\u017cytkownika.\n",
    "2.  Zamienia je na wektor.\n",
    "3.  Liczy podobie\u0144stwo do ka\u017cdego dokumentu w bazie.\n",
    "4.  Zwraca ten najlepszy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21fb4127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TEST WYSZUKIWARKI (RETRIEVAL) ---\n",
      "Pytanie: 'Jakie jest has\u0142o do neta?'\n",
      "Znaleziono (Pewno\u015b\u0107 0.42):\n",
      "   -> ID_03: Has\u0142o do Wi-Fi: Sie\u0107 nazywa si\u0119 'Firma_Guest', has\u0142o to 'SuperTajne123'.\n",
      "------------------------------\n",
      "Pytanie: 'Ile mam wolnego w roku?'\n",
      "Znaleziono (Pewno\u015b\u0107 0.00):\n",
      "   -> ID_01: Polityka urlopowa: Ka\u017cdemu pracownikowi przys\u0142uguje 26 dni urlopu rocznie.\n",
      "------------------------------\n",
      "Pytanie: 'Co zrobi\u0107 jak komputer nie dzia\u0142a?'\n",
      "Znaleziono (Pewno\u015b\u0107 0.30):\n",
      "   -> ID_04: Ekspres do kawy: Aby zrobi\u0107 latte, naci\u015bnij dwa razy guzik z ziarnem.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def retrieve_context(user_query, documents, vectorizer, document_vectors):\n",
    "    # 1. Zamie\u0144 pytanie na wektor\n",
    "    query_vec = vectorizer.transform([user_query])\n",
    "    \n",
    "    # 2. Policz podobie\u0144stwo (Cosine Similarity)\n",
    "    similarities = cosine_similarity(query_vec, document_vectors).flatten()\n",
    "    \n",
    "    # 3. Znajd\u017a indeks najlepszego dokumentu\n",
    "    best_idx = np.argmax(similarities)\n",
    "    best_score = similarities[best_idx]\n",
    "    \n",
    "    # Zwr\u00f3\u0107 tre\u015b\u0107 dokumentu i jego wynik pewno\u015bci\n",
    "    return documents[best_idx], best_score\n",
    "\n",
    "# TESTUJEMY!\n",
    "pytania = [\n",
    "    \"Jakie jest has\u0142o do neta?\",\n",
    "    \"Ile mam wolnego w roku?\",\n",
    "    \"Co zrobi\u0107 jak komputer nie dzia\u0142a?\"\n",
    "]\n",
    "\n",
    "print(\"--- TEST WYSZUKIWARKI (RETRIEVAL) ---\")\n",
    "for p in pytania:\n",
    "    doc, score = retrieve_context(p, knowledge_base, vectorizer, base_vectors)\n",
    "    print(f\"Pytanie: '{p}'\")\n",
    "    print(f\"Znaleziono (Pewno\u015b\u0107 {score:.2f}):\\n   -> {doc}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063d9ced",
   "metadata": {},
   "source": [
    "## Krok 2: Augmentation & Generation (Konstrukcja Promptu)\n",
    "\n",
    "Teraz najwa\u017cniejsza cz\u0119\u015b\u0107. Nie mamy tu podpi\u0119tego prawdziwego GPT-4 (bo wymaga klucza API), ale **zasymulujemy** to, co dzieje si\u0119 wewn\u0105trz aplikacji typu ChatPDF.\n",
    "\n",
    "Musimy stworzy\u0107 **Prompt Systemowy**, kt\u00f3ry \u0142\u0105czy:\n",
    "1.  Rol\u0119 AI (\"Jeste\u015b pomocnym asystentem\").\n",
    "2.  Znaleziony kontekst (\"Tu masz dane: ...\").\n",
    "3.  Pytanie u\u017cytkownika."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fa68736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TO WYSY\u0141AMY DO GPT-4 (POD MASK\u0104) ---\n",
      "\n",
      "    --- ROLA ---\n",
      "    Jeste\u015b asystentem biurowym. Odpowiadaj TYLKO na podstawie poni\u017cszego kontekstu.\n",
      "    Nie u\u017cywaj wiedzy z zewn\u0105trz.\n",
      "\n",
      "    --- KONTEKST (TAJNE DANE FIRMOWE) ---\n",
      "    ID_03: Has\u0142o do Wi-Fi: Sie\u0107 nazywa si\u0119 'Firma_Guest', has\u0142o to 'SuperTajne123'.\n",
      "\n",
      "    --- PYTANIE U\u017bYTKOWNIKA ---\n",
      "    Ej, jak si\u0119 po\u0142\u0105czy\u0107 z wifi?\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def generate_rag_prompt(user_query):\n",
    "    # 1. Najpierw znajd\u017a wiedz\u0119 (Retrieval)\n",
    "    context_text, score = retrieve_context(user_query, knowledge_base, vectorizer, base_vectors)\n",
    "    \n",
    "    # Je\u015bli system nie jest pewny (niski score), nie zmy\u015blajmy\n",
    "    if score < 0.2:\n",
    "        return \"System: Nie znalaz\u0142em informacji w bazie dokument\u00f3w. Prosz\u0119 skontaktowa\u0107 si\u0119 z HR.\"\n",
    "    \n",
    "    # 2. Sklej wszystko w jeden Prompt dla LLM (Augmentation)\n",
    "    final_prompt = f\"\"\"\n",
    "    --- ROLA ---\n",
    "    Jeste\u015b asystentem biurowym. Odpowiadaj TYLKO na podstawie poni\u017cszego kontekstu.\n",
    "    Nie u\u017cywaj wiedzy z zewn\u0105trz.\n",
    "    \n",
    "    --- KONTEKST (TAJNE DANE FIRMOWE) ---\n",
    "    {context_text}\n",
    "    \n",
    "    --- PYTANIE U\u017bYTKOWNIKA ---\n",
    "    {user_query}\n",
    "    \"\"\"\n",
    "    \n",
    "    return final_prompt\n",
    "\n",
    "# SYMULACJA RAG\n",
    "pytanie_usera = \"Ej, jak si\u0119 po\u0142\u0105czy\u0107 z wifi?\"\n",
    "\n",
    "prompt = generate_rag_prompt(pytanie_usera)\n",
    "\n",
    "print(\"--- TO WYSY\u0141AMY DO GPT-4 (POD MASK\u0104) ---\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e03c448",
   "metadata": {},
   "source": [
    "## \ud83e\udde0 Podsumowanie: Iluzja Inteligencji\n",
    "\n",
    "Zobacz, co si\u0119 sta\u0142o.\n",
    "U\u017cytkownik zapyta\u0142 o \"wifi\". W bazie nie by\u0142o s\u0142owa \"wifi\" (by\u0142o \"Wi-Fi\" lub \"sie\u0107\"), ale wektoryzator to wy\u0142apa\u0142 (lub wy\u0142apa\u0142by lepszy model Embeddings).\n",
    "\n",
    "System RAG dzia\u0142a jak sufler w teatrze:\n",
    "1.  Aktor (LLM) zapomnia\u0142 tekstu.\n",
    "2.  Sufler (Retrieval) szybko kartkuje scenariusz i podpowiada **tylko to jedno zdanie**, kt\u00f3re jest teraz potrzebne.\n",
    "3.  Aktor wypowiada je p\u0142ynnie.\n",
    "\n",
    "**Wniosek:**\n",
    "Jako AI Engineer, Twoim g\u0142\u00f3wnym zadaniem jest budowanie **dobrego Suflera**.\n",
    "Bo nawet GPT-5 nie odpowie poprawnie na pytanie o Twoj\u0105 firm\u0119, je\u015bli Sufler poda mu z\u0142\u0105 stron\u0119 scenariusza (z\u0142y kontekst)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}