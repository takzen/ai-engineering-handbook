{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdd5fe80",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/takzen/ai-engineering-handbook/blob/main/68_RLHF_PPO_ChatGPT_Alignment.ipynb\" target=\"_parent\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e65e7",
   "metadata": {},
   "source": [
    "#  RLHF & PPO: Jak powsta ChatGPT?\n",
    "\n",
    "Trenowanie asystenta AI skada si z 3 faz:\n",
    "1.  **Pre-training:** Model czyta internet i uczy si m贸wi (GPT-3).\n",
    "2.  **SFT (Supervised Fine-Tuning):** Model uczy si formatu pyta i odpowiedzi.\n",
    "3.  **RLHF (PPO):** Model uczy si, kt贸re odpowiedzi ludzie lubi bardziej.\n",
    "\n",
    "**Problem z RL w tekstach:**\n",
    "Jeli nagrodzimy model tylko za \"pozytywny wyd藕wik\", to model zacznie w k贸ko powtarza sowo \"Mio Mio Mio\", bo to daje max punkt贸w. Traci zdolno m贸wienia po polsku.\n",
    "\n",
    "**Rozwizanie PPO (Proximal Policy Optimization):**\n",
    "Mamy dwa modele:\n",
    "1.  **Policy Model (Ucze):** Ten, kt贸rego trenujemy.\n",
    "2.  **Reference Model (Nauczyciel):** Kopia modelu z fazy 2 (zamro偶ona).\n",
    "\n",
    "Liczymy **KL Divergence (Kar za odmienno)**. Jeli Ucze zaczyna gada bzdury (zbyt r贸偶ni si od Nauczyciela), dostaje kar, nawet jeli nagroda za tre bya wysoka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffcc717d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urzdzenie: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "# Konfiguracja\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LR = 3e-4\n",
    "CLIP_EPS = 0.2  # Najwa偶niejszy parametr PPO (o ile mo偶emy zmieni polityk?)\n",
    "\n",
    "print(f\"Urzdzenie: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8448ad",
   "metadata": {},
   "source": [
    "## Model Actor-Critic\n",
    "\n",
    "W PPO potrzebujemy dw贸ch g贸w:\n",
    "1.  **Actor:** Wybiera akcj (sowo). Zwraca prawdopodobiestwo.\n",
    "2.  **Critic:** Ocenia stan. M贸wi: \"Jestem w dobrej sytuacji, spodziewam si du偶ej nagrody\".\n",
    "\n",
    "Dla uproszczenia zrobimy prost sie, kt贸ra gra w gr liczbow (zamiast generowa tekst, generuje liczby, kt贸re maj by bliskie celu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e54af62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modele gotowe. Mamy Ucznia i Nauczyciela (Reference).\n"
     ]
    }
   ],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super().__init__()\n",
    "        # Wsp贸lny pie (Backbone)\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Gowa Aktora (Policy) - zwraca logity akcji\n",
    "        self.actor = nn.Linear(64, action_dim)\n",
    "        \n",
    "        # Gowa Krytyka (Value) - zwraca jedn liczb (ocen sytuacji)\n",
    "        self.critic = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.shared(x)\n",
    "        action_logits = self.actor(features)\n",
    "        state_value = self.critic(features)\n",
    "        return action_logits, state_value\n",
    "\n",
    "# Inicjalizacja\n",
    "# Input: Stan gry (wektor 4 liczb)\n",
    "# Action: 2 mo偶liwe ruchy\n",
    "model = ActorCritic(4, 2).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Reference Model (Kopia, kt贸rej nie trenujemy!)\n",
    "ref_model = copy.deepcopy(model)\n",
    "ref_model.eval() # Zamra偶amy\n",
    "\n",
    "print(\"Modele gotowe. Mamy Ucznia i Nauczyciela (Reference).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f10862",
   "metadata": {},
   "source": [
    "## Matematyka PPO (Clipped Loss)\n",
    "\n",
    "To jest serce algorytmu. Wz贸r wyglda strasznie, ale idea jest prosta:\n",
    "$$ Ratio = \\frac{P_{nowe}}{P_{stare}} $$\n",
    "\n",
    "Jeli `Ratio` jest bliskie 1, to znaczy, 偶e model zmieni si niewiele (bezpiecznie).\n",
    "Jeli `Ratio` jest np. 2.0 (model nagle 2x chtniej wybiera akcj), PPO ucina (clip) ten zysk, 偶eby model nie \"wybuch\" od zbyt gwatownej zmiany.\n",
    "\n",
    "$$ Loss = - \\min(Ratio \\cdot A, \\text{clip}(Ratio, 1-\\epsilon, 1+\\epsilon) \\cdot A) $$\n",
    "*   $A$ to Advantage (Zaleta) - czy akcja bya lepsza ni偶 oczekiwano?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da5e8a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funkcja kosztu PPO zdefiniowana.\n"
     ]
    }
   ],
   "source": [
    "def ppo_loss(old_log_probs, new_log_probs, advantages, returns, values):\n",
    "    # 1. Obliczamy Ratio (r_t)\n",
    "    # exp(new - old) to matematycznie to samo co (new_prob / old_prob)\n",
    "    ratio = (new_log_probs - old_log_probs).exp()\n",
    "    \n",
    "    # 2. Surrogate Loss 1 (Zwyky)\n",
    "    surr1 = ratio * advantages\n",
    "    \n",
    "    # 3. Surrogate Loss 2 (Przycity - Clipped)\n",
    "    # To jest hamulec bezpieczestwa PPO!\n",
    "    surr2 = torch.clamp(ratio, 1.0 - CLIP_EPS, 1.0 + CLIP_EPS) * advantages\n",
    "    \n",
    "    # 4. Loss Policy (Actor) - bierzemy minimum (pesymistyczne podejcie)\n",
    "    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "    \n",
    "    # 5. Loss Value (Critic) - MSE midzy przewidywaniem a wynikiem\n",
    "    value_loss = F.mse_loss(values.flatten(), returns)\n",
    "    \n",
    "    # Suma\n",
    "    total_loss = policy_loss + 0.5 * value_loss\n",
    "    return total_loss\n",
    "\n",
    "print(\"Funkcja kosztu PPO zdefiniowana.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c4ce27",
   "metadata": {},
   "source": [
    "## Symulacja Treningu (Z kar KL)\n",
    "\n",
    "Zasymulujemy jeden krok treningowy RLHF.\n",
    "1.  **Rollout:** Model generuje akcj.\n",
    "2.  **Reward:** Dostaje nagrod od rodowiska.\n",
    "3.  **KL Penalty:** Sprawdzamy, jak bardzo ta akcja r贸偶ni si od tego, co zrobiby Reference Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc783c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- WPYW REFERENCE MODEL ---\n",
      "Oryginalna nagroda: 1.0\n",
      "Kara KL (odchylenie): -0.0190\n",
      "Nagroda po korekcie: 1.0019\n",
      "PPO Step 0: Loss=0.4167\n",
      "PPO Step 1: Loss=0.4132\n",
      "PPO Step 2: Loss=0.4071\n",
      "PPO Step 3: Loss=0.3999\n",
      "PPO Step 4: Loss=0.3922\n"
     ]
    }
   ],
   "source": [
    "# Symulacja danych (Batch)\n",
    "states = torch.randn(10, 4).to(DEVICE) # 10 sytuacji\n",
    "actions = torch.randint(0, 2, (10,)).to(DEVICE) # 10 podjtych decyzji\n",
    "rewards = torch.tensor([1.0] * 5 + [-1.0] * 5).to(DEVICE) # Nagrody (5 dobrych, 5 zych)\n",
    "\n",
    "# --- KROK 1: Obliczamy co myl oba modele ---\n",
    "# Nowy model (ten co si uczy)\n",
    "logits, values = model(states)\n",
    "new_log_probs = F.log_softmax(logits, dim=1)\n",
    "# Wybieramy prawdop. tylko dla akcji, kt贸re faktycznie podjlimy\n",
    "new_log_probs_actions = new_log_probs.gather(1, actions.unsqueeze(1)).flatten()\n",
    "\n",
    "# Stary model (Reference - zamro偶ony)\n",
    "with torch.no_grad():\n",
    "    ref_logits, _ = ref_model(states)\n",
    "    ref_log_probs = F.log_softmax(ref_logits, dim=1)\n",
    "    ref_log_probs_actions = ref_log_probs.gather(1, actions.unsqueeze(1)).flatten()\n",
    "\n",
    "# --- KROK 2: KL Divergence (Kara) ---\n",
    "# Wz贸r: log(P_new) - log(P_ref)\n",
    "kl_div = new_log_probs_actions - ref_log_probs_actions\n",
    "\n",
    "# Odejmujemy KL od nagrody!\n",
    "beta_kl = 0.1 # Sia kary\n",
    "adjusted_rewards = rewards - (beta_kl * kl_div)\n",
    "\n",
    "# --- POPRAWKA: DETACH ---\n",
    "# Odpinamy nagrody od grafu. Traktujemy je teraz jako stae liczby (target).\n",
    "adjusted_rewards = adjusted_rewards.detach()\n",
    "\n",
    "print(\"--- WPYW REFERENCE MODEL ---\")\n",
    "print(f\"Oryginalna nagroda: {rewards[0].item()}\")\n",
    "print(f\"Kara KL (odchylenie): {kl_div[0].item():.4f}\")\n",
    "print(f\"Nagroda po korekcie: {adjusted_rewards[0].item():.4f}\")\n",
    "\n",
    "# --- KROK 3: PPO Update ---\n",
    "# Uproszczone Advantage\n",
    "advantages = adjusted_rewards \n",
    "\n",
    "# Zapisujemy \"stare\" prawdopodobiestwa (te偶 odpite, bo to kopia z przeszoci)\n",
    "old_log_probs_actions = new_log_probs_actions.detach()\n",
    "\n",
    "# Symulacja kilku epok PPO na tym samym batchu\n",
    "for i in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Recalculate (bo model si zmienia w ptli - budujemy nowy graf w ka偶dej iteracji)\n",
    "    logits, values = model(states)\n",
    "    curr_log_probs = F.log_softmax(logits, dim=1).gather(1, actions.unsqueeze(1)).flatten()\n",
    "    \n",
    "    # Teraz adjusted_rewards jest bezpieczne (detached)\n",
    "    loss = ppo_loss(old_log_probs_actions, curr_log_probs, advantages, adjusted_rewards, values)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"PPO Step {i}: Loss={loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386cde06",
   "metadata": {},
   "source": [
    "##  Podsumowanie: Alignment Tax\n",
    "\n",
    "To, co zrobilimy (dodanie kary `beta_kl * kl_div`), nazywa si w bran偶y **Alignment Tax (Podatek od dopasowania)**.\n",
    "\n",
    "Model staje si \"grzeczniejszy\" i bardziej zgodny z instrukcjami czowieka (RLHF), ale przez to, 偶e musi trzyma si blisko modelu bazowego (Reference), traci troch ze swojej kreatywnoci i \"inteligencji\".\n",
    "\n",
    "**Kluczowe wnioski:**\n",
    "1.  **PPO Clip:** Zapobiega gwatownym zmianom modelu (stabilno).\n",
    "2.  **Reference Model:** Dziaa jak kotwica. Nie pozwala modelowi zapomnie jzyka polskiego, gdy uczy si by miym asystentem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-handbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
