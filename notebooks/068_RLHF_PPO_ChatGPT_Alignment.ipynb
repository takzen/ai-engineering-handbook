{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/takzen/ai-engineering-handbook/blob/main/notebooks/068_RLHF_PPO_ChatGPT_Alignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e65e7",
   "metadata": {},
   "source": [
    "# \ud83e\udd16 RLHF & PPO: Jak powsta\u0142 ChatGPT?\n",
    "\n",
    "Trenowanie asystenta AI sk\u0142ada si\u0119 z 3 faz:\n",
    "1.  **Pre-training:** Model czyta internet i uczy si\u0119 m\u00f3wi\u0107 (GPT-3).\n",
    "2.  **SFT (Supervised Fine-Tuning):** Model uczy si\u0119 formatu pyta\u0144 i odpowiedzi.\n",
    "3.  **RLHF (PPO):** Model uczy si\u0119, kt\u00f3re odpowiedzi ludzie lubi\u0105 bardziej.\n",
    "\n",
    "**Problem z RL w tekstach:**\n",
    "Je\u015bli nagrodzimy model tylko za \"pozytywny wyd\u017awi\u0119k\", to model zacznie w k\u00f3\u0142ko powtarza\u0107 s\u0142owo \"Mi\u0142o\u015b\u0107 Mi\u0142o\u015b\u0107 Mi\u0142o\u015b\u0107\", bo to daje max punkt\u00f3w. Traci zdolno\u015b\u0107 m\u00f3wienia po polsku.\n",
    "\n",
    "**Rozwi\u0105zanie PPO (Proximal Policy Optimization):**\n",
    "Mamy dwa modele:\n",
    "1.  **Policy Model (Ucze\u0144):** Ten, kt\u00f3rego trenujemy.\n",
    "2.  **Reference Model (Nauczyciel):** Kopia modelu z fazy 2 (zamro\u017cona).\n",
    "\n",
    "Liczymy **KL Divergence (Kar\u0119 za odmienno\u015b\u0107)**. Je\u015bli Ucze\u0144 zaczyna gada\u0107 bzdury (zbyt r\u00f3\u017cni si\u0119 od Nauczyciela), dostaje kar\u0119, nawet je\u015bli nagroda za tre\u015b\u0107 by\u0142a wysoka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffcc717d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urz\u0105dzenie: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "# Konfiguracja\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LR = 3e-4\n",
    "CLIP_EPS = 0.2  # Najwa\u017cniejszy parametr PPO (o ile mo\u017cemy zmieni\u0107 polityk\u0119?)\n",
    "\n",
    "print(f\"Urz\u0105dzenie: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8448ad",
   "metadata": {},
   "source": [
    "## Model Actor-Critic\n",
    "\n",
    "W PPO potrzebujemy dw\u00f3ch g\u0142\u00f3w:\n",
    "1.  **Actor:** Wybiera akcj\u0119 (s\u0142owo). Zwraca prawdopodobie\u0144stwo.\n",
    "2.  **Critic:** Ocenia stan. M\u00f3wi: \"Jestem w dobrej sytuacji, spodziewam si\u0119 du\u017cej nagrody\".\n",
    "\n",
    "Dla uproszczenia zrobimy prost\u0105 sie\u0107, kt\u00f3ra gra w gr\u0119 liczbow\u0105 (zamiast generowa\u0107 tekst, generuje liczby, kt\u00f3re maj\u0105 by\u0107 bliskie celu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e54af62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modele gotowe. Mamy Ucznia i Nauczyciela (Reference).\n"
     ]
    }
   ],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super().__init__()\n",
    "        # Wsp\u00f3lny pie\u0144 (Backbone)\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # G\u0142owa Aktora (Policy) - zwraca logity akcji\n",
    "        self.actor = nn.Linear(64, action_dim)\n",
    "        \n",
    "        # G\u0142owa Krytyka (Value) - zwraca jedn\u0105 liczb\u0119 (ocen\u0119 sytuacji)\n",
    "        self.critic = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.shared(x)\n",
    "        action_logits = self.actor(features)\n",
    "        state_value = self.critic(features)\n",
    "        return action_logits, state_value\n",
    "\n",
    "# Inicjalizacja\n",
    "# Input: Stan gry (wektor 4 liczb)\n",
    "# Action: 2 mo\u017cliwe ruchy\n",
    "model = ActorCritic(4, 2).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Reference Model (Kopia, kt\u00f3rej nie trenujemy!)\n",
    "ref_model = copy.deepcopy(model)\n",
    "ref_model.eval() # Zamra\u017camy\n",
    "\n",
    "print(\"Modele gotowe. Mamy Ucznia i Nauczyciela (Reference).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f10862",
   "metadata": {},
   "source": [
    "## Matematyka PPO (Clipped Loss)\n",
    "\n",
    "To jest serce algorytmu. Wz\u00f3r wygl\u0105da strasznie, ale idea jest prosta:\n",
    "$$ Ratio = \\frac{P_{nowe}}{P_{stare}} $$\n",
    "\n",
    "Je\u015bli `Ratio` jest bliskie 1, to znaczy, \u017ce model zmieni\u0142 si\u0119 niewiele (bezpiecznie).\n",
    "Je\u015bli `Ratio` jest np. 2.0 (model nagle 2x ch\u0119tniej wybiera akcj\u0119), PPO ucina (clip) ten zysk, \u017ceby model nie \"wybuch\u0142\" od zbyt gwa\u0142townej zmiany.\n",
    "\n",
    "$$ Loss = - \\min(Ratio \\cdot A, \\text{clip}(Ratio, 1-\\epsilon, 1+\\epsilon) \\cdot A) $$\n",
    "*   $A$ to Advantage (Zaleta) - czy akcja by\u0142a lepsza ni\u017c oczekiwano?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da5e8a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funkcja kosztu PPO zdefiniowana.\n"
     ]
    }
   ],
   "source": [
    "def ppo_loss(old_log_probs, new_log_probs, advantages, returns, values):\n",
    "    # 1. Obliczamy Ratio (r_t)\n",
    "    # exp(new - old) to matematycznie to samo co (new_prob / old_prob)\n",
    "    ratio = (new_log_probs - old_log_probs).exp()\n",
    "    \n",
    "    # 2. Surrogate Loss 1 (Zwyk\u0142y)\n",
    "    surr1 = ratio * advantages\n",
    "    \n",
    "    # 3. Surrogate Loss 2 (Przyci\u0119ty - Clipped)\n",
    "    # To jest hamulec bezpiecze\u0144stwa PPO!\n",
    "    surr2 = torch.clamp(ratio, 1.0 - CLIP_EPS, 1.0 + CLIP_EPS) * advantages\n",
    "    \n",
    "    # 4. Loss Policy (Actor) - bierzemy minimum (pesymistyczne podej\u015bcie)\n",
    "    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "    \n",
    "    # 5. Loss Value (Critic) - MSE mi\u0119dzy przewidywaniem a wynikiem\n",
    "    value_loss = F.mse_loss(values.flatten(), returns)\n",
    "    \n",
    "    # Suma\n",
    "    total_loss = policy_loss + 0.5 * value_loss\n",
    "    return total_loss\n",
    "\n",
    "print(\"Funkcja kosztu PPO zdefiniowana.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c4ce27",
   "metadata": {},
   "source": [
    "## Symulacja Treningu (Z kar\u0105 KL)\n",
    "\n",
    "Zasymulujemy jeden krok treningowy RLHF.\n",
    "1.  **Rollout:** Model generuje akcj\u0119.\n",
    "2.  **Reward:** Dostaje nagrod\u0119 od \u015brodowiska.\n",
    "3.  **KL Penalty:** Sprawdzamy, jak bardzo ta akcja r\u00f3\u017cni si\u0119 od tego, co zrobi\u0142by Reference Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc783c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- WP\u0141YW REFERENCE MODEL ---\n",
      "Oryginalna nagroda: 1.0\n",
      "Kara KL (odchylenie): -0.0190\n",
      "Nagroda po korekcie: 1.0019\n",
      "PPO Step 0: Loss=0.4167\n",
      "PPO Step 1: Loss=0.4132\n",
      "PPO Step 2: Loss=0.4071\n",
      "PPO Step 3: Loss=0.3999\n",
      "PPO Step 4: Loss=0.3922\n"
     ]
    }
   ],
   "source": [
    "# Symulacja danych (Batch)\n",
    "states = torch.randn(10, 4).to(DEVICE) # 10 sytuacji\n",
    "actions = torch.randint(0, 2, (10,)).to(DEVICE) # 10 podj\u0119tych decyzji\n",
    "rewards = torch.tensor([1.0] * 5 + [-1.0] * 5).to(DEVICE) # Nagrody (5 dobrych, 5 z\u0142ych)\n",
    "\n",
    "# --- KROK 1: Obliczamy co my\u015bl\u0105 oba modele ---\n",
    "# Nowy model (ten co si\u0119 uczy)\n",
    "logits, values = model(states)\n",
    "new_log_probs = F.log_softmax(logits, dim=1)\n",
    "# Wybieramy prawdop. tylko dla akcji, kt\u00f3re faktycznie podj\u0119li\u015bmy\n",
    "new_log_probs_actions = new_log_probs.gather(1, actions.unsqueeze(1)).flatten()\n",
    "\n",
    "# Stary model (Reference - zamro\u017cony)\n",
    "with torch.no_grad():\n",
    "    ref_logits, _ = ref_model(states)\n",
    "    ref_log_probs = F.log_softmax(ref_logits, dim=1)\n",
    "    ref_log_probs_actions = ref_log_probs.gather(1, actions.unsqueeze(1)).flatten()\n",
    "\n",
    "# --- KROK 2: KL Divergence (Kara) ---\n",
    "# Wz\u00f3r: log(P_new) - log(P_ref)\n",
    "kl_div = new_log_probs_actions - ref_log_probs_actions\n",
    "\n",
    "# Odejmujemy KL od nagrody!\n",
    "beta_kl = 0.1 # Si\u0142a kary\n",
    "adjusted_rewards = rewards - (beta_kl * kl_div)\n",
    "\n",
    "# --- POPRAWKA: DETACH ---\n",
    "# Odpinamy nagrody od grafu. Traktujemy je teraz jako sta\u0142e liczby (target).\n",
    "adjusted_rewards = adjusted_rewards.detach()\n",
    "\n",
    "print(\"--- WP\u0141YW REFERENCE MODEL ---\")\n",
    "print(f\"Oryginalna nagroda: {rewards[0].item()}\")\n",
    "print(f\"Kara KL (odchylenie): {kl_div[0].item():.4f}\")\n",
    "print(f\"Nagroda po korekcie: {adjusted_rewards[0].item():.4f}\")\n",
    "\n",
    "# --- KROK 3: PPO Update ---\n",
    "# Uproszczone Advantage\n",
    "advantages = adjusted_rewards \n",
    "\n",
    "# Zapisujemy \"stare\" prawdopodobie\u0144stwa (te\u017c odpi\u0119te, bo to kopia z przesz\u0142o\u015bci)\n",
    "old_log_probs_actions = new_log_probs_actions.detach()\n",
    "\n",
    "# Symulacja kilku epok PPO na tym samym batchu\n",
    "for i in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Recalculate (bo model si\u0119 zmienia w p\u0119tli - budujemy nowy graf w ka\u017cdej iteracji)\n",
    "    logits, values = model(states)\n",
    "    curr_log_probs = F.log_softmax(logits, dim=1).gather(1, actions.unsqueeze(1)).flatten()\n",
    "    \n",
    "    # Teraz adjusted_rewards jest bezpieczne (detached)\n",
    "    loss = ppo_loss(old_log_probs_actions, curr_log_probs, advantages, adjusted_rewards, values)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"PPO Step {i}: Loss={loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386cde06",
   "metadata": {},
   "source": [
    "## \ud83e\udde0 Podsumowanie: Alignment Tax\n",
    "\n",
    "To, co zrobili\u015bmy (dodanie kary `beta_kl * kl_div`), nazywa si\u0119 w bran\u017cy **Alignment Tax (Podatek od dopasowania)**.\n",
    "\n",
    "Model staje si\u0119 \"grzeczniejszy\" i bardziej zgodny z instrukcjami cz\u0142owieka (RLHF), ale przez to, \u017ce musi trzyma\u0107 si\u0119 blisko modelu bazowego (Reference), traci troch\u0119 ze swojej kreatywno\u015bci i \"inteligencji\".\n",
    "\n",
    "**Kluczowe wnioski:**\n",
    "1.  **PPO Clip:** Zapobiega gwa\u0142townym zmianom modelu (stabilno\u015b\u0107).\n",
    "2.  **Reference Model:** Dzia\u0142a jak kotwica. Nie pozwala modelowi zapomnie\u0107 j\u0119zyka polskiego, gdy uczy si\u0119 by\u0107 mi\u0142ym asystentem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-handbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}