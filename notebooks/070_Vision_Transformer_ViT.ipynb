{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/takzen/ai-engineering-handbook/blob/main/notebooks/070_Vision_Transformer_ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e262d8",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/takzen/ai-engineering-handbook/blob/main/70_Vision_Transformer_ViT.ipynb\" target=\"_parent\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aad70b",
   "metadata": {},
   "source": [
    "# \ud83d\udc41\ufe0f Vision Transformer (ViT): Obraz jest wart 16x16 s\u0142\u00f3w\n",
    "\n",
    "CNN ma tzw. **Inductive Bias** (zak\u0142ada z g\u00f3ry, \u017ce lokalno\u015b\u0107 jest wa\u017cna).\n",
    "Transformer nie zak\u0142ada niczego. Uczy si\u0119 relacji mi\u0119dzy pikselem w lewym g\u00f3rnym rogu, a pikselem w prawym dolnym rogu od razu (Global Attention).\n",
    "\n",
    "**Algorytm ViT:**\n",
    "1.  **Patching:** Podziel obrazek na kwadraty (np. 16x16 pikseli).\n",
    "2.  **Flatten:** Sp\u0142aszcz ka\u017cdy kwadrat do wektora. To s\u0105 nasze \"s\u0142owa\".\n",
    "3.  **Position Embedding:** Dodaj informacj\u0119, gdzie ten kwadrat by\u0142 na obrazku (u\u017cyjemy trenowalnych parametr\u00f3w, a nie sinusa).\n",
    "4.  **CLS Token:** Dodaj jeden specjalny, pusty wektor na pocz\u0105tku. Po przej\u015bciu przez sie\u0107, to on b\u0119dzie zawiera\u0142 informacj\u0119 \"Co jest na obrazku?\".\n",
    "5.  **Transformer Encoder:** Standardowe bloki (Attention + MLP).\n",
    "\n",
    "U\u017cyjemy zbioru CIFAR-10 (obrazki 32x32). Podzielimy je na Patche 4x4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf7d7b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obrazek: 32x32\n",
      "Patch: 4x4\n",
      "Liczba Patchy: 64 (To b\u0119dzie d\u0142ugo\u015b\u0107 naszego 'zdania')\n",
      "Wymiar jednego Patcha (sp\u0142aszczony): 48\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Konfiguracja\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "IMG_SIZE = 32\n",
    "PATCH_SIZE = 4\n",
    "EMBED_DIM = 128  # Rozmiar wektora, w jaki zamienimy patch\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 4\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Sprawd\u017amy matematyk\u0119\n",
    "num_patches = (IMG_SIZE // PATCH_SIZE) ** 2\n",
    "input_dim = 3 * PATCH_SIZE * PATCH_SIZE # 3 kana\u0142y RGB * 4 * 4 piksele\n",
    "\n",
    "print(f\"Obrazek: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"Patch: {PATCH_SIZE}x{PATCH_SIZE}\")\n",
    "print(f\"Liczba Patchy: {num_patches} (To b\u0119dzie d\u0142ugo\u015b\u0107 naszego 'zdania')\")\n",
    "print(f\"Wymiar jednego Patcha (sp\u0142aszczony): {input_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adabdcd3",
   "metadata": {},
   "source": [
    "## Krok 1: Patch Embedding Layer\n",
    "\n",
    "Musimy zamieni\u0107 obrazek (3D) na sekwencj\u0119 wektor\u00f3w (2D).\n",
    "Mogliby\u015bmy u\u017cy\u0107 p\u0119tli i ci\u0105\u0107 obrazek, ale jest sprytniejszy spos\u00f3b.\n",
    "\n",
    "**In\u017cynierski Trik:**\n",
    "U\u017cycie warstwy `Conv2d` z rozmiarem kernela r\u00f3wnym `patch_size` i krokiem (`stride`) r\u00f3wnym `patch_size` robi dok\u0142adnie to samo! Tnie obrazek na kawa\u0142ki i od razu rzutuje je na wymiar `EMBED_DIM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12845878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wej\u015bcie: torch.Size([1, 3, 32, 32])\n",
      "Wyj\u015bcie (Sekwencja): torch.Size([1, 64, 128]) -> [Batch, Liczba Patchy, Wymiar]\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=3, patch_size=4, emb_size=128, img_size=32):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        # Trik: Conv2d jako tokenizer\n",
    "        self.projection = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [Batch, C, H, W] -> [Batch, Emb_Size, H/P, W/P]\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        # Sp\u0142aszczamy: [Batch, Emb_Size, Num_Patches]\n",
    "        x = x.flatten(2)\n",
    "        \n",
    "        # Transpozycja: [Batch, Num_Patches, Emb_Size] (Tak lubi\u0105 Transformery)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "# Test\n",
    "dummy_img = torch.randn(1, 3, 32, 32)\n",
    "pe = PatchEmbedding(patch_size=PATCH_SIZE, emb_size=EMBED_DIM)\n",
    "out = pe(dummy_img)\n",
    "print(f\"Wej\u015bcie: {dummy_img.shape}\")\n",
    "print(f\"Wyj\u015bcie (Sekwencja): {out.shape} -> [Batch, Liczba Patchy, Wymiar]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4966a18c",
   "metadata": {},
   "source": [
    "## Krok 2: The ViT (Sk\u0142adamy ca\u0142o\u015b\u0107)\n",
    "\n",
    "Tutaj dzieje si\u0119 magia.\n",
    "1.  Tworzymy **CLS Token** (learnable parameter). Doklejamy go na pocz\u0105tek sekwencji.\n",
    "2.  Tworzymy **Position Embeddings** (te\u017c learnable). Dodajemy je do sekwencji.\n",
    "3.  Przepuszczamy przez **Transformer Encoder** (PyTorch ma gotowy modu\u0142 `nn.TransformerEncoder`, ale to jest dok\u0142adnie to, co budowali\u015bmy w notatniku 46).\n",
    "4.  Na ko\u0144cu bierzemy TYLKO pierwszy token (CLS) i na jego podstawie klasyfikujemy obraz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae206631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT gotowy. Liczba parametr\u00f3w: 2,388,362\n"
     ]
    }
   ],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads, n_layers, num_classes, patch_size, img_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Tokenizacja (Patching)\n",
    "        self.patch_embed = PatchEmbedding(patch_size=patch_size, emb_size=embed_dim, img_size=img_size)\n",
    "        \n",
    "        # Liczba patchy\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # 2. Token CLS (Klasyfikacyjny) - parametryzowany wektor\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        \n",
    "        # 3. Pozycje (Learnable) - dla wszystkich patchy + 1 (CLS)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 1 + self.num_patches, embed_dim))\n",
    "        \n",
    "        # 4. Transformer Encoder (Stack blok\u00f3w)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=n_heads, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # 5. G\u0142owica klasyfikuj\u0105ca (MLP Head)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, 3, 32, 32]\n",
    "        \n",
    "        # Embeddings\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Doklejamy CLS Token\n",
    "        batch_size = x.shape[0]\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1) # Kopiujemy dla ca\u0142ego batcha\n",
    "        x = torch.cat((cls_tokens, x), dim=1) # [Batch, N+1, Dim]\n",
    "        \n",
    "        # Dodajemy pozycje\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Transformer\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Klasyfikacja: Bierzemy tylko token 0 (CLS)\n",
    "        cls_output = x[:, 0]\n",
    "        \n",
    "        return self.mlp_head(cls_output)\n",
    "\n",
    "# Inicjalizacja modelu\n",
    "model = VisionTransformer(\n",
    "    embed_dim=EMBED_DIM,\n",
    "    n_heads=NUM_HEADS,\n",
    "    n_layers=NUM_LAYERS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    img_size=IMG_SIZE\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"ViT gotowy. Liczba parametr\u00f3w: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14c98b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start treningu ViT...\n",
      "Epoka 0, Batch 0, Loss: 2.4581\n",
      "Epoka 0, Batch 100, Loss: 2.1943\n",
      "Epoka 0, Batch 200, Loss: 1.9984\n",
      "Epoka 0, Batch 300, Loss: 2.1356\n",
      "Epoka 0, Batch 400, Loss: 2.0505\n",
      "Epoka 0, Batch 500, Loss: 2.1221\n",
      "Epoka 0, Batch 600, Loss: 1.7942\n",
      "Epoka 0, Batch 700, Loss: 1.8145\n",
      "Epoka 1, Batch 0, Loss: 1.8641\n",
      "Epoka 1, Batch 100, Loss: 2.0023\n",
      "Epoka 1, Batch 200, Loss: 1.9983\n",
      "Epoka 1, Batch 300, Loss: 1.8797\n",
      "Epoka 1, Batch 400, Loss: 1.9642\n",
      "Epoka 1, Batch 500, Loss: 2.0154\n",
      "Epoka 1, Batch 600, Loss: 1.9771\n",
      "Epoka 1, Batch 700, Loss: 1.8348\n",
      "\u2705 Trening zako\u0144czony.\n"
     ]
    }
   ],
   "source": [
    "# POBIERANIE DANYCH (CIFAR-10)\n",
    "# Obrazki: Samolot, Auto, Ptak, Kot...\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# TRENING (Szybki test - 2 epoki)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Start treningu ViT...\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(2): # Tylko 2 epoki, bo Transformer jest wolny na CPU\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoka {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\u2705 Trening zako\u0144czony.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab91906",
   "metadata": {},
   "source": [
    "## \ud83e\udde0 Podsumowanie: ViT vs CNN\n",
    "\n",
    "Co zauwa\u017cysz?\n",
    "ViT prawdopodobnie uczy si\u0119 **wolniej** lub daje gorsze wyniki na ma\u0142ym zbiorze (CIFAR-10) ni\u017c proste CNN.\n",
    "\n",
    "**Dlaczego?**\n",
    "*   **CNN** ma \"wrodzon\u0105 wiedz\u0119\" (Inductive Bias): wie, \u017ce piksele obok siebie s\u0105 wa\u017cne. To pomaga przy ma\u0142ych danych.\n",
    "*   **ViT** musi si\u0119 wszystkiego nauczy\u0107 od zera (nawet tego, co to znaczy \"by\u0107 obok siebie\").\n",
    "\n",
    "**Ale...**\n",
    "Gdy dasz ViT-owi 300 milion\u00f3w zdj\u0119\u0107 (zbi\u00f3r JFT-300M), ViT mia\u017cd\u017cy CNN. CNN dochodzi do szklanego sufitu, a ViT skaluje si\u0119 w niesko\u0144czono\u015b\u0107. Dlatego GPT-4V, Gemini i DALL-E u\u017cywaj\u0105 Transformer\u00f3w do obrazu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-handbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}