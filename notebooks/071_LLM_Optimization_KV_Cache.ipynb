{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/takzen/ai-engineering-handbook/blob/main/notebooks/071_LLM_Optimization_KV_Cache.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742f89dc",
   "metadata": {},
   "source": [
    "# \u26a1 LLM Optimization: KV Cache & Flash Attention\n",
    "\n",
    "Generowanie tekstu w modelach GPT jest procesem **autoregresyjnym**:\n",
    "1.  Wpisujesz \"Ala\".\n",
    "2.  Model liczy wszystko i zwraca \"ma\".\n",
    "3.  Wpisujesz \"Ala ma\".\n",
    "4.  Model liczy wszystko OD ZERA i zwraca \"kota\".\n",
    "\n",
    "To marnotrawstwo. Obliczenia dla \"Ala\" w kroku 4 s\u0105 identyczne jak w kroku 2.\n",
    "\n",
    "**Rozwi\u0105zanie: KV Cache.**\n",
    "Zamiast wyrzuca\u0107 wektory Key i Value dla poprzednich s\u0142\u00f3w, trzymamy je w pami\u0119ci (Cache).\n",
    "W nowym kroku obliczamy Attention tylko dla **jednego, nowego tokena** i doklejamy go do Cache'a.\n",
    "\n",
    "Z\u0142o\u017cono\u015b\u0107 obliczeniowa spada z $O(N^2)$ (dla ca\u0142ej sekwencji) do $O(N)$ (dla kroku)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d100aecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symulacja na: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Konfiguracja\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "D_MODEL = 512\n",
    "NUM_HEADS = 8\n",
    "HEAD_DIM = D_MODEL // NUM_HEADS\n",
    "SEQ_LEN = 100 # D\u0142ugo\u015b\u0107 generowanego tekstu\n",
    "\n",
    "print(f\"Symulacja na: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb7ca4c",
   "metadata": {},
   "source": [
    "## Implementacja Standardowa (Bez Cache)\n",
    "\n",
    "Najpierw zbudujmy \"g\u0142upi\u0105\" warstw\u0119 Attention, kt\u00f3ra za ka\u017cdym razem przelicza ca\u0142e zdanie od pocz\u0105tku.\n",
    "To jest to, co robi model podczas *treningu* (bo wtedy znamy ca\u0142e zdanie), ale podczas *generowania* jest to bardzo nieefektywne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172526f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Attention gotowe.\n"
     ]
    }
   ],
   "source": [
    "class StandardAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(D_MODEL, D_MODEL, bias=False)\n",
    "        self.W_k = nn.Linear(D_MODEL, D_MODEL, bias=False)\n",
    "        self.W_v = nn.Linear(D_MODEL, D_MODEL, bias=False)\n",
    "        self.out = nn.Linear(D_MODEL, D_MODEL, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [Batch, Seq_Len, D_Model]\n",
    "        batch, seq_len, _ = x.shape\n",
    "        \n",
    "        # 1. Projekcje Q, K, V\n",
    "        Q = self.W_q(x).view(batch, seq_len, NUM_HEADS, HEAD_DIM).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch, seq_len, NUM_HEADS, HEAD_DIM).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch, seq_len, NUM_HEADS, HEAD_DIM).transpose(1, 2)\n",
    "        \n",
    "        # 2. Scaled Dot-Product Attention\n",
    "        # (Tutaj PyTorch robi to za nas wydajnie, ale i tak liczy ca\u0142\u0105 macierz N*N)\n",
    "        attn_output = F.scaled_dot_product_attention(Q, K, V, is_causal=True)\n",
    "        \n",
    "        # 3. Scalenie g\u0142owic\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch, seq_len, D_MODEL)\n",
    "        return self.out(attn_output)\n",
    "\n",
    "print(\"Standard Attention gotowe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723b5d1",
   "metadata": {},
   "source": [
    "## Implementacja z KV Cache\n",
    "\n",
    "Teraz wersja sprytna.\n",
    "Metoda `forward` przyjmuje dodatkowy argument `kv_cache`.\n",
    "1.  Je\u015bli to pierwszy krok -> licz wszystko.\n",
    "2.  Je\u015bli to kolejny krok -> wej\u015bcie to tylko **ostatni token**.\n",
    "3.  Oblicz K i V tylko dla tego tokena.\n",
    "4.  Doklej do `kv_cache`.\n",
    "5.  U\u017cyj ca\u0142ego cache do policzenia Attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9741a858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached Attention gotowe.\n"
     ]
    }
   ],
   "source": [
    "class CachedAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(D_MODEL, D_MODEL, bias=False)\n",
    "        self.W_k = nn.Linear(D_MODEL, D_MODEL, bias=False)\n",
    "        self.W_v = nn.Linear(D_MODEL, D_MODEL, bias=False)\n",
    "        self.out = nn.Linear(D_MODEL, D_MODEL, bias=False)\n",
    "\n",
    "    def forward(self, x, kv_cache=None):\n",
    "        # x shape: [Batch, 1, D_Model] (Tylko NOWY token!)\n",
    "        batch, seq_len, _ = x.shape \n",
    "        \n",
    "        # 1. Projekcje (tylko dla nowego tokena)\n",
    "        q = self.W_q(x).view(batch, seq_len, NUM_HEADS, HEAD_DIM).transpose(1, 2)\n",
    "        k = self.W_k(x).view(batch, seq_len, NUM_HEADS, HEAD_DIM).transpose(1, 2)\n",
    "        v = self.W_v(x).view(batch, seq_len, NUM_HEADS, HEAD_DIM).transpose(1, 2)\n",
    "        \n",
    "        # 2. Obs\u0142uga Cache\n",
    "        if kv_cache is not None:\n",
    "            prev_k, prev_v = kv_cache\n",
    "            # Doklejamy nowe k i v do starych\n",
    "            k = torch.cat([prev_k, k], dim=2)\n",
    "            v = torch.cat([prev_v, v], dim=2)\n",
    "            \n",
    "        # Zapisujemy nowy cache na przysz\u0142o\u015b\u0107\n",
    "        new_cache = (k, v)\n",
    "        \n",
    "        # 3. Attention\n",
    "        # Q ma d\u0142ugo\u015b\u0107 1 (nowy token). K i V maj\u0105 d\u0142ugo\u015b\u0107 ca\u0142ej historii.\n",
    "        # Dzi\u0119ki temu nowy token \"patrzy\" na wszystkich poprzednik\u00f3w.\n",
    "        attn_output = F.scaled_dot_product_attention(q, k, v, is_causal=False) # Causal niepotrzebny, bo Q to 1 token\n",
    "        \n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch, seq_len, D_MODEL)\n",
    "        return self.out(attn_output), new_cache\n",
    "\n",
    "print(\"Cached Attention gotowe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2089aeb",
   "metadata": {},
   "source": [
    "## Wy\u015bcig: Generowanie Tekstu\n",
    "\n",
    "Zasymulujemy proces generowania 100 kolejnych token\u00f3w.\n",
    "*   **Standard:** W ka\u017cdej p\u0119tli podajemy ca\u0142\u0105 histori\u0119 (`input_ids`).\n",
    "*   **Cached:** W ka\u017cdej p\u0119tli podajemy tylko nowy token i cache.\n",
    "\n",
    "Zmierzymy czas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e22c998c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Time: 0.1483 s\n",
      "Cached Time:   0.0291 s\n",
      "\ud83d\ude80 Przyspieszenie: 5.09x\n"
     ]
    }
   ],
   "source": [
    "# Inicjalizacja modeli\n",
    "model_std = StandardAttention().to(DEVICE).eval()\n",
    "model_cached = CachedAttention().to(DEVICE).eval()\n",
    "\n",
    "# Dummy input (Startujemy od 10 token\u00f3w)\n",
    "initial_input = torch.randn(1, 10, D_MODEL).to(DEVICE)\n",
    "\n",
    "# --- TEST 1: STANDARD ---\n",
    "start_time = time.time()\n",
    "current_input = initial_input.clone()\n",
    "\n",
    "for _ in range(SEQ_LEN):\n",
    "    with torch.no_grad():\n",
    "        # Musimy poda\u0107 CA\u0141\u0104 histori\u0119\n",
    "        out = model_std(current_input)\n",
    "        # Bierzemy ostatni wektor jako \"nowy token\" (symulacja)\n",
    "        next_token = out[:, -1:, :] \n",
    "        # Doklejamy do wej\u015bcia\n",
    "        current_input = torch.cat([current_input, next_token], dim=1)\n",
    "\n",
    "time_std = time.time() - start_time\n",
    "print(f\"Standard Time: {time_std:.4f} s\")\n",
    "\n",
    "\n",
    "# --- TEST 2: KV CACHE ---\n",
    "start_time = time.time()\n",
    "current_token = initial_input[:, -1:, :].clone() # Tylko ostatni na start\n",
    "# Najpierw musimy \"nape\u0142ni\u0107\" cache histori\u0105 (Prefill phase)\n",
    "# W uproszczeniu: pomijamy prefill i zak\u0142adamy, \u017ce startujemy od zera albo robimy pass na initial_input\n",
    "# Zr\u00f3bmy poprawny start:\n",
    "cache = None\n",
    "# 1. Prefill (przetwarzamy prompt)\n",
    "with torch.no_grad():\n",
    "    _, cache = model_cached(initial_input, kv_cache=None)\n",
    "\n",
    "# 2. Generation loop\n",
    "for _ in range(SEQ_LEN):\n",
    "    with torch.no_grad():\n",
    "        # Podajemy TYLKO ostatni token i cache\n",
    "        out, cache = model_cached(current_token, kv_cache=cache)\n",
    "        next_token = out # To ju\u017c jest tylko 1 token\n",
    "        current_token = next_token\n",
    "\n",
    "time_cache = time.time() - start_time\n",
    "print(f\"Cached Time:   {time_cache:.4f} s\")\n",
    "\n",
    "speedup = time_std / time_cache\n",
    "print(f\"\ud83d\ude80 Przyspieszenie: {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9104d985",
   "metadata": {},
   "source": [
    "## Czym jest Flash Attention? (Teoria)\n",
    "\n",
    "Przyspieszyli\u015bmy obliczenia (mniej FLOPs). Ale jest jeszcze problem **pami\u0119ci (VRAM)**.\n",
    "Przy d\u0142ugich sekwencjach (np. 100k token\u00f3w), macierz uwagi $N \\times N$ nie mie\u015bci si\u0119 w pami\u0119ci GPU.\n",
    "\n",
    "**Flash Attention (2022/2023):**\n",
    "To in\u017cynierski majstersztyk na poziomie sprz\u0119towym (CUDA).\n",
    "GPU ma dwa rodzaje pami\u0119ci:\n",
    "1.  **HBM (High Bandwidth Memory):** Wielka, ale wolna (jak lod\u00f3wka w kuchni).\n",
    "2.  **SRAM (Static RAM):** Malutka, ale superszybka (jak deska do krojenia).\n",
    "\n",
    "Tradycyjne Attention ci\u0105gle przenosi macierze z HBM do SRAM i z powrotem.\n",
    "**Flash Attention** u\u017cywa techniki **Tiling (Kafelkowanie)**. Dzieli macierz na ma\u0142e klocki, kt\u00f3re mieszcz\u0105 si\u0119 w SRAM, liczy wszystko \"na desce do krojenia\" i odsy\u0142a do \"lod\u00f3wki\" tylko gotowy wynik.\n",
    "\n",
    "Dzi\u0119ki temu jest:\n",
    "1.  Szybsze (mniej czekania na dane).\n",
    "2.  Liniowe pami\u0119ciowo (nie tworzy gigantycznej macierzy $N \\times N$).\n",
    "\n",
    "W PyTorch 2.0+ funkcja `F.scaled_dot_product_attention` automatycznie u\u017cywa Flash Attention, je\u015bli masz odpowiedni\u0105 kart\u0119 graficzn\u0105!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847fc2af",
   "metadata": {},
   "source": [
    "## \ud83e\udde0 Podsumowanie: Dlaczego to kluczowe?\n",
    "\n",
    "1.  **KV Cache** jest obowi\u0105zkowe przy generowaniu tekstu (Inference). Bez tego ChatGPT generowa\u0142by jedno zdanie minut\u0119.\n",
    "    *   *Koszt:* Zu\u017cywa VRAM (pami\u0119\u0107 karty) na przechowywanie kluczy i warto\u015bci. Im d\u0142u\u017csza rozmowa, tym wi\u0119cej pami\u0119ci zajmuje cache.\n",
    "2.  **Flash Attention** pozwala na obs\u0142ug\u0119 **d\u0142ugich kontekst\u00f3w** (np. GPT-4 Turbo 128k, Claude 200k). Bez tego macierz uwagi po prostu by si\u0119 nie zmie\u015bci\u0142a w pami\u0119ci.\n",
    "\n",
    "Jako in\u017cynier AI, musisz wiedzie\u0107, \u017ce **\"Memory is the bottleneck\"**. Wi\u0119kszo\u015b\u0107 nowoczesnych optymalizacji (PagedAttention w vLLM) polega na lepszym zarz\u0105dzaniu KV Cachem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-handbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}