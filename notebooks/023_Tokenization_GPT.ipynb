{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/takzen/ai-engineering-handbook/blob/main/notebooks/023_Tokenization_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcca121f",
   "metadata": {},
   "source": [
    "# \ud83e\udde9 Tokenizacja: Dlaczego AI nie umie literowa\u0107?\n",
    "\n",
    "Dla cz\u0142owieka zdanie *\"Kocham AI\"* to dwa s\u0142owa i 8 liter (plus spacja).\n",
    "Dla komputera to ci\u0105g liczb.\n",
    "\n",
    "Mamy trzy podej\u015bcia do ci\u0119cia tekstu:\n",
    "1.  **Char-level:** Ka\u017cda litera to osobna liczba (a=1, b=2).\n",
    "    *   *Wada:* Teksty s\u0105 strasznie d\u0142ugie. Model gubi w\u0105tek.\n",
    "2.  **Word-level:** Ka\u017cde s\u0142owo to liczba (dom=500, kot=200).\n",
    "    *   *Wada:* Co ze s\u0142owami \"domku\", \"domkiem\", \"przydomowy\"? S\u0142ownik robi si\u0119 gigantyczny.\n",
    "3.  **Sub-word (BPE - Byte Pair Encoding):** Z\u0142oty \u015brodek.\n",
    "    *   Cz\u0119ste s\u0142owa (\"the\", \"and\", \"jest\") s\u0105 jednym kawa\u0142kiem.\n",
    "    *   Rzadkie s\u0142owa (\"przydomowy\") s\u0105 sklejane z kawa\u0142k\u00f3w (\"przy\", \"dom\", \"owy\").\n",
    "\n",
    "To standard w dzisiejszym AI (GPT, Llama, Claude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b08996a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enkoder za\u0142adowany. Zobaczmy, jak GPT-4 widzi \u015bwiat.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.2 environment at: venv\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m7 packages\u001b[0m \u001b[2min 772ms\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 942ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 13ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2025.11.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtiktoken\u001b[0m\u001b[2m==0.12.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Najpierw musimy zainstalowa\u0107 bibliotek\u0119 od OpenAI\n",
    "# Odkomentuj lini\u0119 poni\u017cej, je\u015bli uruchamiasz to pierwszy raz:\n",
    "!uv pip install tiktoken\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# \u0141adujemy \"s\u0142ownik\" kt\u00f3rego u\u017cywa GPT-4\n",
    "# cl100k_base to nazwa kodowania dla GPT-3.5 i GPT-4\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "print(\"Enkoder za\u0142adowany. Zobaczmy, jak GPT-4 widzi \u015bwiat.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a17882a",
   "metadata": {},
   "source": [
    "## Eksperyment 1: S\u0142owo vs Token\n",
    "\n",
    "Sprawd\u017amy proste zdanie. Zobaczysz, \u017ce czasem token to ca\u0142e s\u0142owo (wraz ze spacj\u0105!), a czasem kawa\u0142ek s\u0142owa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c43ee87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tekst oryginalny: 'Sztuczna inteligencja jest super'\n",
      "Tokeny (ID): [50, 11949, 98667, 3458, 90745, 967, 5697, 13599, 2307]\n",
      "Liczba s\u0142\u00f3w: 4\n",
      "Liczba token\u00f3w: 9\n",
      "------------------------------\n",
      "JAK GPT WIDZI TE KAWA\u0141KI?\n",
      "ID 50     -> 'S'\n",
      "ID 11949  -> 'zt'\n",
      "ID 98667  -> 'ucz'\n",
      "ID 3458   -> 'na'\n",
      "ID 90745  -> ' intelig'\n",
      "ID 967    -> 'enc'\n",
      "ID 5697   -> 'ja'\n",
      "ID 13599  -> ' jest'\n",
      "ID 2307   -> ' super'\n"
     ]
    }
   ],
   "source": [
    "text = \"Sztuczna inteligencja jest super\"\n",
    "\n",
    "# 1. Zamieniamy tekst na liczby (Tokeny)\n",
    "tokens = encoding.encode(text)\n",
    "\n",
    "print(f\"Tekst oryginalny: '{text}'\")\n",
    "print(f\"Tokeny (ID): {tokens}\")\n",
    "print(f\"Liczba s\u0142\u00f3w: {len(text.split())}\")\n",
    "print(f\"Liczba token\u00f3w: {len(tokens)}\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"JAK GPT WIDZI TE KAWA\u0141KI?\")\n",
    "\n",
    "# 2. Zamieniamy liczby z powrotem na tekst, kawa\u0142ek po kawa\u0142ku\n",
    "for t in tokens:\n",
    "    decoded = encoding.decode([t])\n",
    "    print(f\"ID {t:<6} -> '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331a532a",
   "metadata": {},
   "source": [
    "## Eksperyment 2: Problem \"Strawberry\" (i matematyki)\n",
    "\n",
    "Dlaczego ChatGPT myli si\u0119 w prostych zadaniach?\n",
    "Sp\u00f3jrzmy na s\u0142owo \"Strawberry\" i dzia\u0142anie matematyczne.\n",
    "\n",
    "Je\u015bli model widzi token \"berry\" jako jedn\u0105 ca\u0142o\u015b\u0107 (jedn\u0105 liczb\u0119), to **nie ma fizycznej mo\u017cliwo\u015bci**, \u017ceby wiedzia\u0142, ile tam jest liter \"r\", chyba \u017ce nauczy\u0142 si\u0119 tego na pami\u0119\u0107. On nie widzi liter w \u015brodku tokena!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45a7bafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ANALIZA TRUDNYCH S\u0141\u00d3W ---\n",
      "S\u0142owo: 'Strawberry'\n",
      "Tokeny: ['Str', 'aw', 'berry']\n",
      "ID:     [2645, 675, 15717]\n",
      "--------------------\n",
      "S\u0142owo: '9.11'\n",
      "Tokeny: ['9', '.', '11']\n",
      "ID:     [24, 13, 806]\n",
      "--------------------\n",
      "S\u0142owo: '9.9'\n",
      "Tokeny: ['9', '.', '9']\n",
      "ID:     [24, 13, 24]\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "tricky_words = [\"Strawberry\", \"9.11\", \"9.9\"]\n",
    "\n",
    "print(\"--- ANALIZA TRUDNYCH S\u0141\u00d3W ---\")\n",
    "\n",
    "for word in tricky_words:\n",
    "    ids = encoding.encode(word)\n",
    "    parts = [encoding.decode([id]) for id in ids]\n",
    "    \n",
    "    print(f\"S\u0142owo: '{word}'\")\n",
    "    print(f\"Tokeny: {parts}\")\n",
    "    print(f\"ID:     {ids}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "# ANALIZA WYNIK\u00d3W (Dla Twojej informacji):\n",
    "# 1. \"Strawberry\" -> Cz\u0119sto dzieli si\u0119 na ['Str', 'aw', 'berry'].\n",
    "#    Model widzi 3 klocki. W klocku 'berry' s\u0105 dwa 'r', ale model widzi to jako jeden symbol.\n",
    "# 2. \"9.11\" vs \"9.9\" ->\n",
    "#    9.9 to cz\u0119sto jeden token.\n",
    "#    9.11 to czasem dwa tokeny ['9.', '11'].\n",
    "#    Dlatego modele s\u0142abo licz\u0105 na liczbach zmiennoprzecinkowych - traktuj\u0105 je jak tekst!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f372fbd4",
   "metadata": {},
   "source": [
    "## Eksperyment 3: J\u0119zyk ma znaczenie (Angielski vs Polski)\n",
    "\n",
    "Tokenizatory s\u0105 trenowane g\u0142\u00f3wnie na angielskim internecie.\n",
    "To sprawia, \u017ce angielski jest \"ta\u0144szy\" (mniej token\u00f3w na zdanie) i model lepiej go rozumie.\n",
    "Polski jest \"dro\u017cszy\" (s\u0142owa s\u0105 szatkowane na wi\u0119cej drobnych kawa\u0142k\u00f3w, bo maj\u0105 ko\u0144c\u00f3wki fleksyjne).\n",
    "\n",
    "**P\u0142acisz za tokeny, nie za s\u0142owa!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84ebf537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angielski: 'I verify that the calculations are correct.'\n",
      "Liczba token\u00f3w: 8\n",
      "Tokeny: ['I', ' verify', ' that', ' the', ' calculations', ' are', ' correct', '.']\n",
      "\n",
      "\n",
      "Polski: 'Potwierdzam, \u017ce obliczenia s\u0105 poprawne.'\n",
      "Liczba token\u00f3w: 15\n",
      "Tokeny: ['Pot', 'w', 'ier', 'dz', 'am', ',', ' \u017ce', ' ob', 'lic', 'zenia', ' s\u0105', ' pop', 'raw', 'ne', '.']\n",
      "------------------------------\n",
      "Wniosek: To samo zdanie w j\u0119zyku polskim zajmuje 88% wi\u0119cej token\u00f3w (pami\u0119ci i pieni\u0119dzy)!\n"
     ]
    }
   ],
   "source": [
    "text_en = \"I verify that the calculations are correct.\"\n",
    "text_pl = \"Potwierdzam, \u017ce obliczenia s\u0105 poprawne.\"\n",
    "\n",
    "tokens_en = encoding.encode(text_en)\n",
    "tokens_pl = encoding.encode(text_pl)\n",
    "\n",
    "print(f\"Angielski: '{text_en}'\")\n",
    "print(f\"Liczba token\u00f3w: {len(tokens_en)}\")\n",
    "print(f\"Tokeny: {[encoding.decode([t]) for t in tokens_en]}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Polski: '{text_pl}'\")\n",
    "print(f\"Liczba token\u00f3w: {len(tokens_pl)}\")\n",
    "print(f\"Tokeny: {[encoding.decode([t]) for t in tokens_pl]}\")\n",
    "\n",
    "diff = (len(tokens_pl) - len(tokens_en)) / len(tokens_en) * 100\n",
    "print(\"-\" * 30)\n",
    "print(f\"Wniosek: To samo zdanie w j\u0119zyku polskim zajmuje {diff:.0f}% wi\u0119cej token\u00f3w (pami\u0119ci i pieni\u0119dzy)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdc315a",
   "metadata": {},
   "source": [
    "## \ud83e\udde0 Podsumowanie: \u015awiat z Klock\u00f3w Lego\n",
    "\n",
    "Co musisz zapami\u0119ta\u0107 jako AI Engineer?\n",
    "\n",
    "1.  **Token $\\neq$ S\u0142owo.** \u015arednio 1000 s\u0142\u00f3w to oko\u0142o 1300 token\u00f3w (w angielskim). W polskim ta r\u00f3\u017cnica jest jeszcze wi\u0119ksza.\n",
    "2.  **\u015alepota modelu.** Model widzi tokeny jako niepodzielne klocki. Je\u015bli token `12345` oznacza \"Strawberry\", to model nie wie, z jakich liter si\u0119 sk\u0142ada, dop\u00f3ki nie zobaczy token\u00f3w literowych `S-t-r-a-w...`.\n",
    "3.  **Optymalizacja koszt\u00f3w.** Pisz\u0105c prompty, pami\u0119taj, \u017ce j\u0119zyk polski zjada limit kontekstu szybciej ni\u017c angielski. Czasami warto przet\u0142umaczy\u0107 instrukcj\u0119 systemow\u0105 na angielski, nawet je\u015bli bot ma odpowiada\u0107 po polsku.\n",
    "\n",
    "**Tu jest haczyk.**\n",
    "Dlatego GPT-4 jest lepsze w kodowaniu ni\u017c w pisaniu wierszy rymowanych. Kod ma jasn\u0105 struktur\u0119 token\u00f3w. Rymy wymagaj\u0105 s\u0142yszenia brzmienia ko\u0144c\u00f3wek s\u0142\u00f3w, a tokenizator cz\u0119sto te ko\u0144c\u00f3wki ucina w dziwnych miejscach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}