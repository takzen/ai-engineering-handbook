{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n<a href=\"https://colab.research.google.com/github/takzen/ai-engineering-handbook/blob/main/notebooks/080_Speculative_Decoding.ipynb\" target=\"_parent\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n<a href=\"https://colab.research.google.com/github/takzen/ai-engineering-handbook/blob/main/80_Speculative_Decoding.ipynb\" target=\"_parent\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8b485d",
   "metadata": {},
   "source": [
    "# \u23e9 Speculative Decoding: Jak przyspieszy\u0107 LLM 2x bez utraty jako\u015bci?\n",
    "\n",
    "Wielkie modele (Target Model) s\u0105 ograniczone przez **Memory Bandwidth** (przepustowo\u015b\u0107 pami\u0119ci). Za\u0142adowanie wag trwa d\u0142u\u017cej ni\u017c same obliczenia.\n",
    "Dlatego uruchomienie modelu dla 1 tokena kosztuje tyle samo czasu, co dla 5 token\u00f3w (je\u015bli robimy to r\u00f3wnolegle).\n",
    "\n",
    "**Algorytm Spekulacyjny:**\n",
    "1.  **Draft:** Ma\u0142y model (np. GPT-2) generuje szybko `K` token\u00f3w (np. \"Ala ma kota\").\n",
    "2.  **Verify:** Du\u017cy model (np. GPT-4) dostaje ten ci\u0105g i liczy prawdopodobie\u0144stwa dla wszystkich pozycji naraz.\n",
    "3.  **Accept/Reject:** Sprawdzamy, czy Du\u017cy Model zgadza si\u0119 z Ma\u0142ym.\n",
    "    *   Akceptujemy prefiks, kt\u00f3ry jest zgodny.\n",
    "    *   Reszt\u0119 odrzucamy i generujemy poprawny token z Du\u017cego Modelu.\n",
    "\n",
    "Zrobimy symulacj\u0119 tego procesu, u\u017cywaj\u0105c sztucznych op\u00f3\u017anie\u0144 (`time.sleep`), \u017ceby pokaza\u0107 zysk czasowy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1b3d553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zdanie docelowe: ['Sztuczna', 'inteligencja', 'zmienia', '\u015bwiat', 'na', 'lepsze']\n",
      "Draft jest 10x szybszy od Targetu.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Konfiguracja symulacji\n",
    "# Target Model jest m\u0105dry (dok\u0142adny), ale wolny\n",
    "TARGET_TIME_PER_TOKEN = 0.1  # 100 ms\n",
    "# Draft Model jest g\u0142upi (cz\u0119sto si\u0119 myli), ale szybki\n",
    "DRAFT_TIME_PER_TOKEN = 0.01  # 10 ms\n",
    "\n",
    "# D\u0142ugo\u015b\u0107 spekulacji (ile token\u00f3w zgadujemy naraz?)\n",
    "K_LOOKAHEAD = 4 \n",
    "\n",
    "# Nasze \"S\u0142ownictwo\" (uproszczone)\n",
    "target_sentence = \"Sztuczna inteligencja zmienia \u015bwiat na lepsze\".split()\n",
    "vocab = {word: i for i, word in enumerate(target_sentence)}\n",
    "vocab_inv = {i: word for word, i in vocab.items()}\n",
    "\n",
    "print(f\"Zdanie docelowe: {target_sentence}\")\n",
    "print(f\"Draft jest {TARGET_TIME_PER_TOKEN / DRAFT_TIME_PER_TOKEN:.0f}x szybszy od Targetu.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b63a578",
   "metadata": {},
   "source": [
    "## Symulacja Modeli\n",
    "\n",
    "Stworzymy dwie funkcje:\n",
    "1.  `target_model`: Zawsze zwraca poprawne s\u0142owo, ale \"my\u015bli\" d\u0142ugo.\n",
    "2.  `draft_model`: My\u015bli szybko, ale ma np. 70% szans na trafienie (symulacja mniejszej inteligencji)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "323648c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modele gotowe.\n"
     ]
    }
   ],
   "source": [
    "class MockModel:\n",
    "    def __init__(self, name, latency, accuracy, true_sentence):\n",
    "        self.name = name\n",
    "        self.latency = latency\n",
    "        self.accuracy = accuracy\n",
    "        self.true_sentence = true_sentence\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Zwraca nast\u0119pny token (lub list\u0119 token\u00f3w).\n",
    "        \"\"\"\n",
    "        # Symulacja czasu oblicze\u0144\n",
    "        time.sleep(self.latency)\n",
    "        \n",
    "        current_len = len(input_ids)\n",
    "        if current_len >= len(self.true_sentence):\n",
    "            return \"<EOS>\" # Koniec zdania\n",
    "        \n",
    "        # Prawdziwy token, kt\u00f3ry powinien by\u0107\n",
    "        true_token = self.true_sentence[current_len]\n",
    "        \n",
    "        # Czy model zgad\u0142?\n",
    "        if random.random() < self.accuracy:\n",
    "            return true_token\n",
    "        else:\n",
    "            # Pomy\u0142ka (losowe s\u0142owo ze s\u0142ownika)\n",
    "            return random.choice(list(vocab.keys()))\n",
    "\n",
    "# Inicjalizacja\n",
    "# Target: 100% dok\u0142adno\u015bci (to nasza wyrocznia), wolny\n",
    "target_model = MockModel(\"Target (Big)\", TARGET_TIME_PER_TOKEN, 1.0, target_sentence)\n",
    "\n",
    "# Draft: 70% dok\u0142adno\u015bci, szybki\n",
    "draft_model = MockModel(\"Draft (Small)\", DRAFT_TIME_PER_TOKEN, 0.7, target_sentence)\n",
    "\n",
    "print(\"Modele gotowe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc73e2b2",
   "metadata": {},
   "source": [
    "## Metoda 1: Standardowa Generacja (Autoregresyjna)\n",
    "\n",
    "To jest spos\u00f3b, w jaki normalnie dzia\u0142a ChatGPT. Jeden token po drugim, u\u017cywaj\u0105c tylko du\u017cego modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d720ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STANDARD GENERATION ---\n",
      "Generated: Sztuczna\n",
      "Generated: inteligencja\n",
      "Generated: zmienia\n",
      "Generated: \u015bwiat\n",
      "Generated: na\n",
      "Generated: lepsze\n",
      "\n",
      "Czas Standardowy: 0.6032 s\n",
      "Pr\u0119dko\u015b\u0107: 9.95 tokens/s\n"
     ]
    }
   ],
   "source": [
    "def standard_generation():\n",
    "    current_text = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"--- STANDARD GENERATION ---\")\n",
    "    while len(current_text) < len(target_sentence):\n",
    "        # Generujemy 1 token Du\u017cym Modelem\n",
    "        token = target_model.forward(current_text)\n",
    "        current_text.append(token)\n",
    "        print(f\"Generated: {token}\")\n",
    "        \n",
    "    total_time = time.time() - start_time\n",
    "    return total_time, len(current_text)\n",
    "\n",
    "time_std, tokens_std = standard_generation()\n",
    "print(f\"\\nCzas Standardowy: {time_std:.4f} s\")\n",
    "print(f\"Pr\u0119dko\u015b\u0107: {tokens_std / time_std:.2f} tokens/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abb0801",
   "metadata": {},
   "source": [
    "## Metoda 2: Speculative Decoding\n",
    "\n",
    "Teraz algorytm spekulacyjny:\n",
    "1.  P\u0119tla g\u0142\u00f3wna.\n",
    "2.  **Krok Draftu:** Ma\u0142y model generuje `K` token\u00f3w jeden po drugim (szybko).\n",
    "3.  **Krok Weryfikacji:** Du\u017cy model sprawdza te `K` token\u00f3w + 1 dodatkowy (r\u00f3wnolegle).\n",
    "    *   *Uwaga: W naszej symulacji Pythona \"r\u00f3wnoleg\u0142o\u015b\u0107\" symulujemy tym, \u017ce `target_model` sprawdza list\u0119 token\u00f3w w tym samym czasie co pojedynczy token (bo koszt to g\u0142\u00f3wnie za\u0142adowanie wag).*\n",
    "4.  **Akceptacja:** Bierzemy tyle token\u00f3w, ile si\u0119 zgadza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a09fcc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SPECULATIVE GENERATION ---\n",
      "Draft: ['Sztuczna', 'inteligencja', '\u015bwiat', '\u015bwiat'] -> Zaakceptowano: ['Sztuczna', 'inteligencja', 'zmienia'] \u2705\n",
      "Draft: ['\u015bwiat', 'na', 'lepsze'] -> Zaakceptowano: ['\u015bwiat', 'na', 'lepsze'] \u2705\n",
      "\n",
      "Czas Spekulacyjny: 0.2736 s\n",
      "Pr\u0119dko\u015b\u0107: 21.93 tokens/s\n"
     ]
    }
   ],
   "source": [
    "def speculative_generation():\n",
    "    current_text = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"--- SPECULATIVE GENERATION ---\")\n",
    "    \n",
    "    while len(current_text) < len(target_sentence):\n",
    "        # 1. DRAFT PHASE (Ma\u0142y model zgaduje K razy)\n",
    "        draft_tokens = []\n",
    "        temp_context = current_text.copy()\n",
    "        \n",
    "        for _ in range(K_LOOKAHEAD):\n",
    "            if len(temp_context) >= len(target_sentence): break\n",
    "            token = draft_model.forward(temp_context)\n",
    "            draft_tokens.append(token)\n",
    "            temp_context.append(token)\n",
    "            \n",
    "        # 2. VERIFICATION PHASE (Du\u017cy model sprawdza)\n",
    "        # W prawdziwym GPU to dzieje si\u0119 w JEDNYM przebiegu (batch)\n",
    "        # Symulujemy koszt jednego uruchomienia du\u017cego modelu\n",
    "        time.sleep(TARGET_TIME_PER_TOKEN) \n",
    "        \n",
    "        # Sprawdzamy poprawno\u015b\u0107 (Verify)\n",
    "        # Target model generuje \"swoj\u0105 wersj\u0119\" dla ka\u017cdego kroku w drafcie\n",
    "        # (W prawdziwym \u015bwiecie liczy prawdopodobie\u0144stwa P(x))\n",
    "        accepted_tokens = []\n",
    "        for i, draft_tok in enumerate(draft_tokens):\n",
    "            # Jaki token powinien by\u0107 na tej pozycji wg Targetu?\n",
    "            true_pos = len(current_text) + i\n",
    "            if true_pos >= len(target_sentence): break\n",
    "            \n",
    "            target_tok = target_sentence[true_pos]\n",
    "            \n",
    "            if draft_tok == target_tok:\n",
    "                accepted_tokens.append(draft_tok)\n",
    "            else:\n",
    "                # B\u0142\u0105d! Odrzucamy reszt\u0119 draftu\n",
    "                # Target model \"poprawia\" ten jeden b\u0142\u0105d\n",
    "                accepted_tokens.append(target_tok)\n",
    "                break # Przerywamy akceptacj\u0119\n",
    "        \n",
    "        # Aktualizacja tekstu\n",
    "        current_text.extend(accepted_tokens)\n",
    "        \n",
    "        # Logowanie\n",
    "        status = \"\u2705\" if len(accepted_tokens) > 1 else \"\u274c\"\n",
    "        print(f\"Draft: {draft_tokens} -> Zaakceptowano: {accepted_tokens} {status}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    return total_time, len(current_text)\n",
    "\n",
    "time_spec, tokens_spec = speculative_generation()\n",
    "print(f\"\\nCzas Spekulacyjny: {time_spec:.4f} s\")\n",
    "print(f\"Pr\u0119dko\u015b\u0107: {tokens_spec / time_spec:.2f} tokens/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f4347c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Standard: 0.6032s\n",
      "Speculative: 0.2736s\n",
      "\ud83d\ude80 PRZYSPIESZENIE: 2.20x\n",
      "Sukces! Ma\u0142y model skutecznie pom\u00f3g\u0142 du\u017cemu.\n"
     ]
    }
   ],
   "source": [
    "# PODSUMOWANIE\n",
    "speedup = time_std / time_spec\n",
    "print(\"-\" * 30)\n",
    "print(f\"Standard: {time_std:.4f}s\")\n",
    "print(f\"Speculative: {time_spec:.4f}s\")\n",
    "print(f\"\ud83d\ude80 PRZYSPIESZENIE: {speedup:.2f}x\")\n",
    "\n",
    "if speedup > 1.0:\n",
    "    print(\"Sukces! Ma\u0142y model skutecznie pom\u00f3g\u0142 du\u017cemu.\")\n",
    "else:\n",
    "    print(\"Pora\u017cka. Ma\u0142y model myli\u0142 si\u0119 zbyt cz\u0119sto (narzut czasowy draftu zjad\u0142 zysk).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c7d947",
   "metadata": {},
   "source": [
    "## \ud83e\udde0 Podsumowanie: Hazard, kt\u00f3ry si\u0119 op\u0142aca\n",
    "\n",
    "Je\u015bli ma\u0142y model ma skuteczno\u015b\u0107 (Acceptance Rate) powy\u017cej ~50-60%, spekulacja si\u0119 op\u0142aca.\n",
    "Je\u015bli ma\u0142y model jest beznadziejny, spekulacja tylko spowalnia (bo tracimy czas na draft, kt\u00f3ry i tak idzie do kosza).\n",
    "\n",
    "**Dlaczego to dzia\u0142a na GPU?**\n",
    "Uruchomienie modelu GPT-4 dla sekwencji o d\u0142ugo\u015bci 1 (input) kosztuje np. 50ms.\n",
    "Uruchomienie go dla sekwencji o d\u0142ugo\u015bci 5 (input) te\u017c kosztuje ok. 50-55ms.\n",
    "Dzi\u0119ki temu **weryfikacja 5 token\u00f3w jest prawie darmowa** w por\u00f3wnaniu do generowania ich pojedynczo.\n",
    "\n",
    "To obecnie standardowa technika w bibliotekach takich jak **vLLM** czy **HuggingFace TGI**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-handbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}