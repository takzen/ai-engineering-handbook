{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3a22544",
   "metadata": {},
   "source": [
    "# ‚è≥ TFT: Transformer do zada≈Ñ specjalnych (Time Series)\n",
    "\n",
    "Standardowy Transformer (GPT) traktuje wszystko jako tekst.\n",
    "TFT jest zaprojektowany specjalnie dla liczb i czasu.\n",
    "\n",
    "RozwiƒÖzuje problem **Heterogenicznych Danych**:\n",
    "1.  **Zmienne statyczne:** (ID sklepu, lokalizacja) -> Nie zmieniajƒÖ siƒô w czasie.\n",
    "2.  **Zmienne dynamiczne znane:** (Dzie≈Ñ tygodnia, ≈öwiƒôta) -> Znamy je na rok do przodu.\n",
    "3.  **Zmienne dynamiczne nieznane:** (Sprzeda≈º) -> Znamy tylko przesz≈Ço≈õƒá.\n",
    "\n",
    "**Kluczowa innowacja: Gating (Bramkowanie).**\n",
    "Wiƒôkszo≈õƒá sieci neuronowych to \"czarne skrzynki\". TFT u≈ºywa mechanizmu **GLU (Gated Linear Unit)**, kt√≥ry dzia≈Ça jak kran. Mo≈ºe ca≈Çkowicie odciƒÖƒá dop≈Çyw informacji z danej kolumny, je≈õli uzna jƒÖ za szum.\n",
    "\n",
    "Zbudujemy od zera serce TFT: **Gated Residual Network (GRN)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b5be9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UrzƒÖdzenie: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Konfiguracja\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "HIDDEN_DIM = 64  # Rozmiar ukryty (dla ka≈ºdego feature'a)\n",
    "DROPOUT = 0.1\n",
    "\n",
    "print(f\"UrzƒÖdzenie: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90df7c",
   "metadata": {},
   "source": [
    "## Krok 1: GLU (Gated Linear Unit)\n",
    "\n",
    "To prosty, ale genialny mechanizm.\n",
    "$$ GLU(x) = \\sigma(W_1 x + b_1) \\odot (W_2 x + b_2) $$\n",
    "\n",
    "*   Czƒô≈õƒá prawa ($W_2 x$): Przetwarza dane (Informacja).\n",
    "*   Czƒô≈õƒá lewa ($\\sigma(...)$): Sigmoid zwraca warto≈õci 0-1 (Bramka).\n",
    "\n",
    "Mno≈ºymy Informacjƒô przez Bramkƒô. Je≈õli Bramka = 0, informacja znika."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e58ca468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wej≈õcie: torch.Size([5, 64])\n",
      "Wyj≈õcie: torch.Size([5, 64]) (Wymiar zachowany, ale przefiltrowany)\n"
     ]
    }
   ],
   "source": [
    "class GLU(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        # Wersja PyTorchowa GLU oczekuje wej≈õcia 2x wiƒôkszego, \n",
    "        # bo dzieli je na p√≥≈Ç (jedna po≈Çowa to dane, druga to bramka).\n",
    "        self.linear = nn.Linear(input_dim, input_dim * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Dim]\n",
    "        val = self.linear(x)\n",
    "        # F.glu dzieli tensor na p√≥≈Ç i robi: A * sigmoid(B)\n",
    "        return F.glu(val, dim=-1)\n",
    "\n",
    "# Test\n",
    "glu = GLU(HIDDEN_DIM)\n",
    "dummy = torch.randn(5, HIDDEN_DIM)\n",
    "out = glu(dummy)\n",
    "print(f\"Wej≈õcie: {dummy.shape}\")\n",
    "print(f\"Wyj≈õcie: {out.shape} (Wymiar zachowany, ale przefiltrowany)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f8b6c",
   "metadata": {},
   "source": [
    "## Krok 2: GRN (Gated Residual Network)\n",
    "\n",
    "To jest podstawowy klocek TFT (u≈ºywany wszƒôdzie).\n",
    "Sk≈Çada siƒô z:\n",
    "1.  **Skip Connection:** Orygina≈Ç dodawany na ko≈Ñcu (pamiƒôtasz ResNet?).\n",
    "2.  **LayerNorm:** Stabilizacja.\n",
    "3.  **Dwie warstwy Linear + ELU:** Nieliniowe przetwarzanie.\n",
    "4.  **GLU:** Bramkowanie na ko≈Ñcu.\n",
    "5.  **Context (Optional):** GRN mo≈ºe przyjmowaƒá dodatkowy wektor kontekstu (np. \"To jest Sklep nr 5\"), kt√≥ry wp≈Çywa na przetwarzanie.\n",
    "\n",
    "$$ GRN(x, c) = LayerNorm(x + GLU(Linear(ELU(Linear(x, c))))) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8c435b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRN Output: torch.Size([32, 64])\n"
     ]
    }
   ],
   "source": [
    "class GRN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Warstwa 1\n",
    "        # Je≈õli mamy kontekst, doklejamy go (lub rzutujemy)\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        if context_dim is not None:\n",
    "            self.context_projection = nn.Linear(context_dim, hidden_dim, bias=False)\n",
    "            \n",
    "        # Warstwa 2\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Bramka i Normalizacja\n",
    "        self.glu = GLU(hidden_dim)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # Projekcja rezydualna (je≈õli wej≈õcie ma inny wymiar ni≈º wyj≈õcie)\n",
    "        self.skip_projection = nn.Linear(input_dim, hidden_dim) if input_dim != hidden_dim else nn.Identity()\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        # x: [Batch, Input_Dim]\n",
    "        residual = self.skip_projection(x)\n",
    "        \n",
    "        # 1. Pierwsza warstwa + Kontekst\n",
    "        x = self.fc1(x)\n",
    "        if context is not None:\n",
    "            # Dodajemy kontekst (np. wektor statyczny sklepu) do przetwarzania\n",
    "            x = x + self.context_projection(context)\n",
    "            \n",
    "        x = F.elu(x) # Exponential Linear Unit (standard w TFT)\n",
    "        \n",
    "        # 2. Druga warstwa\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # 3. Bramkowanie (GLU) + Dropout\n",
    "        x = F.dropout(x, p=DROPOUT, training=self.training)\n",
    "        x = self.glu(x)\n",
    "        \n",
    "        # 4. Add & Norm\n",
    "        return self.norm(x + residual)\n",
    "\n",
    "# Test z Kontekstem\n",
    "grn = GRN(input_dim=10, hidden_dim=64, context_dim=5)\n",
    "x_in = torch.randn(32, 10) # 32 pr√≥bki, 10 cech\n",
    "c_in = torch.randn(32, 5)  # Kontekst (np. ID sklepu)\n",
    "\n",
    "out = grn(x_in, c_in)\n",
    "print(f\"GRN Output: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b8790",
   "metadata": {},
   "source": [
    "## Krok 3: Variable Selection Network (VSN)\n",
    "\n",
    "To jest unikalne dla TFT.\n",
    "Zamiast wrzucaƒá wszystkie cechy do jednego worka (jak w MLP), TFT przetwarza **ka≈ºdƒÖ kolumnƒô osobno** przez w≈Çasny GRN.\n",
    "Na ko≈Ñcu sieƒá decyduje (wa≈ºy), kt√≥re kolumny sƒÖ wa≈ºne dla danej pr√≥bki.\n",
    "\n",
    "Dziƒôki temu TFT jest **Interpretowalny**. Powie Ci: *\"Dla tej prognozy wziƒô≈Çam pod uwagƒô 80% Sprzeda≈ºy Wczorajszej i 20% Pogody, a zignorowa≈Çam Dzie≈Ñ Tygodnia\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "836e5417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyj≈õcie VSN: torch.Size([32, 64]) -> Jeden wektor reprezentujƒÖcy ca≈Çy krok czasowy.\n",
      "--- WAGI WA≈ªNO≈öCI (Feature Importance) dla pierwszego przyk≈Çadu ---\n",
      "[0.08056269 0.12061661 0.79882073]\n"
     ]
    }
   ],
   "source": [
    "class VariableSelectionNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, input_dim, hidden_dim, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs # Ile mamy kolumn (zmiennych)?\n",
    "        \n",
    "        # Dla ka≈ºdej zmiennej tworzymy osobny GRN\n",
    "        self.single_variable_grns = nn.ModuleList([\n",
    "            GRN(input_dim, hidden_dim, context_dim) for _ in range(num_inputs)\n",
    "        ])\n",
    "        \n",
    "        # GRN wa≈ºƒÖcy (decyduje o wagach dla ka≈ºdej zmiennej)\n",
    "        # Wej≈õcie to sp≈Çaszczone wszystkie zmienne\n",
    "        self.weighting_grn = GRN(num_inputs * input_dim, num_inputs, context_dim)\n",
    "        \n",
    "    def forward(self, x_list, context=None):\n",
    "        # x_list: Lista tensor√≥w (ka≈ºdy to jedna zmienna np. [Batch, 1])\n",
    "        # Musimy je najpierw zrzutowaƒá na ten sam wymiar (Embedding), tu pomijamy dla uproszczenia\n",
    "        # Zak≈Çadamy, ≈ºe x_list to tensor [Batch, Num_Inputs, Input_Dim]\n",
    "        \n",
    "        batch_size = x_list.shape[0]\n",
    "        \n",
    "        # 1. Przetwarzamy ka≈ºdƒÖ zmiennƒÖ przez jej GRN\n",
    "        processed_vars = []\n",
    "        for i in range(self.num_inputs):\n",
    "            var_out = self.single_variable_grns[i](x_list[:, i, :], context)\n",
    "            processed_vars.append(var_out)\n",
    "            \n",
    "        processed_vars = torch.stack(processed_vars, dim=1) # [Batch, Num, Hidden]\n",
    "        \n",
    "        # 2. Obliczamy wagi wa≈ºno≈õci (Weights)\n",
    "        # Sp≈Çaszczamy wej≈õcie dla Weighting GRN\n",
    "        flat_input = x_list.view(batch_size, -1)\n",
    "        weights = self.weighting_grn(flat_input, context)\n",
    "        weights = F.softmax(weights, dim=-1) # [Batch, Num_Inputs]\n",
    "        \n",
    "        # 3. Suma wa≈ºona\n",
    "        # weights: [Batch, Num, 1]\n",
    "        weights = weights.unsqueeze(-1)\n",
    "        combined = torch.sum(processed_vars * weights, dim=1)\n",
    "        \n",
    "        return combined, weights\n",
    "\n",
    "# Symulacja: Mamy 3 zmienne (np. Sprzeda≈º, Pogoda, Cena), ka≈ºda ma wymiar 64 (po embeddingu)\n",
    "vsn = VariableSelectionNetwork(num_inputs=3, input_dim=64, hidden_dim=64)\n",
    "\n",
    "dummy_vars = torch.randn(32, 3, 64) # [Batch, Zmienne, Cechy]\n",
    "out, weights = vsn(dummy_vars)\n",
    "\n",
    "print(f\"Wyj≈õcie VSN: {out.shape} -> Jeden wektor reprezentujƒÖcy ca≈Çy krok czasowy.\")\n",
    "print(\"--- WAGI WA≈ªNO≈öCI (Feature Importance) dla pierwszego przyk≈Çadu ---\")\n",
    "print(weights[0].squeeze().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39215b8f",
   "metadata": {},
   "source": [
    "## üß† Podsumowanie: Dlaczego TFT jest SOTA?\n",
    "\n",
    "TFT ≈ÇƒÖczy zalety wszystkich ≈õwiat√≥w:\n",
    "1.  **RNN (LSTM):** U≈ºywa ich do lokalnego przetwarzania sekwencji (nie pokazali≈õmy tego tutaj, ale sƒÖ w pe≈Çnej architekturze).\n",
    "2.  **Transformer (Attention):** U≈ºywa Multi-Head Attention do patrzenia na d≈Çugoterminowe zale≈ºno≈õci (np. \"sprzeda≈º rok temu\").\n",
    "3.  **Drzewa Decyzyjne (Selection):** Dziƒôki `VariableSelectionNetwork` potrafi odrzucaƒá szum, co zwykle robiƒÖ XGBoosty.\n",
    "\n",
    "Dlatego TFT wygrywa konkursy forecastingowe (np. M5 Competition) i jest u≈ºywany w Google Cloud Forecasting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-handbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
