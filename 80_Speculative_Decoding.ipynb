{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f8b485d",
   "metadata": {},
   "source": [
    "# ‚è© Speculative Decoding: Jak przyspieszyƒá LLM 2x bez utraty jako≈õci?\n",
    "\n",
    "Wielkie modele (Target Model) sƒÖ ograniczone przez **Memory Bandwidth** (przepustowo≈õƒá pamiƒôci). Za≈Çadowanie wag trwa d≈Çu≈ºej ni≈º same obliczenia.\n",
    "Dlatego uruchomienie modelu dla 1 tokena kosztuje tyle samo czasu, co dla 5 token√≥w (je≈õli robimy to r√≥wnolegle).\n",
    "\n",
    "**Algorytm Spekulacyjny:**\n",
    "1.  **Draft:** Ma≈Çy model (np. GPT-2) generuje szybko `K` token√≥w (np. \"Ala ma kota\").\n",
    "2.  **Verify:** Du≈ºy model (np. GPT-4) dostaje ten ciƒÖg i liczy prawdopodobie≈Ñstwa dla wszystkich pozycji naraz.\n",
    "3.  **Accept/Reject:** Sprawdzamy, czy Du≈ºy Model zgadza siƒô z Ma≈Çym.\n",
    "    *   Akceptujemy prefiks, kt√≥ry jest zgodny.\n",
    "    *   Resztƒô odrzucamy i generujemy poprawny token z Du≈ºego Modelu.\n",
    "\n",
    "Zrobimy symulacjƒô tego procesu, u≈ºywajƒÖc sztucznych op√≥≈∫nie≈Ñ (`time.sleep`), ≈ºeby pokazaƒá zysk czasowy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1b3d553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zdanie docelowe: ['Sztuczna', 'inteligencja', 'zmienia', '≈õwiat', 'na', 'lepsze']\n",
      "Draft jest 10x szybszy od Targetu.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Konfiguracja symulacji\n",
    "# Target Model jest mƒÖdry (dok≈Çadny), ale wolny\n",
    "TARGET_TIME_PER_TOKEN = 0.1  # 100 ms\n",
    "# Draft Model jest g≈Çupi (czƒôsto siƒô myli), ale szybki\n",
    "DRAFT_TIME_PER_TOKEN = 0.01  # 10 ms\n",
    "\n",
    "# D≈Çugo≈õƒá spekulacji (ile token√≥w zgadujemy naraz?)\n",
    "K_LOOKAHEAD = 4 \n",
    "\n",
    "# Nasze \"S≈Çownictwo\" (uproszczone)\n",
    "target_sentence = \"Sztuczna inteligencja zmienia ≈õwiat na lepsze\".split()\n",
    "vocab = {word: i for i, word in enumerate(target_sentence)}\n",
    "vocab_inv = {i: word for word, i in vocab.items()}\n",
    "\n",
    "print(f\"Zdanie docelowe: {target_sentence}\")\n",
    "print(f\"Draft jest {TARGET_TIME_PER_TOKEN / DRAFT_TIME_PER_TOKEN:.0f}x szybszy od Targetu.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b63a578",
   "metadata": {},
   "source": [
    "## Symulacja Modeli\n",
    "\n",
    "Stworzymy dwie funkcje:\n",
    "1.  `target_model`: Zawsze zwraca poprawne s≈Çowo, ale \"my≈õli\" d≈Çugo.\n",
    "2.  `draft_model`: My≈õli szybko, ale ma np. 70% szans na trafienie (symulacja mniejszej inteligencji)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "323648c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modele gotowe.\n"
     ]
    }
   ],
   "source": [
    "class MockModel:\n",
    "    def __init__(self, name, latency, accuracy, true_sentence):\n",
    "        self.name = name\n",
    "        self.latency = latency\n",
    "        self.accuracy = accuracy\n",
    "        self.true_sentence = true_sentence\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Zwraca nastƒôpny token (lub listƒô token√≥w).\n",
    "        \"\"\"\n",
    "        # Symulacja czasu oblicze≈Ñ\n",
    "        time.sleep(self.latency)\n",
    "        \n",
    "        current_len = len(input_ids)\n",
    "        if current_len >= len(self.true_sentence):\n",
    "            return \"<EOS>\" # Koniec zdania\n",
    "        \n",
    "        # Prawdziwy token, kt√≥ry powinien byƒá\n",
    "        true_token = self.true_sentence[current_len]\n",
    "        \n",
    "        # Czy model zgad≈Ç?\n",
    "        if random.random() < self.accuracy:\n",
    "            return true_token\n",
    "        else:\n",
    "            # Pomy≈Çka (losowe s≈Çowo ze s≈Çownika)\n",
    "            return random.choice(list(vocab.keys()))\n",
    "\n",
    "# Inicjalizacja\n",
    "# Target: 100% dok≈Çadno≈õci (to nasza wyrocznia), wolny\n",
    "target_model = MockModel(\"Target (Big)\", TARGET_TIME_PER_TOKEN, 1.0, target_sentence)\n",
    "\n",
    "# Draft: 70% dok≈Çadno≈õci, szybki\n",
    "draft_model = MockModel(\"Draft (Small)\", DRAFT_TIME_PER_TOKEN, 0.7, target_sentence)\n",
    "\n",
    "print(\"Modele gotowe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc73e2b2",
   "metadata": {},
   "source": [
    "## Metoda 1: Standardowa Generacja (Autoregresyjna)\n",
    "\n",
    "To jest spos√≥b, w jaki normalnie dzia≈Ça ChatGPT. Jeden token po drugim, u≈ºywajƒÖc tylko du≈ºego modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d720ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STANDARD GENERATION ---\n",
      "Generated: Sztuczna\n",
      "Generated: inteligencja\n",
      "Generated: zmienia\n",
      "Generated: ≈õwiat\n",
      "Generated: na\n",
      "Generated: lepsze\n",
      "\n",
      "Czas Standardowy: 0.6032 s\n",
      "Prƒôdko≈õƒá: 9.95 tokens/s\n"
     ]
    }
   ],
   "source": [
    "def standard_generation():\n",
    "    current_text = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"--- STANDARD GENERATION ---\")\n",
    "    while len(current_text) < len(target_sentence):\n",
    "        # Generujemy 1 token Du≈ºym Modelem\n",
    "        token = target_model.forward(current_text)\n",
    "        current_text.append(token)\n",
    "        print(f\"Generated: {token}\")\n",
    "        \n",
    "    total_time = time.time() - start_time\n",
    "    return total_time, len(current_text)\n",
    "\n",
    "time_std, tokens_std = standard_generation()\n",
    "print(f\"\\nCzas Standardowy: {time_std:.4f} s\")\n",
    "print(f\"Prƒôdko≈õƒá: {tokens_std / time_std:.2f} tokens/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abb0801",
   "metadata": {},
   "source": [
    "## Metoda 2: Speculative Decoding\n",
    "\n",
    "Teraz algorytm spekulacyjny:\n",
    "1.  Pƒôtla g≈Ç√≥wna.\n",
    "2.  **Krok Draftu:** Ma≈Çy model generuje `K` token√≥w jeden po drugim (szybko).\n",
    "3.  **Krok Weryfikacji:** Du≈ºy model sprawdza te `K` token√≥w + 1 dodatkowy (r√≥wnolegle).\n",
    "    *   *Uwaga: W naszej symulacji Pythona \"r√≥wnoleg≈Ço≈õƒá\" symulujemy tym, ≈ºe `target_model` sprawdza listƒô token√≥w w tym samym czasie co pojedynczy token (bo koszt to g≈Ç√≥wnie za≈Çadowanie wag).*\n",
    "4.  **Akceptacja:** Bierzemy tyle token√≥w, ile siƒô zgadza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a09fcc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SPECULATIVE GENERATION ---\n",
      "Draft: ['Sztuczna', 'inteligencja', '≈õwiat', '≈õwiat'] -> Zaakceptowano: ['Sztuczna', 'inteligencja', 'zmienia'] ‚úÖ\n",
      "Draft: ['≈õwiat', 'na', 'lepsze'] -> Zaakceptowano: ['≈õwiat', 'na', 'lepsze'] ‚úÖ\n",
      "\n",
      "Czas Spekulacyjny: 0.2736 s\n",
      "Prƒôdko≈õƒá: 21.93 tokens/s\n"
     ]
    }
   ],
   "source": [
    "def speculative_generation():\n",
    "    current_text = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"--- SPECULATIVE GENERATION ---\")\n",
    "    \n",
    "    while len(current_text) < len(target_sentence):\n",
    "        # 1. DRAFT PHASE (Ma≈Çy model zgaduje K razy)\n",
    "        draft_tokens = []\n",
    "        temp_context = current_text.copy()\n",
    "        \n",
    "        for _ in range(K_LOOKAHEAD):\n",
    "            if len(temp_context) >= len(target_sentence): break\n",
    "            token = draft_model.forward(temp_context)\n",
    "            draft_tokens.append(token)\n",
    "            temp_context.append(token)\n",
    "            \n",
    "        # 2. VERIFICATION PHASE (Du≈ºy model sprawdza)\n",
    "        # W prawdziwym GPU to dzieje siƒô w JEDNYM przebiegu (batch)\n",
    "        # Symulujemy koszt jednego uruchomienia du≈ºego modelu\n",
    "        time.sleep(TARGET_TIME_PER_TOKEN) \n",
    "        \n",
    "        # Sprawdzamy poprawno≈õƒá (Verify)\n",
    "        # Target model generuje \"swojƒÖ wersjƒô\" dla ka≈ºdego kroku w drafcie\n",
    "        # (W prawdziwym ≈õwiecie liczy prawdopodobie≈Ñstwa P(x))\n",
    "        accepted_tokens = []\n",
    "        for i, draft_tok in enumerate(draft_tokens):\n",
    "            # Jaki token powinien byƒá na tej pozycji wg Targetu?\n",
    "            true_pos = len(current_text) + i\n",
    "            if true_pos >= len(target_sentence): break\n",
    "            \n",
    "            target_tok = target_sentence[true_pos]\n",
    "            \n",
    "            if draft_tok == target_tok:\n",
    "                accepted_tokens.append(draft_tok)\n",
    "            else:\n",
    "                # B≈ÇƒÖd! Odrzucamy resztƒô draftu\n",
    "                # Target model \"poprawia\" ten jeden b≈ÇƒÖd\n",
    "                accepted_tokens.append(target_tok)\n",
    "                break # Przerywamy akceptacjƒô\n",
    "        \n",
    "        # Aktualizacja tekstu\n",
    "        current_text.extend(accepted_tokens)\n",
    "        \n",
    "        # Logowanie\n",
    "        status = \"‚úÖ\" if len(accepted_tokens) > 1 else \"‚ùå\"\n",
    "        print(f\"Draft: {draft_tokens} -> Zaakceptowano: {accepted_tokens} {status}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    return total_time, len(current_text)\n",
    "\n",
    "time_spec, tokens_spec = speculative_generation()\n",
    "print(f\"\\nCzas Spekulacyjny: {time_spec:.4f} s\")\n",
    "print(f\"Prƒôdko≈õƒá: {tokens_spec / time_spec:.2f} tokens/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f4347c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Standard: 0.6032s\n",
      "Speculative: 0.2736s\n",
      "üöÄ PRZYSPIESZENIE: 2.20x\n",
      "Sukces! Ma≈Çy model skutecznie pom√≥g≈Ç du≈ºemu.\n"
     ]
    }
   ],
   "source": [
    "# PODSUMOWANIE\n",
    "speedup = time_std / time_spec\n",
    "print(\"-\" * 30)\n",
    "print(f\"Standard: {time_std:.4f}s\")\n",
    "print(f\"Speculative: {time_spec:.4f}s\")\n",
    "print(f\"üöÄ PRZYSPIESZENIE: {speedup:.2f}x\")\n",
    "\n",
    "if speedup > 1.0:\n",
    "    print(\"Sukces! Ma≈Çy model skutecznie pom√≥g≈Ç du≈ºemu.\")\n",
    "else:\n",
    "    print(\"Pora≈ºka. Ma≈Çy model myli≈Ç siƒô zbyt czƒôsto (narzut czasowy draftu zjad≈Ç zysk).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c7d947",
   "metadata": {},
   "source": [
    "## üß† Podsumowanie: Hazard, kt√≥ry siƒô op≈Çaca\n",
    "\n",
    "Je≈õli ma≈Çy model ma skuteczno≈õƒá (Acceptance Rate) powy≈ºej ~50-60%, spekulacja siƒô op≈Çaca.\n",
    "Je≈õli ma≈Çy model jest beznadziejny, spekulacja tylko spowalnia (bo tracimy czas na draft, kt√≥ry i tak idzie do kosza).\n",
    "\n",
    "**Dlaczego to dzia≈Ça na GPU?**\n",
    "Uruchomienie modelu GPT-4 dla sekwencji o d≈Çugo≈õci 1 (input) kosztuje np. 50ms.\n",
    "Uruchomienie go dla sekwencji o d≈Çugo≈õci 5 (input) te≈º kosztuje ok. 50-55ms.\n",
    "Dziƒôki temu **weryfikacja 5 token√≥w jest prawie darmowa** w por√≥wnaniu do generowania ich pojedynczo.\n",
    "\n",
    "To obecnie standardowa technika w bibliotekach takich jak **vLLM** czy **HuggingFace TGI**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-handbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
