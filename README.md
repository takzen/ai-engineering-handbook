# üìò Podrƒôcznik In≈ºynierii AI & Implementacje Referencyjne

Kompleksowy zbi√≥r algorytm√≥w zaimplementowanych od podstaw ("from first principles"), pokrywajƒÖcy pe≈Çne spektrum In≈ºynierii Uczenia Maszynowego ‚Äî od Analizy Statystycznej i Klasycznego ML, a≈º po Du≈ºe Modele Jƒôzykowe (LLM) i architektury Computer Vision.

### üéØ Cel Repozytorium

Ten projekt s≈Çu≈ºy jako **referencja techniczna** oraz baza wiedzy demonstrujƒÖca matematyczne fundamenty stojƒÖce za nowoczesnymi systemami AI. Wykracza poza u≈ºywanie gotowych, wysokopoziomowych API, skupiajƒÖc siƒô na zrozumieniu tego, _jak_ i _dlaczego_ te algorytmy dzia≈ÇajƒÖ "pod maskƒÖ".

### üîë Kluczowe Implementacje (Od Zera)

- **Architektura LLM:** Pe≈Çna implementacja Bloku Transformera (Self-Attention, LayerNorm, Residuals) w PyTorch.
- **Optymalizacja:** Matematyczna implementacja LoRA (Low-Rank Adaptation) do fine-tuningu.
- **Generative AI:** Sieci GAN i VAE (z wykorzystaniem Reparameterization Trick).
- **Computer Vision:** Rƒôczna implementacja mechanizm√≥w IoU (Intersection over Union) oraz NMS (Non-Max Suppression) dla detekcji obiekt√≥w.
- **ML Ops:** Niestandardowe Estymatory (Custom Estimators) i Pipeline'y Scikit-Learn do produkcyjnego przetwarzania danych.

---

## üìÇ Spis Tre≈õci i Tematyka

### üìä Analiza Danych (EDA) i Statystyka

Fundamenty pracy z danymi. Jak zrozumieƒá, co siedzi w tabeli, zanim wrzucimy to do modelu.

| Plik                                                   | Temat                   | Kluczowe pojƒôcia                                                   |
| :----------------------------------------------------- | :---------------------- | :----------------------------------------------------------------- |
| **00_KDE_Tutorial.ipynb**                              | Rozk≈Çady danych         | Kernel Density Estimation (KDE), histogramy, wizualizacja gƒôsto≈õci |
| **01_Correlations_and_Significance_of_Features.ipynb** | Badanie zale≈ºno≈õci      | Korelacja Pearsona/Spearmana, Heatmapy, p-value                    |
| **04_Statistics_and_Scaling.ipynb**                    | Testy hipotez i skala   | **P-value**, Test T-studenta, MinMaxScaler vs StandardScaler       |
| **35_ANOVA_Hypothesis_Testing.ipynb**                  | Por√≥wnywanie grup       | ANOVA, Test F, wariancja miƒôdzygrupowa                             |
| **42_Statistics_Masterclass.ipynb**                    | Statystyka zaawansowana | Testy parametryczne i nieparametryczne, rozk≈Çady                   |

### üõ†Ô∏è In≈ºynieria Cech (Feature Engineering)

Przygotowanie "brudnych" danych, aby algorytmy mog≈Çy z nich korzystaƒá.

| Plik                                    | Temat                    | Kluczowe pojƒôcia                                                        |
| :-------------------------------------- | :----------------------- | :---------------------------------------------------------------------- |
| **02_Advanced_Feature_Selection.ipynb** | Wyb√≥r najlepszych danych | RFE (Recursive Feature Elimination), SelectKBest, redukcja szumu        |
| **03_Encoding_Tutorial.ipynb**          | Zamiana tekstu na liczby | One-Hot Encoding, Label Encoding, Ordinal Encoding                      |
| **13_Missing_Data_Imputation.ipynb**    | Obs≈Çuga brak√≥w danych    | Imputacja (≈õrednia, mediana), imputacja grupowa (Pandas transform), NaN |

### ü§ñ Klasyczny Machine Learning

Algorytmy uczenia z nadzorem (Supervised) i bez nadzoru (Unsupervised).

| Plik                                          | Temat                   | Kluczowe pojƒôcia                                                 |
| :-------------------------------------------- | :---------------------- | :--------------------------------------------------------------- |
| **06_Naive_Bayes_Spam.ipynb**                 | Filtr antyspamowy (NLP) | **Naive Bayes**, Bag of Words, prawdopodobie≈Ñstwo warunkowe      |
| **08_Overfitting_Underfitting.ipynb**         | Diagnoza b≈Çƒôd√≥w modelu  | Przeuczenie vs Niedouczenie, wielomiany, generalizacja           |
| **09_K_Means_Clustering.ipynb**               | Segmentacja klient√≥w    | **Unsupervised Learning**, K-Means, Metoda ≈Åokcia (Elbow Method) |
| **10_Decision_Trees.ipynb**                   | Drzewa Decyzyjne        | White-Box Models, wizualizacja decyzji, Feature Importance       |
| **14_Random_Forest_Ensemble.ipynb**           | Ensemble Learning       | **Random Forest**, Bagging, agregacja predykcji, stabilno≈õƒá      |
| **19_Cross_Validation.ipynb**                 | Walidacja modeli        | **K-Fold**, walidacja krzy≈ºowa, unikanie overfittingu            |
| **27_Hyperparameter_Tuning_GridSearch.ipynb** | Optymalizacja modeli    | **Grid Search**, RandomizedSearch, dob√≥r parametr√≥w              |
| **34_Regularization_Lasso_Ridge.ipynb**       | Regularyzacja           | **Lasso (L1)**, Ridge (L2), ElasticNet, kara za z≈Ço≈ºono≈õƒá        |
| **36_Market_Basket_Apriori.ipynb**            | Analiza Koszykowa       | **Apriori**, Support, Confidence, Lift, regu≈Çy asocjacyjne       |
| **37_Gradient_Boosting_XGBoost.ipynb**        | Gradient Boosting       | **XGBoost**, LightGBM, uczenie sekwencyjne, boosting             |
| **47_SVM_Kernel_Trick.ipynb**                 | Support Vector Machines | **SVM**, Kernel Trick, hyperplanes, separowalno≈õƒá liniowa        |
| **51_Recommender_Systems_SVD.ipynb**          | Systemy Rekomendacyjne  | **SVD**, Matrix Factorization, collaborative filtering           |

### üìè Ewaluacja Modeli

Jak sprawdziƒá, czy model naprawdƒô dzia≈Ça?

| Plik                                           | Temat           | Kluczowe pojƒôcia                                                            |
| :--------------------------------------------- | :-------------- | :-------------------------------------------------------------------------- |
| **07_Confusion_Matrix_Precision_Recall.ipynb** | Metryki sukcesu | **Macierz Pomy≈Çek**, Precision, Recall, F1-Score (dlaczego Accuracy k≈Çamie) |

### üß† Fundamenty LLM i Generative AI

Mechanizmy stojƒÖce za modelami takimi jak GPT.

| Plik                                        | Temat                          | Kluczowe pojƒôcia                                                                      |
| :------------------------------------------ | :----------------------------- | :------------------------------------------------------------------------------------ |
| **05_Top_p_Top_k.ipynb**                    | Sterowanie generowaniem tekstu | Sampling, probabilistyka wyboru s≈Ç√≥w, kreatywno≈õƒá AI                                  |
| **11_Embeddings_Vector_Space.ipynb**        | Wektory s≈Ç√≥w                   | **Embeddings**, przestrze≈Ñ wektorowa, algebra na s≈Çowach (Kr√≥l - Mƒô≈ºczyzna + Kobieta) |
| **12_LLM_Temperature.ipynb**                | Parametr Temperatury           | Softmax, Logits, sterowanie halucynacjami i pewno≈õciƒÖ modelu                          |
| **23_Tokenization_GPT.ipynb**               | Tokenizacja                    | **Byte Pair Encoding**, subword tokenization, problem z liczeniem liter               |
| **24_Self_Attention_Mechanism.ipynb**       | Mechanizm Uwagi                | **Transformer**, Query-Key-Value, kontekst w zdaniach                                 |
| **18_Cosine_Similarity_Search.ipynb**       | Podobie≈Ñstwo wektor√≥w          | **Cosine Similarity**, kƒÖt vs odleg≈Ço≈õƒá, Semantic Search                              |
| **20_RAG_Architecture_Simulation.ipynb**    | Retrieval Augmented Generation | **RAG**, wyszukiwanie w bazie wiedzy, pipeline z embeddingami                         |
| **26_RAG_Chunking_Strategies.ipynb**        | Przygotowanie dokument√≥w       | **Chunking**, Fixed-size, Recursive, Overlap, Windowing                               |
| **46_Transformer_Block_From_Scratch.ipynb** | Blok Transformera              | **Transformer Block**, LayerNorm, Residual Connections, Feed Forward                  |
| **55_LoRA_Fine_Tuning_Math.ipynb**          | Fine-tuning LLM                | **LoRA**, Low-Rank Adaptation, efektywne douczanie modeli                             |

### üßÆ Matematyka i Optymalizacja

Jak maszyny siƒô uczƒÖ pod maskƒÖ?

| Plik                                              | Temat               | Kluczowe pojƒôcia                                                      |
| :------------------------------------------------ | :------------------ | :-------------------------------------------------------------------- |
| **15_Gradient_Descent.ipynb**                     | Optymalizacja       | **Gradient Descent**, Learning Rate, schodzenie po gradiencie         |
| **17_PCA_Dimensionality_Reduction.ipynb**         | Redukcja wymiar√≥w   | **PCA**, Principal Component Analysis, wizualizacja wysokich wymiar√≥w |
| **48_tSNE_vs_PCA_Dimensionality_Reduction.ipynb** | Redukcja nieliniowa | **t-SNE**, UMAP, wizualizacja embedding√≥w                             |

### üî¨ Sieci Neuronowe i Deep Learning

Od pojedynczego neuronu do g≈Çƒôbokich sieci.

| Plik                                            | Temat                           | Kluczowe pojƒôcia                                        |
| :---------------------------------------------- | :------------------------------ | :------------------------------------------------------ |
| **16_Neural_Network_Perceptron.ipynb**          | Pierwszy neuron                 | **Perceptron**, wagi, bias, funkcja aktywacji           |
| **21_MLP_Neural_Network_XOR.ipynb**             | Sieci wielowarstwowe            | **Multi-Layer Perceptron**, warstwy ukryte, XOR problem |
| **22_Activation_Functions.ipynb**               | Funkcje aktywacji               | **ReLU**, Sigmoid, Softmax, nieliniowo≈õƒá                |
| **32_PyTorch_Tensors_Autograd.ipynb**           | Podstawy PyTorch                | **Tensors**, Autograd, automatyczne r√≥≈ºniczkowanie      |
| **33_PyTorch_Neural_Network_Class.ipynb**       | Budowa sieci w PyTorch          | **nn.Module**, forward pass, OOP w deep learningu       |
| **38_CNN_Computer_Vision.ipynb**                | Sieci Konwolucyjne              | **CNN**, Conv2d, MaxPool, filtry, Computer Vision       |
| **39_RNN_LSTM_Sequence_Models.ipynb**           | Sieci Rekurencyjne              | **RNN**, LSTM, przetwarzanie sekwencji, pamiƒôƒá          |
| **40_Autoencoder_Anomaly_Detection.ipynb**      | Detekcja Anomalii               | **Autoencoder**, kompresja, detekcja outlier√≥w          |
| **41_GAN_Generative_Adversarial_Network.ipynb** | Generative Adversarial Networks | **GAN**, Generator, Dyskryminator, generowanie danych   |
| **43_VAE_Variational_Autoencoder.ipynb**        | Variational Autoencoder         | **VAE**, Latent Space, KL Divergence, generowanie       |
| **49_Object_Detection_IoU.ipynb**               | Detekcja Obiekt√≥w               | **IoU**, Intersection over Union, bounding boxes        |
| **50_UNet_Image_Segmentation.ipynb**            | Segmentacja Obrazu              | **U-Net**, segmentacja pikselowa, architektura U        |

### üéÆ Reinforcement Learning

Uczenie przez nagrody i kary.

| Plik                                  | Temat           | Kluczowe pojƒôcia                               |
| :------------------------------------ | :-------------- | :--------------------------------------------- |
| **44_RL_Q_Learning_FrozenLake.ipynb** | Q-Learning      | **Q-Table**, R√≥wnanie Bellmana, nagrody i kary |
| **45_RL_Deep_Q_Learning_DQN.ipynb**   | Deep Q-Learning | **DQN**, Replay Buffer, sieci neuronowe w RL   |

### üß¨ Algorytmy Zaawansowane

Specjalistyczne techniki i podej≈õcia.

| Plik                                      | Temat                  | Kluczowe pojƒôcia                                            |
| :---------------------------------------- | :--------------------- | :---------------------------------------------------------- |
| **52_Genetic_Algorithms_Evolution.ipynb** | Algorytmy Genetyczne   | **Evolutionary Algorithms**, krzy≈ºowanie, mutacja, selekcja |
| **53_Monte_Carlo_Simulation.ipynb**       | Symulacje Monte Carlo  | Symulacje probabilistyczne, analiza ryzyka                  |
| **54_FFT_Signal_Processing.ipynb**        | Przetwarzanie Sygna≈Ç√≥w | **FFT**, Transformata Fouriera, analiza czƒôstotliwo≈õci      |

### üíª In≈ºynieria i Deployment

Praktyczne umiejƒôtno≈õci produkcyjne.

| Plik                                         | Temat               | Kluczowe pojƒôcia                                               |
| :------------------------------------------- | :------------------ | :------------------------------------------------------------- |
| **25_Model_Persistence_Pickle_Joblib.ipynb** | Zapisywanie modeli  | **Pickle**, Joblib, serializacja obiekt√≥w                      |
| **30_Sklearn_Pipelines.ipynb**               | RurociƒÖgi ML        | **Pipeline**, StandardScaler, data leakage prevention          |
| **31_Custom_Transformers.ipynb**             | W≈Çasne transformery | **BaseEstimator**, TransformerMixin, fit-transform pattern     |
| **28_Python_Dataclasses_for_ML.ipynb**       | Konfiguracja modeli | **Dataclasses**, structured configs, TrainingArguments pattern |
| **29_OOP_Classmethod_Staticmethod.ipynb**    | Wzorce projektowe   | **@classmethod**, @staticmethod, ModelLoader, factory pattern  |

---

## üõ†Ô∏è Technologie

Projekt oparty na standardowym stacku Data Science:

- **Python 3.x**
- **Pandas & NumPy** (Manipulacja danymi i obliczenia)
- **Scikit-Learn** (Algorytmy ML, Preprocessing, Metryki)
- **Matplotlib & Seaborn** (Wizualizacja danych)
- **SciPy** (Testy statystyczne)
- **PyTorch** (Deep Learning Framework)

## üöÄ Jak uruchomiƒá?

1.  Sklonuj repozytorium:
    ```bash
    git clone https://github.com/takzen/ai-engineering-handbook
    ```
2.  Zainstaluj wymagane biblioteki:
    ```bash
    pip install pandas numpy scikit-learn matplotlib seaborn scipy torch
    ```
3.  Uruchom Jupyter Notebook:
    ```bash
    jupyter notebook
    ```

---

Autor: Krzysztof Pika
