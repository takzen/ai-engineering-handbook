{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "742f89dc",
   "metadata": {},
   "source": [
    "# ‚ö° LLM Optimization: KV Cache & Flash Attention\n",
    "\n",
    "Generowanie tekstu w modelach GPT jest procesem **autoregresyjnym**:\n",
    "1.  Wpisujesz \"Ala\".\n",
    "2.  Model liczy wszystko i zwraca \"ma\".\n",
    "3.  Wpisujesz \"Ala ma\".\n",
    "4.  Model liczy wszystko OD ZERA i zwraca \"kota\".\n",
    "\n",
    "To marnotrawstwo. Obliczenia dla \"Ala\" w kroku 4 sƒÖ identyczne jak w kroku 2.\n",
    "\n",
    "**RozwiƒÖzanie: KV Cache.**\n",
    "Zamiast wyrzucaƒá wektory Key i Value dla poprzednich s≈Ç√≥w, trzymamy je w pamiƒôci (Cache).\n",
    "W nowym kroku obliczamy Attention tylko dla **jednego, nowego tokena** i doklejamy go do Cache'a.\n",
    "\n",
    "Z≈Ço≈ºono≈õƒá obliczeniowa spada z $O(N^2)$ (dla ca≈Çej sekwencji) do $O(N)$ (dla kroku)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d100aecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symulacja na: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Konfiguracja\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "D_MODEL = 512\n",
    "NUM_HEADS = 8\n",
    "HEAD_DIM = D_MODEL // NUM_HEADS\n",
    "SEQ_LEN = 100 # D≈Çugo≈õƒá generowanego tekstu\n",
    "\n",
    "print(f\"Symulacja na: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb7ca4c",
   "metadata": {},
   "source": [
    "## Implementacja Standardowa (Bez Cache)\n",
    "\n",
    "Najpierw zbudujmy \"g≈ÇupiƒÖ\" warstwƒô Attention, kt√≥ra za ka≈ºdym razem przelicza ca≈Çe zdanie od poczƒÖtku.\n",
    "To jest to, co robi model podczas *treningu* (bo wtedy znamy ca≈Çe zdanie), ale podczas *generowania* jest to bardzo nieefektywne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172526f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Attention gotowe.\n"
     ]
    }
   ],
   "source": [
    "class StandardAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(D_MODEL, D_MODEL, bias=False)\n",
    "        self.W_k = nn.Linear(D_MODEL, D_MODEL, bias=False)\n",
    "        self.W_v = nn.Linear(D_MODEL, D_MODEL, bias=False)\n",
    "        self.out = nn.Linear(D_MODEL, D_MODEL, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [Batch, Seq_Len, D_Model]\n",
    "        batch, seq_len, _ = x.shape\n",
    "        \n",
    "        # 1. Projekcje Q, K, V\n",
    "        Q = self.W_q(x).view(batch, seq_len, NUM_HEADS, HEAD_DIM).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch, seq_len, NUM_HEADS, HEAD_DIM).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch, seq_len, NUM_HEADS, HEAD_DIM).transpose(1, 2)\n",
    "        \n",
    "        # 2. Scaled Dot-Product Attention\n",
    "        # (Tutaj PyTorch robi to za nas wydajnie, ale i tak liczy ca≈ÇƒÖ macierz N*N)\n",
    "        attn_output = F.scaled_dot_product_attention(Q, K, V, is_causal=True)\n",
    "        \n",
    "        # 3. Scalenie g≈Çowic\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch, seq_len, D_MODEL)\n",
    "        return self.out(attn_output)\n",
    "\n",
    "print(\"Standard Attention gotowe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723b5d1",
   "metadata": {},
   "source": [
    "## Implementacja z KV Cache\n",
    "\n",
    "Teraz wersja sprytna.\n",
    "Metoda `forward` przyjmuje dodatkowy argument `kv_cache`.\n",
    "1.  Je≈õli to pierwszy krok -> licz wszystko.\n",
    "2.  Je≈õli to kolejny krok -> wej≈õcie to tylko **ostatni token**.\n",
    "3.  Oblicz K i V tylko dla tego tokena.\n",
    "4.  Doklej do `kv_cache`.\n",
    "5.  U≈ºyj ca≈Çego cache do policzenia Attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9741a858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached Attention gotowe.\n"
     ]
    }
   ],
   "source": [
    "class CachedAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(D_MODEL, D_MODEL, bias=False)\n",
    "        self.W_k = nn.Linear(D_MODEL, D_MODEL, bias=False)\n",
    "        self.W_v = nn.Linear(D_MODEL, D_MODEL, bias=False)\n",
    "        self.out = nn.Linear(D_MODEL, D_MODEL, bias=False)\n",
    "\n",
    "    def forward(self, x, kv_cache=None):\n",
    "        # x shape: [Batch, 1, D_Model] (Tylko NOWY token!)\n",
    "        batch, seq_len, _ = x.shape \n",
    "        \n",
    "        # 1. Projekcje (tylko dla nowego tokena)\n",
    "        q = self.W_q(x).view(batch, seq_len, NUM_HEADS, HEAD_DIM).transpose(1, 2)\n",
    "        k = self.W_k(x).view(batch, seq_len, NUM_HEADS, HEAD_DIM).transpose(1, 2)\n",
    "        v = self.W_v(x).view(batch, seq_len, NUM_HEADS, HEAD_DIM).transpose(1, 2)\n",
    "        \n",
    "        # 2. Obs≈Çuga Cache\n",
    "        if kv_cache is not None:\n",
    "            prev_k, prev_v = kv_cache\n",
    "            # Doklejamy nowe k i v do starych\n",
    "            k = torch.cat([prev_k, k], dim=2)\n",
    "            v = torch.cat([prev_v, v], dim=2)\n",
    "            \n",
    "        # Zapisujemy nowy cache na przysz≈Ço≈õƒá\n",
    "        new_cache = (k, v)\n",
    "        \n",
    "        # 3. Attention\n",
    "        # Q ma d≈Çugo≈õƒá 1 (nowy token). K i V majƒÖ d≈Çugo≈õƒá ca≈Çej historii.\n",
    "        # Dziƒôki temu nowy token \"patrzy\" na wszystkich poprzednik√≥w.\n",
    "        attn_output = F.scaled_dot_product_attention(q, k, v, is_causal=False) # Causal niepotrzebny, bo Q to 1 token\n",
    "        \n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch, seq_len, D_MODEL)\n",
    "        return self.out(attn_output), new_cache\n",
    "\n",
    "print(\"Cached Attention gotowe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2089aeb",
   "metadata": {},
   "source": [
    "## Wy≈õcig: Generowanie Tekstu\n",
    "\n",
    "Zasymulujemy proces generowania 100 kolejnych token√≥w.\n",
    "*   **Standard:** W ka≈ºdej pƒôtli podajemy ca≈ÇƒÖ historiƒô (`input_ids`).\n",
    "*   **Cached:** W ka≈ºdej pƒôtli podajemy tylko nowy token i cache.\n",
    "\n",
    "Zmierzymy czas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e22c998c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Time: 0.1483 s\n",
      "Cached Time:   0.0291 s\n",
      "üöÄ Przyspieszenie: 5.09x\n"
     ]
    }
   ],
   "source": [
    "# Inicjalizacja modeli\n",
    "model_std = StandardAttention().to(DEVICE).eval()\n",
    "model_cached = CachedAttention().to(DEVICE).eval()\n",
    "\n",
    "# Dummy input (Startujemy od 10 token√≥w)\n",
    "initial_input = torch.randn(1, 10, D_MODEL).to(DEVICE)\n",
    "\n",
    "# --- TEST 1: STANDARD ---\n",
    "start_time = time.time()\n",
    "current_input = initial_input.clone()\n",
    "\n",
    "for _ in range(SEQ_LEN):\n",
    "    with torch.no_grad():\n",
    "        # Musimy podaƒá CA≈ÅƒÑ historiƒô\n",
    "        out = model_std(current_input)\n",
    "        # Bierzemy ostatni wektor jako \"nowy token\" (symulacja)\n",
    "        next_token = out[:, -1:, :] \n",
    "        # Doklejamy do wej≈õcia\n",
    "        current_input = torch.cat([current_input, next_token], dim=1)\n",
    "\n",
    "time_std = time.time() - start_time\n",
    "print(f\"Standard Time: {time_std:.4f} s\")\n",
    "\n",
    "\n",
    "# --- TEST 2: KV CACHE ---\n",
    "start_time = time.time()\n",
    "current_token = initial_input[:, -1:, :].clone() # Tylko ostatni na start\n",
    "# Najpierw musimy \"nape≈Çniƒá\" cache historiƒÖ (Prefill phase)\n",
    "# W uproszczeniu: pomijamy prefill i zak≈Çadamy, ≈ºe startujemy od zera albo robimy pass na initial_input\n",
    "# Zr√≥bmy poprawny start:\n",
    "cache = None\n",
    "# 1. Prefill (przetwarzamy prompt)\n",
    "with torch.no_grad():\n",
    "    _, cache = model_cached(initial_input, kv_cache=None)\n",
    "\n",
    "# 2. Generation loop\n",
    "for _ in range(SEQ_LEN):\n",
    "    with torch.no_grad():\n",
    "        # Podajemy TYLKO ostatni token i cache\n",
    "        out, cache = model_cached(current_token, kv_cache=cache)\n",
    "        next_token = out # To ju≈º jest tylko 1 token\n",
    "        current_token = next_token\n",
    "\n",
    "time_cache = time.time() - start_time\n",
    "print(f\"Cached Time:   {time_cache:.4f} s\")\n",
    "\n",
    "speedup = time_std / time_cache\n",
    "print(f\"üöÄ Przyspieszenie: {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9104d985",
   "metadata": {},
   "source": [
    "## Czym jest Flash Attention? (Teoria)\n",
    "\n",
    "Przyspieszyli≈õmy obliczenia (mniej FLOPs). Ale jest jeszcze problem **pamiƒôci (VRAM)**.\n",
    "Przy d≈Çugich sekwencjach (np. 100k token√≥w), macierz uwagi $N \\times N$ nie mie≈õci siƒô w pamiƒôci GPU.\n",
    "\n",
    "**Flash Attention (2022/2023):**\n",
    "To in≈ºynierski majstersztyk na poziomie sprzƒôtowym (CUDA).\n",
    "GPU ma dwa rodzaje pamiƒôci:\n",
    "1.  **HBM (High Bandwidth Memory):** Wielka, ale wolna (jak lod√≥wka w kuchni).\n",
    "2.  **SRAM (Static RAM):** Malutka, ale superszybka (jak deska do krojenia).\n",
    "\n",
    "Tradycyjne Attention ciƒÖgle przenosi macierze z HBM do SRAM i z powrotem.\n",
    "**Flash Attention** u≈ºywa techniki **Tiling (Kafelkowanie)**. Dzieli macierz na ma≈Çe klocki, kt√≥re mieszczƒÖ siƒô w SRAM, liczy wszystko \"na desce do krojenia\" i odsy≈Ça do \"lod√≥wki\" tylko gotowy wynik.\n",
    "\n",
    "Dziƒôki temu jest:\n",
    "1.  Szybsze (mniej czekania na dane).\n",
    "2.  Liniowe pamiƒôciowo (nie tworzy gigantycznej macierzy $N \\times N$).\n",
    "\n",
    "W PyTorch 2.0+ funkcja `F.scaled_dot_product_attention` automatycznie u≈ºywa Flash Attention, je≈õli masz odpowiedniƒÖ kartƒô graficznƒÖ!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847fc2af",
   "metadata": {},
   "source": [
    "## üß† Podsumowanie: Dlaczego to kluczowe?\n",
    "\n",
    "1.  **KV Cache** jest obowiƒÖzkowe przy generowaniu tekstu (Inference). Bez tego ChatGPT generowa≈Çby jedno zdanie minutƒô.\n",
    "    *   *Koszt:* Zu≈ºywa VRAM (pamiƒôƒá karty) na przechowywanie kluczy i warto≈õci. Im d≈Çu≈ºsza rozmowa, tym wiƒôcej pamiƒôci zajmuje cache.\n",
    "2.  **Flash Attention** pozwala na obs≈Çugƒô **d≈Çugich kontekst√≥w** (np. GPT-4 Turbo 128k, Claude 200k). Bez tego macierz uwagi po prostu by siƒô nie zmie≈õci≈Ça w pamiƒôci.\n",
    "\n",
    "Jako in≈ºynier AI, musisz wiedzieƒá, ≈ºe **\"Memory is the bottleneck\"**. Wiƒôkszo≈õƒá nowoczesnych optymalizacji (PagedAttention w vLLM) polega na lepszym zarzƒÖdzaniu KV Cachem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-handbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
