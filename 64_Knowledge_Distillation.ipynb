{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aebb050",
   "metadata": {},
   "source": [
    "# ğŸ§ª Knowledge Distillation: Nauczyciel i UczeÅ„\n",
    "\n",
    "Jak zmieÅ›ciÄ‡ inteligencjÄ™ GPT-4 w maÅ‚ym modelu na telefon?\n",
    "Nie wystarczy zmniejszyÄ‡ sieci (kwantyzacja). Trzeba jÄ… \"nauczyÄ‡ sprytu\".\n",
    "\n",
    "**Ideologia:**\n",
    "DuÅ¼y model (Teacher) widzi wiÄ™cej niÅ¼ tylko wynik.\n",
    "PatrzÄ…c na zdjÄ™cie psa, Teacher myÅ›li: *\"To jest Pies na 90%, ale na 9% wyglÄ…da jak Kot (bo ma uszy), a na 1% jak Auto\"*.\n",
    "ZwykÅ‚a etykieta mÃ³wi tylko: *\"To jest Pies\"*.\n",
    "\n",
    "UczÄ…c maÅ‚y model (Student) tylko z etykiet, tracimy informacjÄ™ o tym podobieÅ„stwie do Kota.\n",
    "Dlatego kaÅ¼emy Studentowi naÅ›ladowaÄ‡ **Logity Nauczyciela** (zmiÄ™kczone TemperaturÄ…).\n",
    "\n",
    "WzÃ³r na stratÄ™ (Loss):\n",
    "$$ Loss = \\alpha \\cdot L_{soft}(Student, Teacher) + (1-\\alpha) \\cdot L_{hard}(Student, Labels) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "790bf892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UrzÄ…dzenie: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "# Konfiguracja\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Dane (MNIST)\n",
    "train_loader = DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "    batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=1000, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"UrzÄ…dzenie: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0439081",
   "metadata": {},
   "source": [
    "## Definicja Modeli\n",
    "\n",
    "1.  **Teacher:** DuÅ¼a, gÅ‚Ä™boka sieÄ‡ (duÅ¼o neuronÃ³w, Dropout). Powinna osiÄ…gnÄ…Ä‡ Å›wietny wynik.\n",
    "2.  **Student:** Malutka sieÄ‡. Ma maÅ‚o parametrÃ³w, wiÄ™c sama z siebie uczy siÄ™ sÅ‚abo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63c599ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher parameters: 2,395,210\n",
      "Student parameters: 15,910\n",
      "Student jest 150.5x mniejszy!\n"
     ]
    }
   ],
   "source": [
    "class TeacherNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 1200)\n",
    "        self.fc2 = nn.Linear(1200, 1200)\n",
    "        self.fc3 = nn.Linear(1200, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.dropout(self.fc1(x)))\n",
    "        x = F.relu(self.dropout(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class StudentNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Student jest duÅ¼o mniejszy! (Mniej warstw, mniej neuronÃ³w)\n",
    "        self.fc1 = nn.Linear(784, 20) \n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Inicjalizacja\n",
    "teacher = TeacherNet().to(DEVICE)\n",
    "student = StudentNet().to(DEVICE)\n",
    "\n",
    "# Policzmy parametry\n",
    "teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "student_params = sum(p.numel() for p in student.parameters())\n",
    "\n",
    "print(f\"Teacher parameters: {teacher_params:,}\")\n",
    "print(f\"Student parameters: {student_params:,}\")\n",
    "print(f\"Student jest {teacher_params/student_params:.1f}x mniejszy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043df3ce",
   "metadata": {},
   "source": [
    "## Krok 1: Trening Nauczyciela (Normalny)\n",
    "\n",
    "Najpierw musimy mieÄ‡ mÄ…drego nauczyciela. Trenujemy go standardowo (Cross Entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba4d3b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ TrenujÄ™ Nauczyciela...\n",
      "Nauczyciel Accuracy: 97.51%\n"
     ]
    }
   ],
   "source": [
    "def train(model, optimizer, loss_fn, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    return 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "print(\"ğŸ“ TrenujÄ™ Nauczyciela...\")\n",
    "optimizer_t = optim.Adam(teacher.parameters(), lr=0.001)\n",
    "train(teacher, optimizer_t, nn.CrossEntropyLoss(), epochs=3)\n",
    "acc_teacher = evaluate(teacher)\n",
    "print(f\"Nauczyciel Accuracy: {acc_teacher:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2f33e4",
   "metadata": {},
   "source": [
    "## Krok 2: Destylacja (Loss Function)\n",
    "\n",
    "To jest serce algorytmu.\n",
    "Funkcja kosztu skÅ‚ada siÄ™ z dwÃ³ch czÄ™Å›ci:\n",
    "1.  **Soft Loss:** PorÃ³wnujemy *zmiÄ™kczone* wyniki Studenta ze *zmiÄ™kczonymi* wynikami Nauczyciela (KL Divergence).\n",
    "    *   UÅ¼ywamy `Temperature` (T), Å¼eby \"rozmyÄ‡\" pewnoÅ›Ä‡ nauczyciela i pokazaÄ‡ niuanse.\n",
    "2.  **Hard Loss:** PorÃ³wnujemy wynik Studenta z prawdziwÄ… etykietÄ… (Cross Entropy).\n",
    "\n",
    "Parametr `alpha` (np. 0.7) decyduje, jak bardzo Student ma sÅ‚uchaÄ‡ Nauczyciela, a jak bardzo patrzeÄ‡ na prawdÄ™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87286885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gotowi do lekcji.\n"
     ]
    }
   ],
   "source": [
    "def distillation_loss(student_logits, teacher_logits, labels, T=5.0, alpha=0.7):\n",
    "    # 1. Hard Loss (Student vs Prawda)\n",
    "    hard_loss = F.cross_entropy(student_logits, labels)\n",
    "    \n",
    "    # 2. Soft Loss (Student vs Teacher)\n",
    "    # KL Divergence oczekuje log-probabilities na wejÅ›ciu i probabilities na celu\n",
    "    soft_loss = nn.KLDivLoss(reduction=\"batchmean\")(\n",
    "        F.log_softmax(student_logits / T, dim=1),\n",
    "        F.softmax(teacher_logits / T, dim=1)\n",
    "    )\n",
    "    \n",
    "    # WaÅ¼ne: MnoÅ¼ymy soft_loss przez T^2, Å¼eby wyrÃ³wnaÄ‡ gradienty\n",
    "    return alpha * (soft_loss * (T * T)) + (1. - alpha) * hard_loss\n",
    "\n",
    "# Funkcja treningowa z destylacjÄ…\n",
    "def train_distill(student, teacher, optimizer, epochs=3, T=5.0, alpha=0.7):\n",
    "    student.train()\n",
    "    teacher.eval() # Nauczyciel siÄ™ nie uczy! Tylko podpowiada.\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass obu modeli\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(data)\n",
    "            student_logits = student(data)\n",
    "            \n",
    "            # Specjalny Loss\n",
    "            loss = distillation_loss(student_logits, teacher_logits, target, T, alpha)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "print(\"Gotowi do lekcji.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12fc7c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘¶ Student uczy siÄ™ sam...\n",
      "Czas: 16.1s\n",
      "\n",
      "ğŸ‘¨â€ğŸ« Student uczy siÄ™ z Nauczycielem...\n",
      "Czas: 17.2s\n",
      "------------------------------\n",
      "Teacher Accuracy:   97.51%\n",
      "Student (Sam):      93.81%\n",
      "Student (Distilled): 91.88%\n"
     ]
    }
   ],
   "source": [
    "# Eksperyment A: Student uczy siÄ™ sam (od zera, bez pomocy)\n",
    "student_alone = StudentNet().to(DEVICE)\n",
    "optimizer_s = optim.Adam(student_alone.parameters(), lr=0.001)\n",
    "\n",
    "print(\"ğŸ‘¶ Student uczy siÄ™ sam...\")\n",
    "start = time.time()\n",
    "train(student_alone, optimizer_s, nn.CrossEntropyLoss(), epochs=3)\n",
    "print(f\"Czas: {time.time()-start:.1f}s\")\n",
    "acc_alone = evaluate(student_alone)\n",
    "\n",
    "\n",
    "# Eksperyment B: Student uczy siÄ™ od Nauczyciela (Destylacja)\n",
    "# Resetujemy studenta\n",
    "student_distilled = StudentNet().to(DEVICE)\n",
    "optimizer_d = optim.Adam(student_distilled.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\nğŸ‘¨â€ğŸ« Student uczy siÄ™ z Nauczycielem...\")\n",
    "start = time.time()\n",
    "train_distill(student_distilled, teacher, optimizer_d, epochs=3, T=7.0, alpha=0.8)\n",
    "print(f\"Czas: {time.time()-start:.1f}s\")\n",
    "acc_distilled = evaluate(student_distilled)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Teacher Accuracy:   {acc_teacher:.2f}%\")\n",
    "print(f\"Student (Sam):      {acc_alone:.2f}%\")\n",
    "print(f\"Student (Distilled): {acc_distilled:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4d9925",
   "metadata": {},
   "source": [
    "## ğŸ§  Podsumowanie: Dark Knowledge\n",
    "\n",
    "PowinieneÅ› zobaczyÄ‡, Å¼e **Student (Distilled)** ma lepszy wynik niÅ¼ **Student (Sam)**, mimo Å¼e majÄ… **dokÅ‚adnie takÄ… samÄ…, maÅ‚Ä… architekturÄ™**.\n",
    "\n",
    "Dlaczego?\n",
    "Bo Nauczyciel przekazaÅ‚ mu \"CiemnÄ… WiedzÄ™\" (Dark Knowledge).\n",
    "*   Gdy Nauczyciel widzi \"7\", mÃ³wi: *\"To 7, ale trochÄ™ wyglÄ…da jak 1\"*.\n",
    "*   Student (Sam) widzi tylko: *\"To jest 7\"*. JeÅ›li zobaczy krzywÄ… siÃ³demkÄ™, zgÅ‚upieje.\n",
    "*   Student (Distilled) wie: *\"Aha, siÃ³demki mogÄ… przypominaÄ‡ jedynki\"*. NauczyÅ‚ siÄ™ **relacji miÄ™dzy klasami**.\n",
    "\n",
    "To technika obowiÄ…zkowa przy wdraÅ¼aniu modeli na urzÄ…dzenia mobilne (Edge AI)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-handbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
