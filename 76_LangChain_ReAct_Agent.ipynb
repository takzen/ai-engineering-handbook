{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8361772",
   "metadata": {},
   "source": [
    "# üïµÔ∏è ReAct Agent: Jak AI u≈ºywa narzƒôdzi?\n",
    "\n",
    "Modele jƒôzykowe nie umiejƒÖ liczyƒá (tokenizacja!) i nie majƒÖ dostƒôpu do Google.\n",
    "≈ªeby to naprawiƒá, stosujemy wzorzec **ReAct**.\n",
    "\n",
    "Nie jest to ≈ºadna nowa sieƒá neuronowa. To po prostu sprytny **Prompt Engineering + Pƒôtla w Pythonie**.\n",
    "\n",
    "**Algorytm:**\n",
    "1.  Wstrzykujemy do Promptu instrukcjƒô: *\"Masz dostƒôp do narzƒôdzi: [Kalkulator, Wiki]. U≈ºywaj formatu: Thought -> Action -> Observation\"*.\n",
    "2.  Wysy≈Çamy zapytanie do LLM.\n",
    "3.  Je≈õli LLM wygeneruje tekst `Action: Kalkulator`, nasz skrypt w Pythonie to przechwytuje (Regex).\n",
    "4.  Uruchamiamy prawdziwy kalkulator `eval()`.\n",
    "5.  Doklejamy wynik (`Observation: 42`) do historii rozmowy i wysy≈Çamy z powrotem do LLM.\n",
    "6.  LLM widzi wynik i kontynuuje my≈õlenie.\n",
    "\n",
    "Zbudujemy \"Mini-LangChain\" od zera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e06ed19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Narzƒôdzia gotowe: SEARCH, CALC.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 1. DEFINIUJEMY NARZƒòDZIA (Tools)\n",
    "# To sƒÖ funkcje Pythonowe, kt√≥rych Agent bƒôdzie m√≥g≈Ç u≈ºyƒá.\n",
    "\n",
    "def wikipedia_search(query):\n",
    "    # Symulacja wyszukiwarki (Mock)\n",
    "    # W prawdziwym ≈ºyciu tu by≈Çoby zapytanie do API Wikipedii\n",
    "    db = {\n",
    "        \"Python\": \"Python to jƒôzyk programowania stworzony przez Guido van Rossuma.\",\n",
    "        \"Einstein\": \"Albert Einstein urodzi≈Ç siƒô w 1879 roku.\",\n",
    "        \"Warszawa\": \"Warszawa to stolica Polski, populacja ok. 1.8 mln.\"\n",
    "    }\n",
    "    return db.get(query, \"Nie znaleziono informacji.\")\n",
    "\n",
    "def calculator(expression):\n",
    "    # Prosty kalkulator\n",
    "    try:\n",
    "        return str(eval(expression))\n",
    "    except:\n",
    "        return \"B≈ÇƒÖd oblicze≈Ñ.\"\n",
    "\n",
    "# Rejestr narzƒôdzi\n",
    "tools = {\n",
    "    \"SEARCH\": wikipedia_search,\n",
    "    \"CALC\": calculator\n",
    "}\n",
    "\n",
    "print(\"Narzƒôdzia gotowe: SEARCH, CALC.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a5a081",
   "metadata": {},
   "source": [
    "## Symulacja LLM (Mock)\n",
    "\n",
    "Poniewa≈º nie chcemy tutaj wymagaƒá klucza OpenAI API (p≈Çatnego), stworzymy klasƒô `FakeLLM`, kt√≥ra **udaje**, ≈ºe jest GPT-4.\n",
    "Bƒôdzie mia≈Ça zaszyte \"scenariusze\" odpowiedzi w formacie ReAct.\n",
    "\n",
    "Dziƒôki temu zobaczysz dok≈Çadnie, jak wyglƒÖda tekst przesy≈Çany tam i z powrotem, bez wydawania dolar√≥w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04f1e76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FakeGPT gotowy do symulacji.\n"
     ]
    }
   ],
   "source": [
    "class FakeGPT:\n",
    "    def __init__(self):\n",
    "        self.step = 0\n",
    "        \n",
    "    def generate(self, prompt):\n",
    "        \"\"\"\n",
    "        Ta funkcja udaje GPT-4. W prawdziwym kodzie tutaj by≈Çoby:\n",
    "        response = openai.ChatCompletion.create(...)\n",
    "        \"\"\"\n",
    "        print(f\"\\n--- [LLM MY≈öLI...] ---\\n(Otrzyma≈Ç prompt o d≈Çugo≈õci {len(prompt)} znak√≥w)\")\n",
    "        \n",
    "        # Scenariusz: Pytanie \"Ile lat mia≈Çby Einstein dzisiaj (2025)?\"\n",
    "        # Krok 1: Model chce sprawdziƒá, kiedy urodzi≈Ç siƒô Einstein.\n",
    "        if \"Action:\" not in prompt and self.step == 0:\n",
    "            self.step += 1\n",
    "            return \"Thought: Muszƒô sprawdziƒá datƒô urodzenia Einsteina.\\nAction: SEARCH[Einstein]\"\n",
    "        \n",
    "        # Krok 2: Model dosta≈Ç datƒô (1879) i teraz chce policzyƒá wiek.\n",
    "        elif \"Observation: Albert Einstein urodzi≈Ç siƒô w 1879 roku.\" in prompt and self.step == 1:\n",
    "            self.step += 1\n",
    "            return \"Thought: Wiem, ≈ºe urodzi≈Ç siƒô w 1879. Mamy rok 2025. Muszƒô policzyƒá r√≥≈ºnicƒô.\\nAction: CALC[2025-1879]\"\n",
    "        \n",
    "        # Krok 3: Model dosta≈Ç wynik oblicze≈Ñ i ko≈Ñczy zadanie.\n",
    "        elif \"Observation: 146\" in prompt:\n",
    "            return \"Thought: Mam ju≈º wynik.\\nFinal Answer: Albert Einstein mia≈Çby dzisiaj 146 lat.\"\n",
    "            \n",
    "        else:\n",
    "            return \"Error: Nie wiem co robiƒá.\"\n",
    "\n",
    "llm = FakeGPT()\n",
    "print(\"FakeGPT gotowy do symulacji.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d132231d",
   "metadata": {},
   "source": [
    "## Pƒôtla Agenta (The Loop)\n",
    "\n",
    "To jest serce LangChaina.\n",
    "1.  Wstawiamy pytanie do Promptu.\n",
    "2.  **While True:**\n",
    "    *   Zapytaj LLM.\n",
    "    *   Czy w odpowiedzi jest `Final Answer`? -> Koniec.\n",
    "    *   Czy w odpowiedzi jest `Action: NARZƒòDZIE[WSAD]`?\n",
    "    *   Je≈õli tak -> Uruchom funkcjƒô Python, we≈∫ wynik.\n",
    "    *   Doklej `Observation: WYNIK` do historii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84f19622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [LLM MY≈öLI...] ---\n",
      "(Otrzyma≈Ç prompt o d≈Çugo≈õci 423 znak√≥w)\n",
      "ü§ñ AI: Error: Nie wiem co robiƒá.\n",
      "------------------------------\n",
      "üí° WYNIK KO≈ÉCOWY: Nie uda≈Ço siƒô znale≈∫ƒá odpowiedzi.\n"
     ]
    }
   ],
   "source": [
    "# System Prompt (Instrukcja dla modelu)\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Jeste≈õ inteligentnym agentem. Masz dostƒôp do narzƒôdzi:\n",
    "- SEARCH[query]: Wyszukuje w Wikipedii.\n",
    "- CALC[expression]: Liczy wyra≈ºenia matematyczne.\n",
    "\n",
    "U≈ºywaj formatu:\n",
    "Question: Pytanie u≈ºytkownika\n",
    "Thought: Tw√≥j proces my≈õlowy\n",
    "Action: NARZƒòDZIE[argument]\n",
    "Observation: Wynik z narzƒôdzia\n",
    "... (powt√≥rz Thought/Action/Observation)\n",
    "Final Answer: Ostateczna odpowied≈∫\n",
    "\"\"\"\n",
    "\n",
    "def run_agent(question):\n",
    "    # Historia rozmowy zaczyna siƒô od instrukcji i pytania\n",
    "    history = SYSTEM_PROMPT + f\"\\nQuestion: {question}\\n\"\n",
    "    \n",
    "    for i in range(5): # Max 5 krok√≥w (zabezpieczenie przed pƒôtlƒÖ)\n",
    "        # 1. Zapytaj Model\n",
    "        response = llm.generate(history)\n",
    "        \n",
    "        # Dodaj odpowied≈∫ modelu do historii\n",
    "        history += response + \"\\n\"\n",
    "        print(f\"ü§ñ AI: {response}\")\n",
    "        \n",
    "        # 2. Sprawd≈∫, czy koniec\n",
    "        if \"Final Answer:\" in response:\n",
    "            return response.split(\"Final Answer:\")[1].strip()\n",
    "        \n",
    "        # 3. Parsowanie Akcji (Regex)\n",
    "        # Szukamy wzorca: Action: NAZWA[argument]\n",
    "        match = re.search(r\"Action: (\\w+)\\[(.*?)\\]\", response)\n",
    "        \n",
    "        if match:\n",
    "            tool_name = match.group(1)\n",
    "            tool_input = match.group(2)\n",
    "            \n",
    "            # 4. Wykonanie Narzƒôdzia\n",
    "            if tool_name in tools:\n",
    "                print(f\"‚öôÔ∏è SYSTEM: Uruchamiam {tool_name} z argumentem '{tool_input}'...\")\n",
    "                tool_result = tools[tool_name](tool_input)\n",
    "                \n",
    "                # 5. Doklejenie Obserwacji\n",
    "                observation = f\"Observation: {tool_result}\"\n",
    "                history += observation + \"\\n\"\n",
    "                print(f\"üëÄ SYSTEM: {observation}\")\n",
    "            else:\n",
    "                history += \"Observation: Nie ma takiego narzƒôdzia.\\n\"\n",
    "        else:\n",
    "            # Model co≈õ wymamrota≈Ç bez akcji, ko≈Ñczymy\n",
    "            break\n",
    "            \n",
    "    return \"Nie uda≈Ço siƒô znale≈∫ƒá odpowiedzi.\"\n",
    "\n",
    "# URUCHAMIAMY!\n",
    "pytanie = \"Ile lat mia≈Çby Einstein dzisiaj (zak≈ÇadajƒÖc rok 2025)?\"\n",
    "wynik = run_agent(pytanie)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"üí° WYNIK KO≈ÉCOWY: {wynik}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2dabcb",
   "metadata": {},
   "source": [
    "## üß† Podsumowanie: Prompt jako Program\n",
    "\n",
    "To, co w≈Ça≈õnie zobaczy≈Çe≈õ, to mechanizm dzia≈Çania **LangChain** i **AutoGPT**.\n",
    "\n",
    "Zauwa≈º, co siƒô sta≈Ço:\n",
    "1.  Model sam \"zorientowa≈Ç siƒô\", ≈ºe nie zna wieku Einsteina.\n",
    "2.  Zdecydowa≈Ç u≈ºyƒá `SEARCH`.\n",
    "3.  Dosta≈Ç datƒô, ale nie umia≈Ç odjƒÖƒá w pamiƒôci (lub wola≈Ç nie ryzykowaƒá).\n",
    "4.  Zdecydowa≈Ç u≈ºyƒá `CALC`.\n",
    "5.  Dosta≈Ç wynik i sformu≈Çowa≈Ç odpowied≈∫.\n",
    "\n",
    "**To jest In≈ºynieria System√≥w AI.**\n",
    "Nie trenowali≈õmy tu ≈ºadnej sieci. Zbudowali≈õmy **≈õrodowisko (pƒôtlƒô)**, w kt√≥rym model mo≈ºe dzia≈Çaƒá autonomicznie.\n",
    "W prawdziwym ≈õwiecie podmieniasz `FakeGPT` na `openai.ChatCompletion`, a `tools` na Google Search API i masz gotowego asystenta."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-handbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
