{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n<a href=\"https://colab.research.google.com/github/takzen/ai-engineering-handbook/blob/main/76_LangChain_ReAct_Agent.ipynb\" target=\"_parent\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8361772",
   "metadata": {},
   "source": [
    "# \ud83d\udd75\ufe0f ReAct Agent: Jak AI u\u017cywa narz\u0119dzi?\n",
    "\n",
    "Modele j\u0119zykowe nie umiej\u0105 liczy\u0107 (tokenizacja!) i nie maj\u0105 dost\u0119pu do Google.\n",
    "\u017beby to naprawi\u0107, stosujemy wzorzec **ReAct**.\n",
    "\n",
    "Nie jest to \u017cadna nowa sie\u0107 neuronowa. To po prostu sprytny **Prompt Engineering + P\u0119tla w Pythonie**.\n",
    "\n",
    "**Algorytm:**\n",
    "1.  Wstrzykujemy do Promptu instrukcj\u0119: *\"Masz dost\u0119p do narz\u0119dzi: [Kalkulator, Wiki]. U\u017cywaj formatu: Thought -> Action -> Observation\"*.\n",
    "2.  Wysy\u0142amy zapytanie do LLM.\n",
    "3.  Je\u015bli LLM wygeneruje tekst `Action: Kalkulator`, nasz skrypt w Pythonie to przechwytuje (Regex).\n",
    "4.  Uruchamiamy prawdziwy kalkulator `eval()`.\n",
    "5.  Doklejamy wynik (`Observation: 42`) do historii rozmowy i wysy\u0142amy z powrotem do LLM.\n",
    "6.  LLM widzi wynik i kontynuuje my\u015blenie.\n",
    "\n",
    "Zbudujemy \"Mini-LangChain\" od zera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e06ed19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Narz\u0119dzia gotowe: SEARCH, CALC.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 1. DEFINIUJEMY NARZ\u0118DZIA (Tools)\n",
    "# To s\u0105 funkcje Pythonowe, kt\u00f3rych Agent b\u0119dzie m\u00f3g\u0142 u\u017cy\u0107.\n",
    "\n",
    "def wikipedia_search(query):\n",
    "    # Symulacja wyszukiwarki (Mock)\n",
    "    # W prawdziwym \u017cyciu tu by\u0142oby zapytanie do API Wikipedii\n",
    "    db = {\n",
    "        \"Python\": \"Python to j\u0119zyk programowania stworzony przez Guido van Rossuma.\",\n",
    "        \"Einstein\": \"Albert Einstein urodzi\u0142 si\u0119 w 1879 roku.\",\n",
    "        \"Warszawa\": \"Warszawa to stolica Polski, populacja ok. 1.8 mln.\"\n",
    "    }\n",
    "    return db.get(query, \"Nie znaleziono informacji.\")\n",
    "\n",
    "def calculator(expression):\n",
    "    # Prosty kalkulator\n",
    "    try:\n",
    "        return str(eval(expression))\n",
    "    except:\n",
    "        return \"B\u0142\u0105d oblicze\u0144.\"\n",
    "\n",
    "# Rejestr narz\u0119dzi\n",
    "tools = {\n",
    "    \"SEARCH\": wikipedia_search,\n",
    "    \"CALC\": calculator\n",
    "}\n",
    "\n",
    "print(\"Narz\u0119dzia gotowe: SEARCH, CALC.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a5a081",
   "metadata": {},
   "source": [
    "## Symulacja LLM (Mock)\n",
    "\n",
    "Poniewa\u017c nie chcemy tutaj wymaga\u0107 klucza OpenAI API (p\u0142atnego), stworzymy klas\u0119 `FakeLLM`, kt\u00f3ra **udaje**, \u017ce jest GPT-4.\n",
    "B\u0119dzie mia\u0142a zaszyte \"scenariusze\" odpowiedzi w formacie ReAct.\n",
    "\n",
    "Dzi\u0119ki temu zobaczysz dok\u0142adnie, jak wygl\u0105da tekst przesy\u0142any tam i z powrotem, bez wydawania dolar\u00f3w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04f1e76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FakeGPT gotowy do symulacji.\n"
     ]
    }
   ],
   "source": [
    "class FakeGPT:\n",
    "    def __init__(self):\n",
    "        self.step = 0\n",
    "        \n",
    "    def generate(self, prompt):\n",
    "        \"\"\"\n",
    "        Ta funkcja udaje GPT-4. W prawdziwym kodzie tutaj by\u0142oby:\n",
    "        response = openai.ChatCompletion.create(...)\n",
    "        \"\"\"\n",
    "        print(f\"\\n--- [LLM MY\u015aLI...] ---\\n(Otrzyma\u0142 prompt o d\u0142ugo\u015bci {len(prompt)} znak\u00f3w)\")\n",
    "        \n",
    "        # Scenariusz: Pytanie \"Ile lat mia\u0142by Einstein dzisiaj (2025)?\"\n",
    "        # Krok 1: Model chce sprawdzi\u0107, kiedy urodzi\u0142 si\u0119 Einstein.\n",
    "        if \"Action:\" not in prompt and self.step == 0:\n",
    "            self.step += 1\n",
    "            return \"Thought: Musz\u0119 sprawdzi\u0107 dat\u0119 urodzenia Einsteina.\\nAction: SEARCH[Einstein]\"\n",
    "        \n",
    "        # Krok 2: Model dosta\u0142 dat\u0119 (1879) i teraz chce policzy\u0107 wiek.\n",
    "        elif \"Observation: Albert Einstein urodzi\u0142 si\u0119 w 1879 roku.\" in prompt and self.step == 1:\n",
    "            self.step += 1\n",
    "            return \"Thought: Wiem, \u017ce urodzi\u0142 si\u0119 w 1879. Mamy rok 2025. Musz\u0119 policzy\u0107 r\u00f3\u017cnic\u0119.\\nAction: CALC[2025-1879]\"\n",
    "        \n",
    "        # Krok 3: Model dosta\u0142 wynik oblicze\u0144 i ko\u0144czy zadanie.\n",
    "        elif \"Observation: 146\" in prompt:\n",
    "            return \"Thought: Mam ju\u017c wynik.\\nFinal Answer: Albert Einstein mia\u0142by dzisiaj 146 lat.\"\n",
    "            \n",
    "        else:\n",
    "            return \"Error: Nie wiem co robi\u0107.\"\n",
    "\n",
    "llm = FakeGPT()\n",
    "print(\"FakeGPT gotowy do symulacji.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d132231d",
   "metadata": {},
   "source": [
    "## P\u0119tla Agenta (The Loop)\n",
    "\n",
    "To jest serce LangChaina.\n",
    "1.  Wstawiamy pytanie do Promptu.\n",
    "2.  **While True:**\n",
    "    *   Zapytaj LLM.\n",
    "    *   Czy w odpowiedzi jest `Final Answer`? -> Koniec.\n",
    "    *   Czy w odpowiedzi jest `Action: NARZ\u0118DZIE[WSAD]`?\n",
    "    *   Je\u015bli tak -> Uruchom funkcj\u0119 Python, we\u017a wynik.\n",
    "    *   Doklej `Observation: WYNIK` do historii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84f19622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [LLM MY\u015aLI...] ---\n",
      "(Otrzyma\u0142 prompt o d\u0142ugo\u015bci 423 znak\u00f3w)\n",
      "\ud83e\udd16 AI: Error: Nie wiem co robi\u0107.\n",
      "------------------------------\n",
      "\ud83d\udca1 WYNIK KO\u0143COWY: Nie uda\u0142o si\u0119 znale\u017a\u0107 odpowiedzi.\n"
     ]
    }
   ],
   "source": [
    "# System Prompt (Instrukcja dla modelu)\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Jeste\u015b inteligentnym agentem. Masz dost\u0119p do narz\u0119dzi:\n",
    "- SEARCH[query]: Wyszukuje w Wikipedii.\n",
    "- CALC[expression]: Liczy wyra\u017cenia matematyczne.\n",
    "\n",
    "U\u017cywaj formatu:\n",
    "Question: Pytanie u\u017cytkownika\n",
    "Thought: Tw\u00f3j proces my\u015blowy\n",
    "Action: NARZ\u0118DZIE[argument]\n",
    "Observation: Wynik z narz\u0119dzia\n",
    "... (powt\u00f3rz Thought/Action/Observation)\n",
    "Final Answer: Ostateczna odpowied\u017a\n",
    "\"\"\"\n",
    "\n",
    "def run_agent(question):\n",
    "    # Historia rozmowy zaczyna si\u0119 od instrukcji i pytania\n",
    "    history = SYSTEM_PROMPT + f\"\\nQuestion: {question}\\n\"\n",
    "    \n",
    "    for i in range(5): # Max 5 krok\u00f3w (zabezpieczenie przed p\u0119tl\u0105)\n",
    "        # 1. Zapytaj Model\n",
    "        response = llm.generate(history)\n",
    "        \n",
    "        # Dodaj odpowied\u017a modelu do historii\n",
    "        history += response + \"\\n\"\n",
    "        print(f\"\ud83e\udd16 AI: {response}\")\n",
    "        \n",
    "        # 2. Sprawd\u017a, czy koniec\n",
    "        if \"Final Answer:\" in response:\n",
    "            return response.split(\"Final Answer:\")[1].strip()\n",
    "        \n",
    "        # 3. Parsowanie Akcji (Regex)\n",
    "        # Szukamy wzorca: Action: NAZWA[argument]\n",
    "        match = re.search(r\"Action: (\\w+)\\[(.*?)\\]\", response)\n",
    "        \n",
    "        if match:\n",
    "            tool_name = match.group(1)\n",
    "            tool_input = match.group(2)\n",
    "            \n",
    "            # 4. Wykonanie Narz\u0119dzia\n",
    "            if tool_name in tools:\n",
    "                print(f\"\u2699\ufe0f SYSTEM: Uruchamiam {tool_name} z argumentem '{tool_input}'...\")\n",
    "                tool_result = tools[tool_name](tool_input)\n",
    "                \n",
    "                # 5. Doklejenie Obserwacji\n",
    "                observation = f\"Observation: {tool_result}\"\n",
    "                history += observation + \"\\n\"\n",
    "                print(f\"\ud83d\udc40 SYSTEM: {observation}\")\n",
    "            else:\n",
    "                history += \"Observation: Nie ma takiego narz\u0119dzia.\\n\"\n",
    "        else:\n",
    "            # Model co\u015b wymamrota\u0142 bez akcji, ko\u0144czymy\n",
    "            break\n",
    "            \n",
    "    return \"Nie uda\u0142o si\u0119 znale\u017a\u0107 odpowiedzi.\"\n",
    "\n",
    "# URUCHAMIAMY!\n",
    "pytanie = \"Ile lat mia\u0142by Einstein dzisiaj (zak\u0142adaj\u0105c rok 2025)?\"\n",
    "wynik = run_agent(pytanie)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"\ud83d\udca1 WYNIK KO\u0143COWY: {wynik}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2dabcb",
   "metadata": {},
   "source": [
    "## \ud83e\udde0 Podsumowanie: Prompt jako Program\n",
    "\n",
    "To, co w\u0142a\u015bnie zobaczy\u0142e\u015b, to mechanizm dzia\u0142ania **LangChain** i **AutoGPT**.\n",
    "\n",
    "Zauwa\u017c, co si\u0119 sta\u0142o:\n",
    "1.  Model sam \"zorientowa\u0142 si\u0119\", \u017ce nie zna wieku Einsteina.\n",
    "2.  Zdecydowa\u0142 u\u017cy\u0107 `SEARCH`.\n",
    "3.  Dosta\u0142 dat\u0119, ale nie umia\u0142 odj\u0105\u0107 w pami\u0119ci (lub wola\u0142 nie ryzykowa\u0107).\n",
    "4.  Zdecydowa\u0142 u\u017cy\u0107 `CALC`.\n",
    "5.  Dosta\u0142 wynik i sformu\u0142owa\u0142 odpowied\u017a.\n",
    "\n",
    "**To jest In\u017cynieria System\u00f3w AI.**\n",
    "Nie trenowali\u015bmy tu \u017cadnej sieci. Zbudowali\u015bmy **\u015brodowisko (p\u0119tl\u0119)**, w kt\u00f3rym model mo\u017ce dzia\u0142a\u0107 autonomicznie.\n",
    "W prawdziwym \u015bwiecie podmieniasz `FakeGPT` na `openai.ChatCompletion`, a `tools` na Google Search API i masz gotowego asystenta."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-handbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}