# ğŸ—ºï¸ AI Engineering Roadmap

PeÅ‚na Å›cieÅ¼ka edukacyjna repozytorium AI-Engineering-Handbook. KaÅ¼dy notatnik rozwiÄ…zuje konkretny problem inÅ¼ynierski lub biznesowy.

---

## 00_KDE_Tutorial.ipynb â€“ RozkÅ‚ady GÄ™stoÅ›ci

**Dlaczego?** ZwykÅ‚y histogram jest "kanciasty" i zaleÅ¼y od tego, jak szerokie sÅ‚upki ustawisz. MoÅ¼esz przegapiÄ‡ waÅ¼ne niuanse danych.

**Temat:** Kernel Density Estimation (KDE). Jak narysowaÄ‡ gÅ‚adkÄ… krzywÄ… prawdopodobieÅ„stwa, ktÃ³ra lepiej oddaje rzeczywistoÅ›Ä‡ niÅ¼ sÅ‚upki.

---

## 01_Correlations_and_Significance.ipynb â€“ ZaleÅ¼noÅ›ci

**Dlaczego?** Musisz wiedzieÄ‡, ktÃ³re cechy wpÅ‚ywajÄ… na wynik, zanim zaczniesz trenowaÄ‡ model.

**Temat:** Korelacja Pearsona (liniowa) vs Spearmana (rangowa). Heatmapy i puÅ‚apki interpretacji korelacji.

---

## 02_Advanced_Feature_Selection.ipynb â€“ Selekcja Cech

**Dlaczego?** WiÄ™cej danych â‰  lepiej. Åšmieciowe kolumny mylÄ… model. ZwykÅ‚a korelacja nie widzi zaleÅ¼noÅ›ci nieliniowych (np. paraboli).

**Temat:** SHAP, Mutual Information i Permutation Importance â€“ nowoczesne metody oddzielania sygnaÅ‚u od szumu.

---

## 03_Encoding_Tutorial.ipynb â€“ Zamiana SÅ‚Ã³w na Liczby

**Dlaczego?** Model matematyczny nie rozumie sÅ‚owa "Czerwony" ani "BMW". Rozumie tylko liczby.

**Temat:** One-Hot Encoding, Label Encoding i Ordinal Encoding. Jak nie wprowadziÄ‡ faÅ‚szywej hierarchii do danych.

---

## 04_Statistics_and_Scaling.ipynb â€“ Skalowanie

**Dlaczego?** Algorytmy oparte na odlegÅ‚oÅ›ci (jak KNN) zwariujÄ…, jeÅ›li jedna cecha ma zakres 0-1, a druga 0-10000.

**Temat:** Standaryzacja (StandardScaler) vs Normalizacja (MinMax). Kiedy uÅ¼ywaÄ‡ ktÃ³rego?

---

## 05_Top_p_Top_k.ipynb â€“ Generowanie Tekstu

**Dlaczego?** Dlaczego ChatGPT czasem jest kreatywny, a czasem precyzyjny?

**Temat:** Sampling. Sterowanie losowoÅ›ciÄ… wyboru kolejnego sÅ‚owa (Top-k vs Nucleus Sampling).

---

## 06_Naive_Bayes_Spam.ipynb â€“ Klasyfikacja Tekstu

**Dlaczego?** Czasami potrzebujesz super szybkiego i prostego modelu, ktÃ³ry dziaÅ‚a na maÅ‚ej iloÅ›ci danych (np. prosty anty-spam).

**Temat:** Naiwny Klasyfikator Bayesa. Wykorzystanie prawdopodobieÅ„stwa warunkowego do oceny tekstu (Bag of Words).

---

## 07_Confusion_Matrix_Precision_Recall.ipynb â€“ Metryki BÅ‚Ä™dÃ³w

**Dlaczego?** DokÅ‚adnoÅ›Ä‡ (Accuracy) 99% to kÅ‚amstwo, jeÅ›li w danych masz tylko 1% oszustw (model zgaduje "brak oszustwa" i ma racjÄ™, ale jest bezuÅ¼yteczny).

**Temat:** Macierz PomyÅ‚ek. Precision (Precyzja), Recall (CzuÅ‚oÅ›Ä‡) i F1-Score.

---

## 08_Overfitting_Underfitting.ipynb â€“ Diagnostyka

**Dlaczego?** TwÃ³j model dziaÅ‚a Å›wietnie na treningu, a tragicznie na produkcji.

**Temat:** Bias-Variance Tradeoff. Wizualizacja, jak zbyt skomplikowany model "wkuwa na pamiÄ™Ä‡" szum zamiast uczyÄ‡ siÄ™ reguÅ‚.

---

## 09_K_Means_Clustering.ipynb â€“ Grupowanie

**Dlaczego?** Masz bazÄ™ klientÃ³w, ale nie masz etykiet (nie wiesz, kto jest kim). Chcesz ich podzieliÄ‡ na segmenty.

**Temat:** K-Means. Algorytm nienadzorowany i Metoda Åokcia (Elbow Method) do wyznaczania optymalnej liczby grup.

---

## 10_Decision_Trees.ipynb â€“ Drzewa Decyzyjne

**Dlaczego?** Czasami musisz wiedzieÄ‡ dokÅ‚adnie, DLACZEGO model podjÄ…Å‚ decyzjÄ™ (wymÃ³g prawny/biznesowy).

**Temat:** Drzewa Decyzyjne. Algorytm "White Box", ktÃ³ry tworzy czytelne reguÅ‚y (if-else).

---

## 11_Embeddings_Vector_Space.ipynb â€“ Matematyka SÅ‚Ã³w

**Dlaczego?** Jak sprawiÄ‡, Å¼eby komputer rozumiaÅ‚, Å¼e "KrÃ³l" i "KrÃ³lowa" sÄ… blisko siebie znaczeniowo?

**Temat:** Embeddings. Reprezentacja sÅ‚Ã³w jako wektorÃ³w w przestrzeni wielowymiarowej.

---

## 12_LLM_Temperature.ipynb â€“ Parametry Modelu

**Dlaczego?** Jak dziaÅ‚a suwak "Temperature" w API OpenAI?

**Temat:** Softmax z temperaturÄ…. Jak matematycznie spÅ‚aszczyÄ‡ lub wyostrzyÄ‡ rozkÅ‚ad prawdopodobieÅ„stwa.

---

## 13_Missing_Data_Imputation.ipynb â€“ Braki Danych

**Dlaczego?** Dane w Å›wiecie rzeczywistym sÄ… dziurawe. UsuniÄ™cie wierszy z brakami (NaN) to utrata cennych informacji.

**Temat:** Strategie Imputacji. WypeÅ‚nianie Å›redniÄ…, medianÄ… oraz (najlepsze) inteligentne wypeÅ‚nianie grupowe (Pandas transform).

---

## 14_Random_Forest_Ensemble.ipynb â€“ Bagging

**Dlaczego?** Pojedyncze drzewo decyzyjne jest niestabilne i Å‚atwo wkuwa dane na pamiÄ™Ä‡.

**Temat:** Las Losowy. Metoda "SiÅ‚y TÅ‚umu" â€“ 100 drzew gÅ‚osuje nad wynikiem, co wygÅ‚adza bÅ‚Ä™dy i daje stabilnoÅ›Ä‡.

---

## 15_Gradient_Descent.ipynb â€“ Silnik Uczenia

**Dlaczego?** Jak wÅ‚aÅ›ciwie model "wie", w ktÃ³rÄ… stronÄ™ zmieniÄ‡ wagi, Å¼eby zmniejszyÄ‡ bÅ‚Ä…d?

**Temat:** Symulacja "schodzenia z gÃ³ry". Zrozumienie Learning Rate i mechanizmu optymalizacji.

---

## 16_Neural_Network_Perceptron.ipynb â€“ Pierwszy Neuron

**Dlaczego?** Å»eby zrozumieÄ‡ sieÄ‡, musisz zbudowaÄ‡ jej najmniejszÄ… cegieÅ‚kÄ™.

**Temat:** Perceptron. Implementacja od zera i dowÃ³d, dlaczego pojedynczy neuron nie rozwiÄ…Å¼e problemu XOR.

---

## 17_PCA_Dimensionality_Reduction.ipynb â€“ Redukcja WymiarÃ³w

**Dlaczego?** Masz 100 kolumn, a chcesz narysowaÄ‡ wykres 2D.

**Temat:** PCA (Principal Component Analysis). Matematyczne "rzutowanie cienia" danych wielowymiarowych na pÅ‚aszczyznÄ™.

---

## 18_Cosine_Similarity_Search.ipynb â€“ Wyszukiwanie

**Dlaczego?** Wyszukiwanie po sÅ‚owach kluczowych jest sÅ‚abe. Chcemy szukaÄ‡ po znaczeniu.

**Temat:** PodobieÅ„stwo Kosinusowe. Mierzenie kÄ…ta miÄ™dzy wektorami zamiast odlegÅ‚oÅ›ci (kluczowe w RAG).

---

## 19_Cross_Validation.ipynb â€“ Walidacja

**Dlaczego?** Jeden podziaÅ‚ na Train/Test to hazard. MoÅ¼e miaÅ‚eÅ› szczÄ™Å›cie przy losowaniu?

**Temat:** K-Fold Cross Validation. Trenowanie modelu 5 razy na rÃ³Å¼nych kawaÅ‚kach danych, aby mieÄ‡ pewnoÅ›Ä‡ co do wyniku.

---

## 20_RAG_Architecture_Simulation.ipynb â€“ RAG

**Dlaczego?** ChatGPT nie zna Twoich prywatnych dokumentÃ³w i halucynuje.

**Temat:** Retrieval Augmented Generation. Wyszukiwanie fragmentÃ³w wiedzy w bazie i doklejanie ich do promptu.

---

## 21_MLP_Neural_Network_XOR.ipynb â€“ Sieci Wielowarstwowe

**Dlaczego?** Jak naprawiÄ‡ problem XOR, ktÃ³rego nie umiaÅ‚ rozwiÄ…zaÄ‡ Perceptron?

**Temat:** MLP (Multi-Layer Perceptron). Dodanie warstw ukrytych, ktÃ³re "wyginajÄ… przestrzeÅ„".

---

## 22_Activation_Functions.ipynb â€“ Funkcje Aktywacji

**Dlaczego?** Bez aktywacji sieÄ‡ neuronowa to tylko mnoÅ¼enie macierzy (funkcja liniowa).

**Temat:** PrzeglÄ…d funkcji: ReLU (standard), Sigmoid (prawdopodobieÅ„stwo), Softmax (klasyfikacja).

---

## 23_Tokenization_GPT.ipynb â€“ Tokenizacja

**Dlaczego?** Dlaczego modele AI nie umiejÄ… liczyÄ‡ liter w sÅ‚owach?

**Temat:** BPE (Byte Pair Encoding). Jak tekst jest szatkowany na tokeny przed wejÅ›ciem do modelu (biblioteka Tiktoken).

---

## 24_Self_Attention_Mechanism.ipynb â€“ Mechanizm Uwagi

**Dlaczego?** Jak model rozumie kontekst caÅ‚ego zdania naraz?

**Temat:** Matematyka Attention od zera. Macierze Query, Key, Value i iloczyn skalarny uwzglÄ™dniajÄ…cy waÅ¼noÅ›Ä‡ sÅ‚Ã³w.

---

## 25_Model_Persistence_Pickle_Joblib.ipynb â€“ Zapisywanie Modeli

**Dlaczego?** Nie moÅ¼esz trenowaÄ‡ modelu od nowa za kaÅ¼dym razem, gdy klient wchodzi na stronÄ™.

**Temat:** Serializacja. Zapisywanie wytrenowanego obiektu do pliku (.pkl, .joblib) i wczytywanie go na produkcji.

---

## 26_RAG_Chunking_Strategies.ipynb â€“ Przygotowanie RAG

**Dlaczego?** Nie moÅ¼esz wrzuciÄ‡ caÅ‚ej ksiÄ…Å¼ki do bazy wektorowej w jednym kawaÅ‚ku.

**Temat:** Chunking. Strategie ciÄ™cia tekstu (Fixed Size, Recursive, Overlap), Å¼eby nie gubiÄ‡ wÄ…tku.

---

## 27_Hyperparameter_Tuning_GridSearch.ipynb â€“ Strojenie

**Dlaczego?** Zgadywanie, czy lepsze jest 10 drzew czy 50, to strata czasu.

**Temat:** Grid Search. Metoda "Brute Force" do automatycznego sprawdzania wszystkich kombinacji parametrÃ³w.

---

## 28_Python_Dataclasses_for_ML.ipynb â€“ Czysty Kod (Config)

**Dlaczego?** Trzymanie parametrÃ³w modelu w zwykÅ‚ym sÅ‚owniku prowadzi do literÃ³wek i bÅ‚Ä™dÃ³w, ktÃ³rych nie widaÄ‡ od razu.

**Temat:** dataclasses. Typowanie silne w konfiguracji treningu, aby kod byÅ‚ bezpieczny i podpowiadaÅ‚ skÅ‚adniÄ™.

---

## 29_OOP_Classmethod_Staticmethod.ipynb â€“ Wzorce Projektowe

**Dlaczego?** Jak elegancko stworzyÄ‡ model z pliku konfiguracyjnego, a jak rÄ™cznie?

**Temat:** Metody fabryczne (@classmethod) i narzÄ™dziowe (@staticmethod) w kontekÅ›cie budowania klas ML.

---

## 30_Sklearn_Pipelines.ipynb â€“ Automatyzacja

**Dlaczego?** JeÅ›li robisz czyszczenie danych rÄ™cznie przed modelem, na produkcji zapomnisz o jednym kroku i system padnie.

**Temat:** Budowa rurociÄ…gu (Pipeline), ktÃ³ry skleja Imputer, Scaler i Model w jeden obiekt. Ochrona przed wyciekiem danych.

---

## 31_Custom_Transformers.ipynb â€“ WÅ‚asne Klasy

**Dlaczego?** Gotowe biblioteki nie majÄ… funkcji "WyczyÅ›Ä‡ symbol waluty i usuÅ„ nawiasy".

**Temat:** Pisanie wÅ‚asnych klas dziedziczÄ…cych po BaseEstimator, ktÃ³re moÅ¼na wpiÄ…Ä‡ w Pipeline Scikit-Learn.

---

## 32_PyTorch_Tensors_Autograd.ipynb â€“ Silnik PyTorch

**Dlaczego?** Nie da siÄ™ rÄ™cznie liczyÄ‡ pochodnych dla miliona wag w sieci neuronowej.

**Temat:** Tensors (macierze na GPU) i Autograd â€“ mechanizm, ktÃ³ry automatycznie Å›ledzi obliczenia i liczy gradienty wstecz.

---

## 33_PyTorch_Neural_Network_Class.ipynb â€“ Architektura

**Dlaczego?** Gotowe funkcje typu model.fit() to czarna skrzynka. Aby budowaÄ‡ nowe rzeczy, musisz mieÄ‡ kontrolÄ™.

**Temat:** Budowa klasy nn.Module i rÄ™czne pisanie pÄ™tli treningowej (Forward -> Loss -> Backward -> Step).

---

## 34_Regularization_Lasso_Ridge.ipynb â€“ Regularyzacja

**Dlaczego?** Model, ktÃ³ry uczy siÄ™ za mocno (wielkie wagi), nie radzi sobie z nowymi danymi.

**Temat:** Lasso (L1) i Ridge (L2). Matematyczne "kary" nakÅ‚adane na model, ktÃ³re zmuszajÄ… go do upraszczania rzeczywistoÅ›ci (i zerowania zbÄ™dnych cech w Lasso).

---

## 35_ANOVA_Hypothesis_Testing.ipynb â€“ Testy A/B/C

**Dlaczego?** PorÃ³wnywanie 3 grup (Lek A, Lek B, Placebo) za pomocÄ… zwykÅ‚ego testu parami to bÅ‚Ä…d statystyczny.

**Temat:** Analiza Wariancji (ANOVA), Test F oraz Test Tukeya (Post-hoc) do bezpiecznego porÃ³wnywania wielu grup.

---

## 36_Market_Basket_Apriori.ipynb â€“ ReguÅ‚y Asocjacyjne

**Dlaczego?** Chcesz wiedzieÄ‡: "Kto kupiÅ‚ piwo, kupiÅ‚ teÅ¼ chipsy". To nie jest predykcja, to szukanie wzorcÃ³w.

**Temat:** Algorytm Apriori. Zrozumienie metryk Support, Confidence i najwaÅ¼niejszego: Lift (siÅ‚a reguÅ‚y).

---

## 37_Gradient_Boosting_XGBoost.ipynb â€“ Boosting

**Dlaczego?** Na danych tabelarycznych (Excel) sieci neuronowe czÄ™sto przegrywajÄ…. KrÃ³lem jest Boosting.

**Temat:** XGBoost. Algorytm, w ktÃ³rym kaÅ¼de kolejne drzewo naprawia bÅ‚Ä™dy poprzednika (sekwencyjne uczenie).

---

## 38_CNN_Computer_Vision.ipynb â€“ Widzenie Komputerowe

**Dlaczego?** ZwykÅ‚a sieÄ‡ niszczy strukturÄ™ zdjÄ™cia (spÅ‚aszcza je). Musimy widzieÄ‡ ksztaÅ‚ty i krawÄ™dzie.

**Temat:** Sieci Splotowe (CNN). Warstwy Conv2d (filtry) i MaxPool (zmniejszanie).

---

## 39_RNN_LSTM_Sequence_Models.ipynb â€“ Szeregi Czasowe

**Dlaczego?** ZwykÅ‚a sieÄ‡ nie pamiÄ™ta, co byÅ‚o na poprzednim zdjÄ™ciu/kroku.

**Temat:** LSTM (Long Short-Term Memory). SieÄ‡ z "pamiÄ™ciÄ…", idealna do przewidywania gieÅ‚dy, pogody czy tekstu.

---

## 40_Autoencoder_Anomaly_Detection.ipynb â€“ Detekcja Anomalii

**Dlaczego?** Jak wykryÄ‡ awariÄ™ silnika, skoro masz dane tylko z poprawnej pracy?

**Temat:** Autoenkoder. Uczenie nienadzorowane â€“ sieÄ‡ uczy siÄ™ kompresowaÄ‡ "normÄ™". JeÅ›li nie potrafi czegoÅ› skompresowaÄ‡ (duÅ¼y bÅ‚Ä…d), to znaczy, Å¼e to anomalia.

---

## 41_GAN_Generative_Adversarial_Network.ipynb â€“ Generowanie Obrazu

**Dlaczego?** Jak zmusiÄ‡ sieÄ‡ do tworzenia nowych rzeczy?

**Temat:** GAN. Wojna dwÃ³ch sieci: FaÅ‚szerza (Generator) i Policjanta (Dyskryminator).

---

## 42_Statistics_Masterclass.ipynb â€“ Kompendium Statystyki

**Dlaczego?** Musisz wiedzieÄ‡, czy TwÃ³j wynik to "odkrycie", czy przypadek. Åšrednia arytmetyczna czÄ™sto kÅ‚amie przy zarobkach (Bill Gates w barze).

**Temat:** RozkÅ‚ady (Normalny), Prawo Wielkich Liczb, Test Shapiro-Wilka, PuÅ‚apki P-value i Paradoks Simpsona.

---

## 43_VAE_Variational_Autoencoder.ipynb â€“ Latent Space

**Dlaczego?** ZwykÅ‚a kompresja jest "sztywna". Nie da siÄ™ pÅ‚ynnie zmieniÄ‡ cyfry 1 w 7.

**Temat:** VAE i Reparameterization Trick. Uczenie siÄ™ rozkÅ‚adu prawdopodobieÅ„stwa danych, co pozwala na "morfing".

---

## 44_RL_Q_Learning_FrozenLake.ipynb â€“ Tabular RL

**Dlaczego?** Jak nauczyÄ‡ robota chodziÄ‡, nie pokazujÄ…c mu przykÅ‚adÃ³w, tylko dajÄ…c kary i nagrody?

**Temat:** Q-Learning. Tworzenie "Å›ciÄ…gi" (Tabeli Q), ktÃ³ra mÃ³wi, jaki ruch jest najlepszy w danej sytuacji.

---

## 45_RL_Deep_Q_Learning_DQN.ipynb â€“ Deep RL

**Dlaczego?** W grze takiej jak StarCraft jest za duÅ¼o stanÃ³w, Å¼eby zapisaÄ‡ je w tabeli.

**Temat:** DQN. ZastÄ…pienie tabeli sieciÄ… neuronowÄ…, ktÃ³ra "zgaduje" najlepszy ruch. Replay Buffer i Target Network dla stabilnoÅ›ci.

---

## 46_Transformer_Block_From_Scratch.ipynb â€“ Architektura GPT

**Dlaczego?** Attention to za maÅ‚o. Prawdziwy Transformer to kanapka warstw.

**Temat:** Implementacja peÅ‚nego bloku: Attention -> LayerNorm -> FeedForward -> Residual Connection (Add).

---

## 47_SVM_Kernel_Trick.ipynb â€“ SVM i NieliniowoÅ›Ä‡

**Dlaczego?** Jak rozdzieliÄ‡ dane (czerwone w Å›rodku, niebieskie na zewnÄ…trz), ktÃ³rych nie da siÄ™ przeciÄ…Ä‡ prostÄ… kreskÄ…?

**Temat:** Kernel Trick (RBF). Rzutowanie danych w wyÅ¼szy wymiar, gdzie stajÄ… siÄ™ separowalne liniowo.

---

## 48_tSNE_vs_PCA_Dimensionality_Reduction.ipynb â€“ Wizualizacja Danych

**Dlaczego?** Masz dane 64-wymiarowe. Ekran jest 2D. PCA spÅ‚aszcza dane jak walec (gubiÄ…c strukturÄ™), t-SNE je "rozprostowuje".

**Temat:** Nieliniowa redukcja wymiarÃ³w. PorÃ³wnanie, jak PCA i t-SNE radzÄ… sobie z klastrowaniem cyfr (MNIST).

---

## 49_Object_Detection_IoU.ipynb â€“ Detekcja ObiektÃ³w

**Dlaczego?** SieÄ‡ narysowaÅ‚a ramkÄ™ wokÃ³Å‚ kota. SkÄ…d wiesz, czy trafiÅ‚a dobrze?

**Temat:** Intersection over Union (IoU). Matematyka oceniania, jak bardzo dwie ramki na siebie nachodzÄ….

---

## 50_UNet_Image_Segmentation.ipynb â€“ Segmentacja

**Dlaczego?** Czasem nie wystarczy wiedzieÄ‡ "tu jest rak". Musisz wiedzieÄ‡ dokÅ‚adnie, ktÃ³ry piksel to rak.

**Temat:** Architektura U-Net. PoÅ‚Ä…czenia skrÃ³towe (Skip Connections), ktÃ³re pozwalajÄ… sieci widzieÄ‡ jednoczeÅ›nie kontekst i precyzyjne detale.

---

## 51_Recommender_Systems_SVD.ipynb â€“ Systemy Rekomendacyjne

**Dlaczego?** Masz miliony filmÃ³w i uÅ¼ytkownikÃ³w. Tabela jest pusta w 99%. Jak zgadnÄ…Ä‡ ocenÄ™ filmu, ktÃ³rego nie widziaÅ‚eÅ›?

**Temat:** Faktoryzacja Macierzy (SVD). Rozbicie tabeli na ukryte cechy uÅ¼ytkownikÃ³w i filmÃ³w.

---

## 52_Genetic_Algorithms_Evolution.ipynb â€“ Algorytmy Genetyczne

**Dlaczego?** Gradient nie dziaÅ‚a, gdy problem jest poszarpany lub dyskretny (np. co spakowaÄ‡ do plecaka).

**Temat:** Ewolucja. Symulacja populacji, krzyÅ¼owania i mutacji w celu znalezienia optymalnego rozwiÄ…zania bez uÅ¼ycia pochodnych.

---

## 53_Monte_Carlo_Simulation.ipynb â€“ Symulacje Ryzyka

**Dlaczego?** W finansach nie pytamy "ile zarobiÄ™?", tylko "jaka jest szansa, Å¼e zbankrutujÄ™?". PrzeszÅ‚oÅ›Ä‡ nie gwarantuje przyszÅ‚oÅ›ci.

**Temat:** Generowanie 1000 alternatywnych scenariuszy gieÅ‚dowych (Geometryczne Ruchy Browna) i obliczanie VaR (Value at Risk).

---

## 54_FFT_Signal_Processing.ipynb â€“ Przetwarzanie SygnaÅ‚u

**Dlaczego?** Dane to nie tylko tabelki, to teÅ¼ dÅºwiÄ™k i wibracje. Na wykresie czasowym szumu nie widaÄ‡.

**Temat:** Szybka Transformata Fouriera (FFT). Zamiana osi czasu na oÅ› czÄ™stotliwoÅ›ci, aby "zobaczyÄ‡" i wyciÄ…Ä‡ pisk z nagrania.

---

## 55_LoRA_Fine_Tuning_Math.ipynb â€“ Fine-Tuning

**Dlaczego?** Douczanie modelu GPT waÅ¼Ä…cego 100GB jest niemoÅ¼liwe na laptopie.

**Temat:** LoRA (Low-Rank Adaptation). Matematyczny trik polegajÄ…cy na douczaniu tylko malutkich macierzy-nakÅ‚adek (AdapterÃ³w).

---

## 56_Positional_Encoding_Transformer.ipynb â€“ Czas w Transformerze

**Dlaczego?** Transformer czyta caÅ‚e zdanie naraz (rÃ³wnolegle). Nie wie, co byÅ‚o wczeÅ›niej, a co pÃ³Åºniej.

**Temat:** Positional Encoding. Dodawanie fal sinusoidalnych do wektorÃ³w sÅ‚Ã³w, aby nadaÄ‡ im "sygnaturÄ™ czasu".

---

## 57_Metric_Learning_Siamese.ipynb â€“ FaceID

**Dlaczego?** Telefon nie ma w bazie milionÃ³w twarzy. On sprawdza, czy Twoja twarz jest podobna do tej zapisanej.

**Temat:** Sieci Syjamskie i Triplet Loss. Uczenie sieci mierzenia odlegÅ‚oÅ›ci miÄ™dzy obiektami, a nie ich klasyfikacji.

---

## 58_Graph_Neural_Networks_GNN.ipynb â€“ Grafy

**Dlaczego?** Social media i chemia to nie tabelki. To relacje (kto zna kogo, jaki atom wiÄ…Å¼e siÄ™ z jakim).

**Temat:** Message Passing. Jak wÄ™zÅ‚y w grafie wymieniajÄ… siÄ™ informacjami ze swoimi sÄ…siadami.

---

## 59_Model_Quantization_INT8.ipynb â€“ Kompresja Modeli

**Dlaczego?** Wielkie modele nie mieszczÄ… siÄ™ w pamiÄ™ci telefonu.

**Temat:** Matematyka rzutowania liczb zmiennoprzecinkowych (FP32) na caÅ‚kowite (INT8). Obliczanie Scale i Zero Point.

---

## 60_Bayesian_Optimization_Optuna.ipynb â€“ Optymalizacja

**Dlaczego?** Grid Search sprawdza wszystko "na siÅ‚Ä™" (strata prÄ…du). Random Search to hazard.

**Temat:** Optuna. Algorytm, ktÃ³ry uczy siÄ™ na bÅ‚Ä™dach i inteligentnie dobiera parametry modelu, Å¼eby zmaksymalizowaÄ‡ wynik.

---

## 61_Normalization_Layers_BN_vs_LN.ipynb â€“ Stabilizacja

**Dlaczego?** Bez normalizacji gÅ‚Ä™bokie sieci przestajÄ… siÄ™ uczyÄ‡ (wybuchajÄ…ce gradienty).

**Temat:** RÃ³Å¼nica miÄ™dzy BatchNorm (dla obrazÃ³w), LayerNorm (dla tekstu/TransformerÃ³w) i InstanceNorm.

---

## 62_Time_Series_Decomposition_STL.ipynb â€“ Szeregi Czasowe

**Dlaczego?** Szef pyta: "Dlaczego sprzedaÅ¼ spadÅ‚a?". Musisz wiedzieÄ‡, czy to trend (kryzys), czy sezonowoÅ›Ä‡ (koniec Å›wiÄ…t).

**Temat:** Dekompozycja STL. Rozbicie wykresu na trzy skÅ‚adniki: Trend, SezonowoÅ›Ä‡ i Reszty (Szum/Anomalie).

---

## 63_Diffusion_Models_DDPM.ipynb â€“ Matematyka Stable Diffusion

**Dlaczego?** GANy (notatnik 41) sÄ… trudne w treningu. Dzisiaj krÃ³luje Dyfuzja.

**Temat:** Zaimplementujemy od zera proces Forward Diffusion (dodawanie szumu do zdjÄ™cia aÅ¼ zniknie) i Reverse Diffusion (nauczenie sieci, jak "odszumiaÄ‡" biaÅ‚y szum, Å¼eby powstaÅ‚ obraz).

---

## 64_Knowledge_Distillation.ipynb â€“ Nauczyciel i UczeÅ„

**Dlaczego?** Kwantyzacja (notatnik 59) to nie jedyny sposÃ³b na maÅ‚e modele. MoÅ¼emy wziÄ…Ä‡ wielkiego modelu (Teacher) i kazaÄ‡ mu uczyÄ‡ maÅ‚ego modelu (Student).

**Temat:** Soft Labels i Temperature. Student nie uczy siÄ™ tylko "To jest kot", ale uczy siÄ™ od Nauczyciela, Å¼e "To jest kot, ale trochÄ™ przypomina psa".

---

## 65_HNSW_Vector_Database.ipynb â€“ Silnik Baz Wektorowych

**Dlaczego?** W RAG (notatnik 20) liczyliÅ›my cosinusy dla wszystkich par. To dziaÅ‚a dla 10k plikÃ³w. Dla 100 milionÃ³w potrzebujesz HNSW.

**Temat:** Zbudujemy (uproszczonÄ…) strukturÄ™ grafowÄ…, ktÃ³ra pozwala znaleÅºÄ‡ najbliÅ¼szy wektor w czasie logarytmicznym, skaczÄ…c po "autostradach" miÄ™dzy wÄ™zÅ‚ami.

---

## 66_Kalman_Filter.ipynb â€“ Åšledzenie w czasie rzeczywistym

**Dlaczego?** GPS w Twoim telefonie kÅ‚amie (ma bÅ‚Ä…d 5m). Jakim cudem nawigacja pokazuje CiÄ™ idealnie na drodze?

**Temat:** Algorytm, ktÃ³ry Å‚Ä…czy PredykcjÄ™ (gdzie powinieneÅ› byÄ‡ wg fizyki) z Pomiarem (gdzie jesteÅ› wg czujnika) i magicznie usuwa szum. Fundament robotyki.

---

## 67_Contrastive_Learning_SimCLR.ipynb â€“ SimCLR / CLIP

**Dlaczego?** Jak nauczyÄ‡ sieÄ‡ rozumieÄ‡ obrazy bez etykiet?

**Temat:** Uczenie kontrastowe. Bierzemy zdjÄ™cie psa, robimy mu 2 rÃ³Å¼ne filtry (obrÃ³t, kolor). MÃ³wimy sieci: "Te dwa rÃ³Å¼ne obrazy to to samo. ZbliÅ¼ ich wektory do siebie".

---

## 68_PPO_RLHF.ipynb â€“ Jak powstaÅ‚ ChatGPT?

**Dlaczego?** ZwykÅ‚y model jÄ™zykowy umie tylko dokoÅ„czyÄ‡ zdanie. Å»eby byÅ‚ asystentem, musi przejÅ›Ä‡ RLHF (Reinforcement Learning from Human Feedback).

**Temat:** Zrozumienie mechanizmu PPO â€“ jak nagradzaÄ‡ model jÄ™zykowy za dobre odpowiedzi, nie psujÄ…c jego zdolnoÅ›ci jÄ™zykowych (Kullback-Leibler penalty).

---

## 69_Data_Drift_Detection.ipynb â€“ KS-Test & PSI

**Dlaczego?** WytrenowaÅ‚eÅ› model na danych z 2023. Jest 2025. Model dziaÅ‚a Åºle. Dlaczego? Bo dane siÄ™ zmieniÅ‚y (Drift).

**Temat:** Matematyczne wykrywanie, czy "Å›wiat siÄ™ zmieniÅ‚". Test Kolmogorova-Smirnova i Population Stability Index. ObowiÄ…zkowe w bankowoÅ›ci.

---

## 70_Vision_Transformers_ViT.ipynb â€“ Koniec ery CNN?

**Dlaczego?** CNN (sploty) patrzÄ… lokalnie (okno 3x3). Transformery patrzÄ… globalnie. Od 2021 roku ViT wypiera CNN w rozpoznawaniu obrazÃ³w.

**Temat:** Jak pociÄ…Ä‡ zdjÄ™cie na "sÅ‚owa" (Patches), dodaÄ‡ im pozycjÄ™ i wrzuciÄ‡ do Transformera, ktÃ³rego zbudowaliÅ›my w notatniku 46.

---

## 71_Flash_Attention_KV_Cache.ipynb â€“ Jak dziaÅ‚ajÄ… szybkie LLM?

**Dlaczego?** ZwykÅ‚e Attention ma zÅ‚oÅ¼onoÅ›Ä‡ O(NÂ²). Dla dÅ‚ugich tekstÃ³w pamiÄ™Ä‡ GPU wybucha.

**Temat:** Zrozumienie optymalizacji sprzÄ™towej (Tiling) i mechanizmu Key-Value Cache (nie licz tego samego dwa razy), ktÃ³ry pozwala ChatGPT odpisywaÄ‡ w czasie rzeczywistym.

---

## 72_Mamba_SSM.ipynb â€“ State Space Models

**Dlaczego?** Transformer zatyka siÄ™ przy dÅ‚ugich tekstach. Mamba ma zÅ‚oÅ¼onoÅ›Ä‡ liniowÄ… i pamiÄ™ta nieskoÅ„czenie dÅ‚ugo.

**Temat:** Architektura SSM. Dyskretyzacja rÃ³wnaÅ„ rÃ³Å¼niczkowych w sieci neuronowej.

---

## 73_Mixture_of_Experts_MoE.ipynb â€“ Architektura GPT-4

**Dlaczego?** GPT-4 to nie jeden wielki model, ale "rada ekspertÃ³w".

**Temat:** Budowa warstwy Gating Network (Routera), ktÃ³ra decyduje, ktÃ³ry maÅ‚y model (Ekspert) ma obsÅ‚uÅ¼yÄ‡ dane zapytanie.

---

## 74_Liquid_Neural_Networks.ipynb â€“ LFC

**Dlaczego?** Sieci neuronowe sÄ… statyczne po treningu. Liquid Networks zmieniajÄ… swoje wagi w czasie rzeczywistym, adaptujÄ…c siÄ™ do otoczenia.

**Temat:** Implementacja sieci opartej na rÃ³wnaniach rÃ³Å¼niczkowych, ktÃ³ra dziaÅ‚a Å›wietnie w robotyce i dronach.

---

## 75_Meta_Learning_MAML.ipynb â€“ Model-Agnostic Meta-Learning

**Dlaczego?** CzÅ‚owiek uczy siÄ™ po jednym przykÅ‚adzie. SieÄ‡ potrzebuje tysiÄ™cy.

**Temat:** Model-Agnostic Meta-Learning. Trenowanie sieci tak, aby jej wagi byÅ‚y "Å‚atwe do szybkiego douczenia" na nowym zadaniu w 5 krokach.

---

## 76_LangChain_ReAct.ipynb â€“ Reason + Act

**Dlaczego?** Model jÄ™zykowy nie ma dostÄ™pu do kalkulatora ani Google.

**Temat:** Budowa pÄ™tli Agenta: MyÅ›l -> Wybierz NarzÄ™dzie -> Wykonaj AkcjÄ™ -> Obserwuj Wynik -> Odpowiedz.

---

## 77_Prompt_Engineering_CoT.ipynb â€“ Chain of Thought

**Dlaczego?** Proste pytanie "Ile to 23\*45?" czÄ™sto myli model.

**Temat:** Implementacja CoT i Tree of Thoughts. Wymuszanie na modelu "myÅ›lenia na gÅ‚os" przed podaniem odpowiedzi.

---

## 78_RAG_Evaluation_RAGAS.ipynb â€“ RAGAS

**Dlaczego?** ZbudowaÅ‚eÅ› RAG (notatnik 20). SkÄ…d wiesz, czy jest dobry?

**Temat:** Metryki: Faithfulness (czy odpowiedÅº wynika z kontekstu?) i Answer Relevance (czy odpowiada na pytanie?).

---

## 79_Product_Quantization_IVF_PQ.ipynb â€“ Vector Database

**Dlaczego?** HNSW (notatnik 65) jest szybkie, ale Å¼re RAM.

**Temat:** Kompresja wektorÃ³w (Product Quantization). Jak zmieÅ›ciÄ‡ miliard wektorÃ³w w pamiÄ™ci, tracÄ…c minimalnie na precyzji (technologia FAISS).

---

## 80_Speculative_Decoding.ipynb â€“ Speculative Decoding

**Dlaczego?** Wielki model jest wolny. MaÅ‚y model jest szybki, ale gÅ‚upi.

**Temat:** UÅ¼ycie maÅ‚ego modelu do "zgadywania" 5 kolejnych sÅ‚Ã³w, a duÅ¼ego tylko do ich "zatwierdzania". Przyspieszenie generowania tekstu 2-3x.

---

## 81_Audio_Mel_Spectrograms.ipynb â€“ Spektrogramy Melowe

**Dlaczego?** Surowe audio to chaos. Modele (Whisper) "patrzÄ…" na dÅºwiÄ™k.

**Temat:** Zamiana audio na Mel-Spectrogram (skala logarytmiczna, dopasowana do ucha ludzkiego).

---

## 82_ASR_CTC_Loss.ipynb â€“ Connectionist Temporal Classification

**Dlaczego?** Plik audio trwa 5 sekund, tekst ma 10 liter. Jak to zmapowaÄ‡ bez wyrÃ³wnywania?

**Temat:** Matematyka CTC. Algorytm, ktÃ³ry pozwala sieci "pluÄ‡" literami w odpowiednich momentach i usuwaÄ‡ duplikaty ("heeeelllo" -> "hello").

---

## 83_Text_to_Speech_Tacotron.ipynb â€“ Tacotron Concept

**Dlaczego?** Chcesz, Å¼eby AI do Ciebie mÃ³wiÅ‚o.

**Temat:** Architektura Encoder-Decoder z mechanizmem Attention, ktÃ³ra zamienia litery na spektrogramy audio.

---

## 84_CLIP_Multimodal.ipynb â€“ Contrastive Language-Image Pretraining

**Dlaczego?** Jak poÅ‚Ä…czyÄ‡ tekst z obrazem? (Podstawa DALL-E).

**Temat:** Trening modelu, ktÃ³ry umie powiedzieÄ‡, jak bardzo zdjÄ™cie psa pasuje do podpisu "Pies na plaÅ¼y".

---

## 85_NeRF_Neural_Radiance_Fields.ipynb â€“ Neural Radiance Fields

**Dlaczego?** Jak z 5 zdjÄ™Ä‡ 2D zrobiÄ‡ model 3D?

**Temat:** Reprezentacja sceny jako funkcji F(x,y,z)â†’(Kolor,GÄ™stoÅ›Ä‡). Ray Marching (strzelanie promieniami przez piksele).

---

## 86_PointNet_Point_Clouds.ipynb â€“ Chmury PunktÃ³w

**Dlaczego?** Lidary w samochodach autonomicznych widzÄ… chmury punktÃ³w, a nie zdjÄ™cia.

**Temat:** Architektura, ktÃ³ra przetwarza zbiÃ³r punktÃ³w 3D (niezaleÅ¼nie od ich kolejnoÅ›ci) i klasyfikuje obiekt (np. "To jest krzesÅ‚o").

---

## 87_Graph_Attention_Networks_GAT.ipynb â€“ Graph Attention Networks

**Dlaczego?** ZwykÅ‚y GCN (notatnik 58) traktuje wszystkich sÄ…siadÃ³w tak samo.

**Temat:** Dodanie mechanizmu Attention do grafÃ³w. WÄ™zeÅ‚ sam decyduje, ktÃ³ry sÄ…siad jest waÅ¼ny.

---

## 88_Temporal_Fusion_Transformer_TFT.ipynb â€“ TFT

**Dlaczego?** LSTM jest stary. TFT to obecny SOTA w prognozowaniu (np. zapotrzebowania na prÄ…d).

**Temat:** Transformer, ktÃ³ry obsÅ‚uguje wiele horyzontÃ³w czasowych i zmienne statyczne (np. lokalizacja sklepu) oraz dynamiczne (pogoda).

---

## 89_NeuralProphet.ipynb â€“ NeuralProphet

**Dlaczego?** Prophet od Facebooka jest super, ale wolny.

**Temat:** Hybryda klasycznej dekompozycji (Trend/Sezon) z sieciami neuronowymi w PyTorch.

---

## 90_Isolation_Forest.ipynb â€“ Isolation Forest

**Dlaczego?** Autoenkoder (notatnik 40) jest super, ale czasem potrzebujesz szybkiego algorytmu drzewiastego do anomalii.

**Temat:** Algorytm, ktÃ³ry prÃ³buje "odizolowaÄ‡" punkt losowymi ciÄ™ciami. Im szybciej siÄ™ to uda, tym bardziej punkt jest podejrzany (anomalia).

---

## 91_LIME.ipynb â€“ Local Interpretable Model-agnostic Explanations

**Dlaczego?** SHAP jest dokÅ‚adny, ale wolny.

**Temat:** Jak wyjaÅ›niÄ‡ decyzjÄ™ skomplikowanego modelu, trenujÄ…c wokÃ³Å‚ jednej prÃ³bki maÅ‚y, prosty model liniowy.

---

## 92_Counterfactual_Explanations.ipynb â€“ Counterfactual Explanations

**Dlaczego?** Klient pyta: "Co muszÄ™ zmieniÄ‡, Å¼eby dostaÄ‡ kredyt?".

**Temat:** Generowanie alternatywnych scenariuszy: "GdybyÅ› zarabiaÅ‚ 500 zÅ‚ wiÄ™cej, dostaÅ‚byÅ› kredyt".

---

## 93_Fairness_Bias_Detection.ipynb â€“ Fairness & Bias Detection

**Dlaczego?** TwÃ³j model moÅ¼e dyskryminowaÄ‡.

**Temat:** Metryki sprawiedliwoÅ›ci (Disparate Impact, Equal Opportunity). Sprawdzanie, czy model traktuje tak samo rÃ³Å¼ne grupy spoÅ‚eczne.

---

## 94_Causal_Inference.ipynb â€“ Do-Calculus

**Dlaczego?** Korelacja to nie przyczynowoÅ›Ä‡. Czy reklama zwiÄ™kszyÅ‚a sprzedaÅ¼, czy po prostu byÅ‚y Å›wiÄ™ta?

**Temat:** Drabina Pearla. OdrÃ³Å¼nianie obserwacji P(Yâˆ£X) od interwencji.

---

## 95_Model_Pruning.ipynb â€“ Przycinanie

**Dlaczego?** Wiele wag w sieci jest bliskich zeru. SÄ… niepotrzebne.

**Temat:** Zerowanie najsÅ‚abszych poÅ‚Ä…czeÅ„ w sieci neuronowej i dotrenowanie reszty. Odchudzanie modelu o 50-90%.

---

## 96_ONNX_Runtime.ipynb â€“ ONNX Runtime

**Dlaczego?** PyTorch jest Å›wietny do treningu, ale ciÄ™Å¼ki na produkcji.

**Temat:** Eksport modelu do formatu ONNX (Open Neural Network Exchange), ktÃ³ry dziaÅ‚a na wszystkim (od serwera po przeglÄ…darkÄ™).

---

## 97_Federated_Learning.ipynb â€“ Federated Learning

**Dlaczego?** Dane (np. medyczne) nie mogÄ… opuÅ›ciÄ‡ telefonu pacjenta.

**Temat:** Trenowanie modelu na tysiÄ…cach urzÄ…dzeÅ„ lokalnie i przesyÅ‚anie do serwera tylko aktualizacji wag (a nie danych).

---

## 98_Adversarial_Attacks_FGSM.ipynb â€“ FGSM

**Dlaczego?** Jak oszukaÄ‡ sieÄ‡, zmieniajÄ…c jeden piksel?

**Temat:** Fast Gradient Sign Method. Generowanie szumu, ktÃ³ry jest niewidoczny dla oka, a sprawia, Å¼e AI widzi banana zamiast pistoletu.

---

## 99_Neural_Architecture_Search_NAS.ipynb â€“ NAS

**Dlaczego?** Po co projektowaÄ‡ sieÄ‡ rÄ™cznie, skoro AI moÅ¼e zaprojektowaÄ‡ AI?

**Temat:** Prosty algorytm, ktÃ³ry sam buduje architekturÄ™ sieci (dobiera liczbÄ™ warstw i filtrÃ³w) metodÄ… ewolucyjnÄ….

---

## 100_World_Models.ipynb â€“ Model-Based RL

**Dlaczego?** Agent RL (DQN) uczy siÄ™ wolno, bo musi "przeÅ¼yÄ‡" grÄ™.

**Temat:** Agent, ktÃ³ry najpierw uczy siÄ™ "modelu Å›wiata" (jak dziaÅ‚a fizyka), a potem "Å›ni" (symuluje) w gÅ‚owie przyszÅ‚oÅ›Ä‡, Å¼eby podjÄ…Ä‡ decyzjÄ™.

---

## 101_KAN_Kolmogorov_Arnold_Networks.ipynb â€“ The Grand Finale: KAN

**Dlaczego?** Przez 70 lat fundamentem AI byÅ‚ MLP (staÅ‚a funkcja aktywacji w neuronie, wagi na krawÄ™dziach). W 2024 roku KAN odwrÃ³ciÅ‚ to do gÃ³ry nogami: brak wag, uczymy siÄ™ funkcji (krzywych) na krawÄ™dziach. To "Nowa Fizyka AI".

**Temat:** Implementacja sieci KoÅ‚mogorowa-Arnolda od zera w PyTorch. Matematyka B-Splines (krzywych sklejanych) i budowa warstwy KANLayer, ktÃ³ra jest dokÅ‚adniejsza i bardziej interpretowalna niÅ¼ klasyczne sieci.
