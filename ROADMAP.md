# ğŸ—ºï¸ AI Engineering Roadmap

PeÅ‚na Å›cieÅ¼ka edukacyjna repozytorium AI-Engineering-Handbook. KaÅ¼dy notatnik rozwiÄ…zuje konkretny problem inÅ¼ynierski lub biznesowy.

---

## 00_KDE_Tutorial.ipynb â€“ RozkÅ‚ady GÄ™stoÅ›ci

**Dlaczego?** ZwykÅ‚y histogram jest "kanciasty" i zaleÅ¼y od tego, jak szerokie sÅ‚upki ustawisz. MoÅ¼esz przegapiÄ‡ waÅ¼ne niuanse danych.

**Temat:** Kernel Density Estimation (KDE). Jak narysowaÄ‡ gÅ‚adkÄ… krzywÄ… prawdopodobieÅ„stwa, ktÃ³ra lepiej oddaje rzeczywistoÅ›Ä‡ niÅ¼ sÅ‚upki.

---

## 01_Correlations_and_Significance.ipynb â€“ ZaleÅ¼noÅ›ci

**Dlaczego?** Musisz wiedzieÄ‡, ktÃ³re cechy wpÅ‚ywajÄ… na wynik, zanim zaczniesz trenowaÄ‡ model.

**Temat:** Korelacja Pearsona (liniowa) vs Spearmana (rangowa). Heatmapy i puÅ‚apki interpretacji korelacji.

---

## 02_Advanced_Feature_Selection.ipynb â€“ Selekcja Cech

**Dlaczego?** WiÄ™cej danych â‰  lepiej. Åšmieciowe kolumny mylÄ… model. ZwykÅ‚a korelacja nie widzi zaleÅ¼noÅ›ci nieliniowych (np. paraboli).

**Temat:** SHAP, Mutual Information i Permutation Importance â€“ nowoczesne metody oddzielania sygnaÅ‚u od szumu.

---

## 03_Encoding_Tutorial.ipynb â€“ Zamiana SÅ‚Ã³w na Liczby

**Dlaczego?** Model matematyczny nie rozumie sÅ‚owa "Czerwony" ani "BMW". Rozumie tylko liczby.

**Temat:** One-Hot Encoding, Label Encoding i Ordinal Encoding. Jak nie wprowadziÄ‡ faÅ‚szywej hierarchii do danych.

---

## 04_Statistics_and_Scaling.ipynb â€“ Skalowanie

**Dlaczego?** Algorytmy oparte na odlegÅ‚oÅ›ci (jak KNN) zwariujÄ…, jeÅ›li jedna cecha ma zakres 0-1, a druga 0-10000.

**Temat:** Standaryzacja (StandardScaler) vs Normalizacja (MinMax). Kiedy uÅ¼ywaÄ‡ ktÃ³rego?

---

## 05_Top_p_Top_k.ipynb â€“ Generowanie Tekstu

**Dlaczego?** Dlaczego ChatGPT czasem jest kreatywny, a czasem precyzyjny?

**Temat:** Sampling. Sterowanie losowoÅ›ciÄ… wyboru kolejnego sÅ‚owa (Top-k vs Nucleus Sampling).

---

## 06_Naive_Bayes_Spam.ipynb â€“ Klasyfikacja Tekstu

**Dlaczego?** Czasami potrzebujesz super szybkiego i prostego modelu, ktÃ³ry dziaÅ‚a na maÅ‚ej iloÅ›ci danych (np. prosty anty-spam).

**Temat:** Naiwny Klasyfikator Bayesa. Wykorzystanie prawdopodobieÅ„stwa warunkowego do oceny tekstu (Bag of Words).

---

## 07_Confusion_Matrix_Precision_Recall.ipynb â€“ Metryki BÅ‚Ä™dÃ³w

**Dlaczego?** DokÅ‚adnoÅ›Ä‡ (Accuracy) 99% to kÅ‚amstwo, jeÅ›li w danych masz tylko 1% oszustw (model zgaduje "brak oszustwa" i ma racjÄ™, ale jest bezuÅ¼yteczny).

**Temat:** Macierz PomyÅ‚ek. Precision (Precyzja), Recall (CzuÅ‚oÅ›Ä‡) i F1-Score.

---

## 08_Overfitting_Underfitting.ipynb â€“ Diagnostyka

**Dlaczego?** TwÃ³j model dziaÅ‚a Å›wietnie na treningu, a tragicznie na produkcji.

**Temat:** Bias-Variance Tradeoff. Wizualizacja, jak zbyt skomplikowany model "wkuwa na pamiÄ™Ä‡" szum zamiast uczyÄ‡ siÄ™ reguÅ‚.

---

## 09_K_Means_Clustering.ipynb â€“ Grupowanie

**Dlaczego?** Masz bazÄ™ klientÃ³w, ale nie masz etykiet (nie wiesz, kto jest kim). Chcesz ich podzieliÄ‡ na segmenty.

**Temat:** K-Means. Algorytm nienadzorowany i Metoda Åokcia (Elbow Method) do wyznaczania optymalnej liczby grup.

---

## 10_Decision_Trees.ipynb â€“ Drzewa Decyzyjne

**Dlaczego?** Czasami musisz wiedzieÄ‡ dokÅ‚adnie, DLACZEGO model podjÄ…Å‚ decyzjÄ™ (wymÃ³g prawny/biznesowy).

**Temat:** Drzewa Decyzyjne. Algorytm "White Box", ktÃ³ry tworzy czytelne reguÅ‚y (if-else).

---

## 11_Embeddings_Vector_Space.ipynb â€“ Matematyka SÅ‚Ã³w

**Dlaczego?** Jak sprawiÄ‡, Å¼eby komputer rozumiaÅ‚, Å¼e "KrÃ³l" i "KrÃ³lowa" sÄ… blisko siebie znaczeniowo?

**Temat:** Embeddings. Reprezentacja sÅ‚Ã³w jako wektorÃ³w w przestrzeni wielowymiarowej.

---

## 12_LLM_Temperature.ipynb â€“ Parametry Modelu

**Dlaczego?** Jak dziaÅ‚a suwak "Temperature" w API OpenAI?

**Temat:** Softmax z temperaturÄ…. Jak matematycznie spÅ‚aszczyÄ‡ lub wyostrzyÄ‡ rozkÅ‚ad prawdopodobieÅ„stwa.

---

## 13_Missing_Data_Imputation.ipynb â€“ Braki Danych

**Dlaczego?** Dane w Å›wiecie rzeczywistym sÄ… dziurawe. UsuniÄ™cie wierszy z brakami (NaN) to utrata cennych informacji.

**Temat:** Strategie Imputacji. WypeÅ‚nianie Å›redniÄ…, medianÄ… oraz (najlepsze) inteligentne wypeÅ‚nianie grupowe (Pandas transform).

---

## 14_Random_Forest_Ensemble.ipynb â€“ Bagging

**Dlaczego?** Pojedyncze drzewo decyzyjne jest niestabilne i Å‚atwo wkuwa dane na pamiÄ™Ä‡.

**Temat:** Las Losowy. Metoda "SiÅ‚y TÅ‚umu" â€“ 100 drzew gÅ‚osuje nad wynikiem, co wygÅ‚adza bÅ‚Ä™dy i daje stabilnoÅ›Ä‡.

---

## 15_Gradient_Descent.ipynb â€“ Silnik Uczenia

**Dlaczego?** Jak wÅ‚aÅ›ciwie model "wie", w ktÃ³rÄ… stronÄ™ zmieniÄ‡ wagi, Å¼eby zmniejszyÄ‡ bÅ‚Ä…d?

**Temat:** Symulacja "schodzenia z gÃ³ry". Zrozumienie Learning Rate i mechanizmu optymalizacji.

---

## 16_Neural_Network_Perceptron.ipynb â€“ Pierwszy Neuron

**Dlaczego?** Å»eby zrozumieÄ‡ sieÄ‡, musisz zbudowaÄ‡ jej najmniejszÄ… cegieÅ‚kÄ™.

**Temat:** Perceptron. Implementacja od zera i dowÃ³d, dlaczego pojedynczy neuron nie rozwiÄ…Å¼e problemu XOR.

---

## 17_PCA_Dimensionality_Reduction.ipynb â€“ Redukcja WymiarÃ³w

**Dlaczego?** Masz 100 kolumn, a chcesz narysowaÄ‡ wykres 2D.

**Temat:** PCA (Principal Component Analysis). Matematyczne "rzutowanie cienia" danych wielowymiarowych na pÅ‚aszczyznÄ™.

---

## 18_Cosine_Similarity_Search.ipynb â€“ Wyszukiwanie

**Dlaczego?** Wyszukiwanie po sÅ‚owach kluczowych jest sÅ‚abe. Chcemy szukaÄ‡ po znaczeniu.

**Temat:** PodobieÅ„stwo Kosinusowe. Mierzenie kÄ…ta miÄ™dzy wektorami zamiast odlegÅ‚oÅ›ci (kluczowe w RAG).

---

## 19_Cross_Validation.ipynb â€“ Walidacja

**Dlaczego?** Jeden podziaÅ‚ na Train/Test to hazard. MoÅ¼e miaÅ‚eÅ› szczÄ™Å›cie przy losowaniu?

**Temat:** K-Fold Cross Validation. Trenowanie modelu 5 razy na rÃ³Å¼nych kawaÅ‚kach danych, aby mieÄ‡ pewnoÅ›Ä‡ co do wyniku.

---

## 20_RAG_Architecture_Simulation.ipynb â€“ RAG

**Dlaczego?** ChatGPT nie zna Twoich prywatnych dokumentÃ³w i halucynuje.

**Temat:** Retrieval Augmented Generation. Wyszukiwanie fragmentÃ³w wiedzy w bazie i doklejanie ich do promptu.

---

## 21_MLP_Neural_Network_XOR.ipynb â€“ Sieci Wielowarstwowe

**Dlaczego?** Jak naprawiÄ‡ problem XOR, ktÃ³rego nie umiaÅ‚ rozwiÄ…zaÄ‡ Perceptron?

**Temat:** MLP (Multi-Layer Perceptron). Dodanie warstw ukrytych, ktÃ³re "wyginajÄ… przestrzeÅ„".

---

## 22_Activation_Functions.ipynb â€“ Funkcje Aktywacji

**Dlaczego?** Bez aktywacji sieÄ‡ neuronowa to tylko mnoÅ¼enie macierzy (funkcja liniowa).

**Temat:** PrzeglÄ…d funkcji: ReLU (standard), Sigmoid (prawdopodobieÅ„stwo), Softmax (klasyfikacja).

---

## 23_Tokenization_GPT.ipynb â€“ Tokenizacja

**Dlaczego?** Dlaczego modele AI nie umiejÄ… liczyÄ‡ liter w sÅ‚owach?

**Temat:** BPE (Byte Pair Encoding). Jak tekst jest szatkowany na tokeny przed wejÅ›ciem do modelu (biblioteka Tiktoken).

---

## 24_Self_Attention_Mechanism.ipynb â€“ Mechanizm Uwagi

**Dlaczego?** Jak model rozumie kontekst caÅ‚ego zdania naraz?

**Temat:** Matematyka Attention od zera. Macierze Query, Key, Value i iloczyn skalarny uwzglÄ™dniajÄ…cy waÅ¼noÅ›Ä‡ sÅ‚Ã³w.

---

## 25_Model_Persistence_Pickle_Joblib.ipynb â€“ Zapisywanie Modeli

**Dlaczego?** Nie moÅ¼esz trenowaÄ‡ modelu od nowa za kaÅ¼dym razem, gdy klient wchodzi na stronÄ™.

**Temat:** Serializacja. Zapisywanie wytrenowanego obiektu do pliku (.pkl, .joblib) i wczytywanie go na produkcji.

---

## 26_RAG_Chunking_Strategies.ipynb â€“ Przygotowanie RAG

**Dlaczego?** Nie moÅ¼esz wrzuciÄ‡ caÅ‚ej ksiÄ…Å¼ki do bazy wektorowej w jednym kawaÅ‚ku.

**Temat:** Chunking. Strategie ciÄ™cia tekstu (Fixed Size, Recursive, Overlap), Å¼eby nie gubiÄ‡ wÄ…tku.

---

## 27_Hyperparameter_Tuning_GridSearch.ipynb â€“ Strojenie

**Dlaczego?** Zgadywanie, czy lepsze jest 10 drzew czy 50, to strata czasu.

**Temat:** Grid Search. Metoda "Brute Force" do automatycznego sprawdzania wszystkich kombinacji parametrÃ³w.

---

## 28_Python_Dataclasses_for_ML.ipynb â€“ Czysty Kod (Config)

**Dlaczego?** Trzymanie parametrÃ³w modelu w zwykÅ‚ym sÅ‚owniku prowadzi do literÃ³wek i bÅ‚Ä™dÃ³w, ktÃ³rych nie widaÄ‡ od razu.

**Temat:** dataclasses. Typowanie silne w konfiguracji treningu, aby kod byÅ‚ bezpieczny i podpowiadaÅ‚ skÅ‚adniÄ™.

---

## 29_OOP_Classmethod_Staticmethod.ipynb â€“ Wzorce Projektowe

**Dlaczego?** Jak elegancko stworzyÄ‡ model z pliku konfiguracyjnego, a jak rÄ™cznie?

**Temat:** Metody fabryczne (@classmethod) i narzÄ™dziowe (@staticmethod) w kontekÅ›cie budowania klas ML.

---

## 30_Sklearn_Pipelines.ipynb â€“ Automatyzacja

**Dlaczego?** JeÅ›li robisz czyszczenie danych rÄ™cznie przed modelem, na produkcji zapomnisz o jednym kroku i system padnie.

**Temat:** Budowa rurociÄ…gu (Pipeline), ktÃ³ry skleja Imputer, Scaler i Model w jeden obiekt. Ochrona przed wyciekiem danych.

---

## 31_Custom_Transformers.ipynb â€“ WÅ‚asne Klasy

**Dlaczego?** Gotowe biblioteki nie majÄ… funkcji "WyczyÅ›Ä‡ symbol waluty i usuÅ„ nawiasy".

**Temat:** Pisanie wÅ‚asnych klas dziedziczÄ…cych po BaseEstimator, ktÃ³re moÅ¼na wpiÄ…Ä‡ w Pipeline Scikit-Learn.

---

## 32_PyTorch_Tensors_Autograd.ipynb â€“ Silnik PyTorch

**Dlaczego?** Nie da siÄ™ rÄ™cznie liczyÄ‡ pochodnych dla miliona wag w sieci neuronowej.

**Temat:** Tensors (macierze na GPU) i Autograd â€“ mechanizm, ktÃ³ry automatycznie Å›ledzi obliczenia i liczy gradienty wstecz.

---

## 33_PyTorch_Neural_Network_Class.ipynb â€“ Architektura

**Dlaczego?** Gotowe funkcje typu model.fit() to czarna skrzynka. Aby budowaÄ‡ nowe rzeczy, musisz mieÄ‡ kontrolÄ™.

**Temat:** Budowa klasy nn.Module i rÄ™czne pisanie pÄ™tli treningowej (Forward -> Loss -> Backward -> Step).

---

## 34_Regularization_Lasso_Ridge.ipynb â€“ Regularyzacja

**Dlaczego?** Model, ktÃ³ry uczy siÄ™ za mocno (wielkie wagi), nie radzi sobie z nowymi danymi.

**Temat:** Lasso (L1) i Ridge (L2). Matematyczne "kary" nakÅ‚adane na model, ktÃ³re zmuszajÄ… go do upraszczania rzeczywistoÅ›ci (i zerowania zbÄ™dnych cech w Lasso).

---

## 35_ANOVA_Hypothesis_Testing.ipynb â€“ Testy A/B/C

**Dlaczego?** PorÃ³wnywanie 3 grup (Lek A, Lek B, Placebo) za pomocÄ… zwykÅ‚ego testu parami to bÅ‚Ä…d statystyczny.

**Temat:** Analiza Wariancji (ANOVA), Test F oraz Test Tukeya (Post-hoc) do bezpiecznego porÃ³wnywania wielu grup.

---

## 36_Market_Basket_Apriori.ipynb â€“ ReguÅ‚y Asocjacyjne

**Dlaczego?** Chcesz wiedzieÄ‡: "Kto kupiÅ‚ piwo, kupiÅ‚ teÅ¼ chipsy". To nie jest predykcja, to szukanie wzorcÃ³w.

**Temat:** Algorytm Apriori. Zrozumienie metryk Support, Confidence i najwaÅ¼niejszego: Lift (siÅ‚a reguÅ‚y).

---

## 37_Gradient_Boosting_XGBoost.ipynb â€“ Boosting

**Dlaczego?** Na danych tabelarycznych (Excel) sieci neuronowe czÄ™sto przegrywajÄ…. KrÃ³lem jest Boosting.

**Temat:** XGBoost. Algorytm, w ktÃ³rym kaÅ¼de kolejne drzewo naprawia bÅ‚Ä™dy poprzednika (sekwencyjne uczenie).

---

## 38_CNN_Computer_Vision.ipynb â€“ Widzenie Komputerowe

**Dlaczego?** ZwykÅ‚a sieÄ‡ niszczy strukturÄ™ zdjÄ™cia (spÅ‚aszcza je). Musimy widzieÄ‡ ksztaÅ‚ty i krawÄ™dzie.

**Temat:** Sieci Splotowe (CNN). Warstwy Conv2d (filtry) i MaxPool (zmniejszanie).

---

## 39_RNN_LSTM_Sequence_Models.ipynb â€“ Szeregi Czasowe

**Dlaczego?** ZwykÅ‚a sieÄ‡ nie pamiÄ™ta, co byÅ‚o na poprzednim zdjÄ™ciu/kroku.

**Temat:** LSTM (Long Short-Term Memory). SieÄ‡ z "pamiÄ™ciÄ…", idealna do przewidywania gieÅ‚dy, pogody czy tekstu.

---

## 40_Autoencoder_Anomaly_Detection.ipynb â€“ Detekcja Anomalii

**Dlaczego?** Jak wykryÄ‡ awariÄ™ silnika, skoro masz dane tylko z poprawnej pracy?

**Temat:** Autoenkoder. Uczenie nienadzorowane â€“ sieÄ‡ uczy siÄ™ kompresowaÄ‡ "normÄ™". JeÅ›li nie potrafi czegoÅ› skompresowaÄ‡ (duÅ¼y bÅ‚Ä…d), to znaczy, Å¼e to anomalia.

---

## 41_GAN_Generative_Adversarial_Network.ipynb â€“ Generowanie Obrazu

**Dlaczego?** Jak zmusiÄ‡ sieÄ‡ do tworzenia nowych rzeczy?

**Temat:** GAN. Wojna dwÃ³ch sieci: FaÅ‚szerza (Generator) i Policjanta (Dyskryminator).

---

## 42_Statistics_Masterclass.ipynb â€“ Kompendium Statystyki

**Dlaczego?** Musisz wiedzieÄ‡, czy TwÃ³j wynik to "odkrycie", czy przypadek. Åšrednia arytmetyczna czÄ™sto kÅ‚amie przy zarobkach (Bill Gates w barze).

**Temat:** RozkÅ‚ady (Normalny), Prawo Wielkich Liczb, Test Shapiro-Wilka, PuÅ‚apki P-value i Paradoks Simpsona.

---

## 43_VAE_Variational_Autoencoder.ipynb â€“ Latent Space

**Dlaczego?** ZwykÅ‚a kompresja jest "sztywna". Nie da siÄ™ pÅ‚ynnie zmieniÄ‡ cyfry 1 w 7.

**Temat:** VAE i Reparameterization Trick. Uczenie siÄ™ rozkÅ‚adu prawdopodobieÅ„stwa danych, co pozwala na "morfing".

---

## 44_RL_Q_Learning_FrozenLake.ipynb â€“ Tabular RL

**Dlaczego?** Jak nauczyÄ‡ robota chodziÄ‡, nie pokazujÄ…c mu przykÅ‚adÃ³w, tylko dajÄ…c kary i nagrody?

**Temat:** Q-Learning. Tworzenie "Å›ciÄ…gi" (Tabeli Q), ktÃ³ra mÃ³wi, jaki ruch jest najlepszy w danej sytuacji.

---

## 45_RL_Deep_Q_Learning_DQN.ipynb â€“ Deep RL

**Dlaczego?** W grze takiej jak StarCraft jest za duÅ¼o stanÃ³w, Å¼eby zapisaÄ‡ je w tabeli.

**Temat:** DQN. ZastÄ…pienie tabeli sieciÄ… neuronowÄ…, ktÃ³ra "zgaduje" najlepszy ruch. Replay Buffer i Target Network dla stabilnoÅ›ci.

---

## 46_Transformer_Block_From_Scratch.ipynb â€“ Architektura GPT

**Dlaczego?** Attention to za maÅ‚o. Prawdziwy Transformer to kanapka warstw.

**Temat:** Implementacja peÅ‚nego bloku: Attention -> LayerNorm -> FeedForward -> Residual Connection (Add).

---

## 47_SVM_Kernel_Trick.ipynb â€“ SVM i NieliniowoÅ›Ä‡

**Dlaczego?** Jak rozdzieliÄ‡ dane (czerwone w Å›rodku, niebieskie na zewnÄ…trz), ktÃ³rych nie da siÄ™ przeciÄ…Ä‡ prostÄ… kreskÄ…?

**Temat:** Kernel Trick (RBF). Rzutowanie danych w wyÅ¼szy wymiar, gdzie stajÄ… siÄ™ separowalne liniowo.

---

## 48_tSNE_vs_PCA_Dimensionality_Reduction.ipynb â€“ Wizualizacja Danych

**Dlaczego?** Masz dane 64-wymiarowe. Ekran jest 2D. PCA spÅ‚aszcza dane jak walec (gubiÄ…c strukturÄ™), t-SNE je "rozprostowuje".

**Temat:** Nieliniowa redukcja wymiarÃ³w. PorÃ³wnanie, jak PCA i t-SNE radzÄ… sobie z klastrowaniem cyfr (MNIST).

---

## 49_Object_Detection_IoU.ipynb â€“ Detekcja ObiektÃ³w

**Dlaczego?** SieÄ‡ narysowaÅ‚a ramkÄ™ wokÃ³Å‚ kota. SkÄ…d wiesz, czy trafiÅ‚a dobrze?

**Temat:** Intersection over Union (IoU). Matematyka oceniania, jak bardzo dwie ramki na siebie nachodzÄ….

---

## 50_UNet_Image_Segmentation.ipynb â€“ Segmentacja

**Dlaczego?** Czasem nie wystarczy wiedzieÄ‡ "tu jest rak". Musisz wiedzieÄ‡ dokÅ‚adnie, ktÃ³ry piksel to rak.

**Temat:** Architektura U-Net. PoÅ‚Ä…czenia skrÃ³towe (Skip Connections), ktÃ³re pozwalajÄ… sieci widzieÄ‡ jednoczeÅ›nie kontekst i precyzyjne detale.

---

## 51_Recommender_Systems_SVD.ipynb â€“ Systemy Rekomendacyjne

**Dlaczego?** Masz miliony filmÃ³w i uÅ¼ytkownikÃ³w. Tabela jest pusta w 99%. Jak zgadnÄ…Ä‡ ocenÄ™ filmu, ktÃ³rego nie widziaÅ‚eÅ›?

**Temat:** Faktoryzacja Macierzy (SVD). Rozbicie tabeli na ukryte cechy uÅ¼ytkownikÃ³w i filmÃ³w.

---

## 52_Genetic_Algorithms_Evolution.ipynb â€“ Algorytmy Genetyczne

**Dlaczego?** Gradient nie dziaÅ‚a, gdy problem jest poszarpany lub dyskretny (np. co spakowaÄ‡ do plecaka).

**Temat:** Ewolucja. Symulacja populacji, krzyÅ¼owania i mutacji w celu znalezienia optymalnego rozwiÄ…zania bez uÅ¼ycia pochodnych.

---

## 53_Monte_Carlo_Simulation.ipynb â€“ Symulacje Ryzyka

**Dlaczego?** W finansach nie pytamy "ile zarobiÄ™?", tylko "jaka jest szansa, Å¼e zbankrutujÄ™?". PrzeszÅ‚oÅ›Ä‡ nie gwarantuje przyszÅ‚oÅ›ci.

**Temat:** Generowanie 1000 alternatywnych scenariuszy gieÅ‚dowych (Geometryczne Ruchy Browna) i obliczanie VaR (Value at Risk).

---

## 54_FFT_Signal_Processing.ipynb â€“ Przetwarzanie SygnaÅ‚u

**Dlaczego?** Dane to nie tylko tabelki, to teÅ¼ dÅºwiÄ™k i wibracje. Na wykresie czasowym szumu nie widaÄ‡.

**Temat:** Szybka Transformata Fouriera (FFT). Zamiana osi czasu na oÅ› czÄ™stotliwoÅ›ci, aby "zobaczyÄ‡" i wyciÄ…Ä‡ pisk z nagrania.

---

## 55_LoRA_Fine_Tuning_Math.ipynb â€“ Fine-Tuning

**Dlaczego?** Douczanie modelu GPT waÅ¼Ä…cego 100GB jest niemoÅ¼liwe na laptopie.

**Temat:** LoRA (Low-Rank Adaptation). Matematyczny trik polegajÄ…cy na douczaniu tylko malutkich macierzy-nakÅ‚adek (AdapterÃ³w).

---

## 56_Positional_Encoding_Transformer.ipynb â€“ Czas w Transformerze

**Dlaczego?** Transformer czyta caÅ‚e zdanie naraz (rÃ³wnolegle). Nie wie, co byÅ‚o wczeÅ›niej, a co pÃ³Åºniej.

**Temat:** Positional Encoding. Dodawanie fal sinusoidalnych do wektorÃ³w sÅ‚Ã³w, aby nadaÄ‡ im "sygnaturÄ™ czasu".

---

## 57_Metric_Learning_Siamese.ipynb â€“ FaceID

**Dlaczego?** Telefon nie ma w bazie milionÃ³w twarzy. On sprawdza, czy Twoja twarz jest podobna do tej zapisanej.

**Temat:** Sieci Syjamskie i Triplet Loss. Uczenie sieci mierzenia odlegÅ‚oÅ›ci miÄ™dzy obiektami, a nie ich klasyfikacji.

---

## 58_Graph_Neural_Networks_GNN.ipynb â€“ Grafy

**Dlaczego?** Social media i chemia to nie tabelki. To relacje (kto zna kogo, jaki atom wiÄ…Å¼e siÄ™ z jakim).

**Temat:** Message Passing. Jak wÄ™zÅ‚y w grafie wymieniajÄ… siÄ™ informacjami ze swoimi sÄ…siadami.

---

## 59_Model_Quantization_INT8.ipynb â€“ Kompresja Modeli

**Dlaczego?** Wielkie modele nie mieszczÄ… siÄ™ w pamiÄ™ci telefonu.

**Temat:** Matematyka rzutowania liczb zmiennoprzecinkowych (FP32) na caÅ‚kowite (INT8). Obliczanie Scale i Zero Point.

---

## 60_Bayesian_Optimization_Optuna.ipynb â€“ Optymalizacja

**Dlaczego?** Grid Search sprawdza wszystko "na siÅ‚Ä™" (strata prÄ…du). Random Search to hazard.

**Temat:** Optuna. Algorytm, ktÃ³ry uczy siÄ™ na bÅ‚Ä™dach i inteligentnie dobiera parametry modelu, Å¼eby zmaksymalizowaÄ‡ wynik.

---

## 61_Normalization_Layers_BN_vs_LN.ipynb â€“ Stabilizacja

**Dlaczego?** Bez normalizacji gÅ‚Ä™bokie sieci przestajÄ… siÄ™ uczyÄ‡ (wybuchajÄ…ce gradienty).

**Temat:** RÃ³Å¼nica miÄ™dzy BatchNorm (dla obrazÃ³w), LayerNorm (dla tekstu/TransformerÃ³w) i InstanceNorm.

---

## 62_Time_Series_Decomposition_STL.ipynb â€“ Szeregi Czasowe

**Dlaczego?** Szef pyta: "Dlaczego sprzedaÅ¼ spadÅ‚a?". Musisz wiedzieÄ‡, czy to trend (kryzys), czy sezonowoÅ›Ä‡ (koniec Å›wiÄ…t).

**Temat:** Dekompozycja STL. Rozbicie wykresu na trzy skÅ‚adniki: Trend, SezonowoÅ›Ä‡ i Reszty (Szum/Anomalie).
