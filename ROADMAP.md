# üó∫Ô∏è AI Engineering Roadmap

Pe≈Çna ≈õcie≈ºka edukacyjna repozytorium AI-Engineering-Handbook. Ka≈ºdy notatnik rozwiƒÖzuje konkretny problem in≈ºynierski lub biznesowy.

---

## 00_KDE_Tutorial.ipynb ‚Äì Rozk≈Çady Gƒôsto≈õci

**Dlaczego?** Zwyk≈Çy histogram jest "kanciasty" i zale≈ºy od tego, jak szerokie s≈Çupki ustawisz. Mo≈ºesz przegapiƒá wa≈ºne niuanse danych.

**Temat:** Kernel Density Estimation (KDE). Jak narysowaƒá g≈ÇadkƒÖ krzywƒÖ prawdopodobie≈Ñstwa, kt√≥ra lepiej oddaje rzeczywisto≈õƒá ni≈º s≈Çupki.

---

## 01_Correlations_and_Significance.ipynb ‚Äì Zale≈ºno≈õci

**Dlaczego?** Musisz wiedzieƒá, kt√≥re cechy wp≈ÇywajƒÖ na wynik, zanim zaczniesz trenowaƒá model.

**Temat:** Korelacja Pearsona (liniowa) vs Spearmana (rangowa). Heatmapy i pu≈Çapki interpretacji korelacji.

---

## 02_Advanced_Feature_Selection.ipynb ‚Äì Selekcja Cech

**Dlaczego?** Wiƒôcej danych ‚â† lepiej. ≈ömieciowe kolumny mylƒÖ model. Zwyk≈Ça korelacja nie widzi zale≈ºno≈õci nieliniowych (np. paraboli).

**Temat:** SHAP, Mutual Information i Permutation Importance ‚Äì nowoczesne metody oddzielania sygna≈Çu od szumu.

---

## 03_Encoding_Tutorial.ipynb ‚Äì Zamiana S≈Ç√≥w na Liczby

**Dlaczego?** Model matematyczny nie rozumie s≈Çowa "Czerwony" ani "BMW". Rozumie tylko liczby.

**Temat:** One-Hot Encoding, Label Encoding i Ordinal Encoding. Jak nie wprowadziƒá fa≈Çszywej hierarchii do danych.

---

## 04_Statistics_and_Scaling.ipynb ‚Äì Skalowanie

**Dlaczego?** Algorytmy oparte na odleg≈Ço≈õci (jak KNN) zwariujƒÖ, je≈õli jedna cecha ma zakres 0-1, a druga 0-10000.

**Temat:** Standaryzacja (StandardScaler) vs Normalizacja (MinMax). Kiedy u≈ºywaƒá kt√≥rego?

---

## 05_Top_p_Top_k.ipynb ‚Äì Generowanie Tekstu

**Dlaczego?** Dlaczego ChatGPT czasem jest kreatywny, a czasem precyzyjny?

**Temat:** Sampling. Sterowanie losowo≈õciƒÖ wyboru kolejnego s≈Çowa (Top-k vs Nucleus Sampling).

---

## 06_Naive_Bayes_Spam.ipynb ‚Äì Klasyfikacja Tekstu

**Dlaczego?** Czasami potrzebujesz super szybkiego i prostego modelu, kt√≥ry dzia≈Ça na ma≈Çej ilo≈õci danych (np. prosty anty-spam).

**Temat:** Naiwny Klasyfikator Bayesa. Wykorzystanie prawdopodobie≈Ñstwa warunkowego do oceny tekstu (Bag of Words).

---

## 07_Confusion_Matrix_Precision_Recall.ipynb ‚Äì Metryki B≈Çƒôd√≥w

**Dlaczego?** Dok≈Çadno≈õƒá (Accuracy) 99% to k≈Çamstwo, je≈õli w danych masz tylko 1% oszustw (model zgaduje "brak oszustwa" i ma racjƒô, ale jest bezu≈ºyteczny).

**Temat:** Macierz Pomy≈Çek. Precision (Precyzja), Recall (Czu≈Ço≈õƒá) i F1-Score.

---

## 08_Overfitting_Underfitting.ipynb ‚Äì Diagnostyka

**Dlaczego?** Tw√≥j model dzia≈Ça ≈õwietnie na treningu, a tragicznie na produkcji.

**Temat:** Bias-Variance Tradeoff. Wizualizacja, jak zbyt skomplikowany model "wkuwa na pamiƒôƒá" szum zamiast uczyƒá siƒô regu≈Ç.

---

## 09_K_Means_Clustering.ipynb ‚Äì Grupowanie

**Dlaczego?** Masz bazƒô klient√≥w, ale nie masz etykiet (nie wiesz, kto jest kim). Chcesz ich podzieliƒá na segmenty.

**Temat:** K-Means. Algorytm nienadzorowany i Metoda ≈Åokcia (Elbow Method) do wyznaczania optymalnej liczby grup.

---

## 10_Decision_Trees.ipynb ‚Äì Drzewa Decyzyjne

**Dlaczego?** Czasami musisz wiedzieƒá dok≈Çadnie, DLACZEGO model podjƒÖ≈Ç decyzjƒô (wym√≥g prawny/biznesowy).

**Temat:** Drzewa Decyzyjne. Algorytm "White Box", kt√≥ry tworzy czytelne regu≈Çy (if-else).

---

## 11_Embeddings_Vector_Space.ipynb ‚Äì Matematyka S≈Ç√≥w

**Dlaczego?** Jak sprawiƒá, ≈ºeby komputer rozumia≈Ç, ≈ºe "Kr√≥l" i "Kr√≥lowa" sƒÖ blisko siebie znaczeniowo?

**Temat:** Embeddings. Reprezentacja s≈Ç√≥w jako wektor√≥w w przestrzeni wielowymiarowej.

---

## 12_LLM_Temperature.ipynb ‚Äì Parametry Modelu

**Dlaczego?** Jak dzia≈Ça suwak "Temperature" w API OpenAI?

**Temat:** Softmax z temperaturƒÖ. Jak matematycznie sp≈Çaszczyƒá lub wyostrzyƒá rozk≈Çad prawdopodobie≈Ñstwa.

---

## 13_Missing_Data_Imputation.ipynb ‚Äì Braki Danych

**Dlaczego?** Dane w ≈õwiecie rzeczywistym sƒÖ dziurawe. Usuniƒôcie wierszy z brakami (NaN) to utrata cennych informacji.

**Temat:** Strategie Imputacji. Wype≈Çnianie ≈õredniƒÖ, medianƒÖ oraz (najlepsze) inteligentne wype≈Çnianie grupowe (Pandas transform).

---

## 14_Random_Forest_Ensemble.ipynb ‚Äì Bagging

**Dlaczego?** Pojedyncze drzewo decyzyjne jest niestabilne i ≈Çatwo wkuwa dane na pamiƒôƒá.

**Temat:** Las Losowy. Metoda "Si≈Çy T≈Çumu" ‚Äì 100 drzew g≈Çosuje nad wynikiem, co wyg≈Çadza b≈Çƒôdy i daje stabilno≈õƒá.

---

## 15_Gradient_Descent.ipynb ‚Äì Silnik Uczenia

**Dlaczego?** Jak w≈Ça≈õciwie model "wie", w kt√≥rƒÖ stronƒô zmieniƒá wagi, ≈ºeby zmniejszyƒá b≈ÇƒÖd?

**Temat:** Symulacja "schodzenia z g√≥ry". Zrozumienie Learning Rate i mechanizmu optymalizacji.

---

## 16_Neural_Network_Perceptron.ipynb ‚Äì Pierwszy Neuron

**Dlaczego?** ≈ªeby zrozumieƒá sieƒá, musisz zbudowaƒá jej najmniejszƒÖ cegie≈Çkƒô.

**Temat:** Perceptron. Implementacja od zera i dow√≥d, dlaczego pojedynczy neuron nie rozwiƒÖ≈ºe problemu XOR.

---

## 17_PCA_Dimensionality_Reduction.ipynb ‚Äì Redukcja Wymiar√≥w

**Dlaczego?** Masz 100 kolumn, a chcesz narysowaƒá wykres 2D.

**Temat:** PCA (Principal Component Analysis). Matematyczne "rzutowanie cienia" danych wielowymiarowych na p≈Çaszczyznƒô.

---

## 18_Cosine_Similarity_Search.ipynb ‚Äì Wyszukiwanie

**Dlaczego?** Wyszukiwanie po s≈Çowach kluczowych jest s≈Çabe. Chcemy szukaƒá po znaczeniu.

**Temat:** Podobie≈Ñstwo Kosinusowe. Mierzenie kƒÖta miƒôdzy wektorami zamiast odleg≈Ço≈õci (kluczowe w RAG).

---

## 19_Cross_Validation.ipynb ‚Äì Walidacja

**Dlaczego?** Jeden podzia≈Ç na Train/Test to hazard. Mo≈ºe mia≈Çe≈õ szczƒô≈õcie przy losowaniu?

**Temat:** K-Fold Cross Validation. Trenowanie modelu 5 razy na r√≥≈ºnych kawa≈Çkach danych, aby mieƒá pewno≈õƒá co do wyniku.

---

## 20_RAG_Architecture_Simulation.ipynb ‚Äì RAG

**Dlaczego?** ChatGPT nie zna Twoich prywatnych dokument√≥w i halucynuje.

**Temat:** Retrieval Augmented Generation. Wyszukiwanie fragment√≥w wiedzy w bazie i doklejanie ich do promptu.

---

## 21_MLP_Neural_Network_XOR.ipynb ‚Äì Sieci Wielowarstwowe

**Dlaczego?** Jak naprawiƒá problem XOR, kt√≥rego nie umia≈Ç rozwiƒÖzaƒá Perceptron?

**Temat:** MLP (Multi-Layer Perceptron). Dodanie warstw ukrytych, kt√≥re "wyginajƒÖ przestrze≈Ñ".

---

## 22_Activation_Functions.ipynb ‚Äì Funkcje Aktywacji

**Dlaczego?** Bez aktywacji sieƒá neuronowa to tylko mno≈ºenie macierzy (funkcja liniowa).

**Temat:** PrzeglƒÖd funkcji: ReLU (standard), Sigmoid (prawdopodobie≈Ñstwo), Softmax (klasyfikacja).

---

## 23_Tokenization_GPT.ipynb ‚Äì Tokenizacja

**Dlaczego?** Dlaczego modele AI nie umiejƒÖ liczyƒá liter w s≈Çowach?

**Temat:** BPE (Byte Pair Encoding). Jak tekst jest szatkowany na tokeny przed wej≈õciem do modelu (biblioteka Tiktoken).

---

## 24_Self_Attention_Mechanism.ipynb ‚Äì Mechanizm Uwagi

**Dlaczego?** Jak model rozumie kontekst ca≈Çego zdania naraz?

**Temat:** Matematyka Attention od zera. Macierze Query, Key, Value i iloczyn skalarny uwzglƒôdniajƒÖcy wa≈ºno≈õƒá s≈Ç√≥w.

---

## 25_Model_Persistence_Pickle_Joblib.ipynb ‚Äì Zapisywanie Modeli

**Dlaczego?** Nie mo≈ºesz trenowaƒá modelu od nowa za ka≈ºdym razem, gdy klient wchodzi na stronƒô.

**Temat:** Serializacja. Zapisywanie wytrenowanego obiektu do pliku (.pkl, .joblib) i wczytywanie go na produkcji.

---

## 26_RAG_Chunking_Strategies.ipynb ‚Äì Przygotowanie RAG

**Dlaczego?** Nie mo≈ºesz wrzuciƒá ca≈Çej ksiƒÖ≈ºki do bazy wektorowej w jednym kawa≈Çku.

**Temat:** Chunking. Strategie ciƒôcia tekstu (Fixed Size, Recursive, Overlap), ≈ºeby nie gubiƒá wƒÖtku.

---

## 27_Hyperparameter_Tuning_GridSearch.ipynb ‚Äì Strojenie

**Dlaczego?** Zgadywanie, czy lepsze jest 10 drzew czy 50, to strata czasu.

**Temat:** Grid Search. Metoda "Brute Force" do automatycznego sprawdzania wszystkich kombinacji parametr√≥w.

---

## 28_Python_Dataclasses_for_ML.ipynb ‚Äì Czysty Kod (Config)

**Dlaczego?** Trzymanie parametr√≥w modelu w zwyk≈Çym s≈Çowniku prowadzi do liter√≥wek i b≈Çƒôd√≥w, kt√≥rych nie widaƒá od razu.

**Temat:** dataclasses. Typowanie silne w konfiguracji treningu, aby kod by≈Ç bezpieczny i podpowiada≈Ç sk≈Çadniƒô.

---

## 29_OOP_Classmethod_Staticmethod.ipynb ‚Äì Wzorce Projektowe

**Dlaczego?** Jak elegancko stworzyƒá model z pliku konfiguracyjnego, a jak rƒôcznie?

**Temat:** Metody fabryczne (@classmethod) i narzƒôdziowe (@staticmethod) w kontek≈õcie budowania klas ML.

---

## 30_Sklearn_Pipelines.ipynb ‚Äì Automatyzacja

**Dlaczego?** Je≈õli robisz czyszczenie danych rƒôcznie przed modelem, na produkcji zapomnisz o jednym kroku i system padnie.

**Temat:** Budowa rurociƒÖgu (Pipeline), kt√≥ry skleja Imputer, Scaler i Model w jeden obiekt. Ochrona przed wyciekiem danych.

---

## 31_Custom_Transformers.ipynb ‚Äì W≈Çasne Klasy

**Dlaczego?** Gotowe biblioteki nie majƒÖ funkcji "Wyczy≈õƒá symbol waluty i usu≈Ñ nawiasy".

**Temat:** Pisanie w≈Çasnych klas dziedziczƒÖcych po BaseEstimator, kt√≥re mo≈ºna wpiƒÖƒá w Pipeline Scikit-Learn.

---

## 32_PyTorch_Tensors_Autograd.ipynb ‚Äì Silnik PyTorch

**Dlaczego?** Nie da siƒô rƒôcznie liczyƒá pochodnych dla miliona wag w sieci neuronowej.

**Temat:** Tensors (macierze na GPU) i Autograd ‚Äì mechanizm, kt√≥ry automatycznie ≈õledzi obliczenia i liczy gradienty wstecz.

---

## 33_PyTorch_Neural_Network_Class.ipynb ‚Äì Architektura

**Dlaczego?** Gotowe funkcje typu model.fit() to czarna skrzynka. Aby budowaƒá nowe rzeczy, musisz mieƒá kontrolƒô.

**Temat:** Budowa klasy nn.Module i rƒôczne pisanie pƒôtli treningowej (Forward -> Loss -> Backward -> Step).

---

## 34_Regularization_Lasso_Ridge.ipynb ‚Äì Regularyzacja

**Dlaczego?** Model, kt√≥ry uczy siƒô za mocno (wielkie wagi), nie radzi sobie z nowymi danymi.

**Temat:** Lasso (L1) i Ridge (L2). Matematyczne "kary" nak≈Çadane na model, kt√≥re zmuszajƒÖ go do upraszczania rzeczywisto≈õci (i zerowania zbƒôdnych cech w Lasso).

---

## 35_ANOVA_Hypothesis_Testing.ipynb ‚Äì Testy A/B/C

**Dlaczego?** Por√≥wnywanie 3 grup (Lek A, Lek B, Placebo) za pomocƒÖ zwyk≈Çego testu parami to b≈ÇƒÖd statystyczny.

**Temat:** Analiza Wariancji (ANOVA), Test F oraz Test Tukeya (Post-hoc) do bezpiecznego por√≥wnywania wielu grup.

---

## 36_Market_Basket_Apriori.ipynb ‚Äì Regu≈Çy Asocjacyjne

**Dlaczego?** Chcesz wiedzieƒá: "Kto kupi≈Ç piwo, kupi≈Ç te≈º chipsy". To nie jest predykcja, to szukanie wzorc√≥w.

**Temat:** Algorytm Apriori. Zrozumienie metryk Support, Confidence i najwa≈ºniejszego: Lift (si≈Ça regu≈Çy).

---

## 37_Gradient_Boosting_XGBoost.ipynb ‚Äì Boosting

**Dlaczego?** Na danych tabelarycznych (Excel) sieci neuronowe czƒôsto przegrywajƒÖ. Kr√≥lem jest Boosting.

**Temat:** XGBoost. Algorytm, w kt√≥rym ka≈ºde kolejne drzewo naprawia b≈Çƒôdy poprzednika (sekwencyjne uczenie).

---

## 38_CNN_Computer_Vision.ipynb ‚Äì Widzenie Komputerowe

**Dlaczego?** Zwyk≈Ça sieƒá niszczy strukturƒô zdjƒôcia (sp≈Çaszcza je). Musimy widzieƒá kszta≈Çty i krawƒôdzie.

**Temat:** Sieci Splotowe (CNN). Warstwy Conv2d (filtry) i MaxPool (zmniejszanie).

---

## 39_RNN_LSTM_Sequence_Models.ipynb ‚Äì Szeregi Czasowe

**Dlaczego?** Zwyk≈Ça sieƒá nie pamiƒôta, co by≈Ço na poprzednim zdjƒôciu/kroku.

**Temat:** LSTM (Long Short-Term Memory). Sieƒá z "pamiƒôciƒÖ", idealna do przewidywania gie≈Çdy, pogody czy tekstu.

---

## 40_Autoencoder_Anomaly_Detection.ipynb ‚Äì Detekcja Anomalii

**Dlaczego?** Jak wykryƒá awariƒô silnika, skoro masz dane tylko z poprawnej pracy?

**Temat:** Autoenkoder. Uczenie nienadzorowane ‚Äì sieƒá uczy siƒô kompresowaƒá "normƒô". Je≈õli nie potrafi czego≈õ skompresowaƒá (du≈ºy b≈ÇƒÖd), to znaczy, ≈ºe to anomalia.

---

## 41_GAN_Generative_Adversarial_Network.ipynb ‚Äì Generowanie Obrazu

**Dlaczego?** Jak zmusiƒá sieƒá do tworzenia nowych rzeczy?

**Temat:** GAN. Wojna dw√≥ch sieci: Fa≈Çszerza (Generator) i Policjanta (Dyskryminator).

---

## 42_Statistics_Masterclass.ipynb ‚Äì Kompendium Statystyki

**Dlaczego?** Musisz wiedzieƒá, czy Tw√≥j wynik to "odkrycie", czy przypadek. ≈örednia arytmetyczna czƒôsto k≈Çamie przy zarobkach (Bill Gates w barze).

**Temat:** Rozk≈Çady (Normalny), Prawo Wielkich Liczb, Test Shapiro-Wilka, Pu≈Çapki P-value i Paradoks Simpsona.

---

## 43_VAE_Variational_Autoencoder.ipynb ‚Äì Latent Space

**Dlaczego?** Zwyk≈Ça kompresja jest "sztywna". Nie da siƒô p≈Çynnie zmieniƒá cyfry 1 w 7.

**Temat:** VAE i Reparameterization Trick. Uczenie siƒô rozk≈Çadu prawdopodobie≈Ñstwa danych, co pozwala na "morfing".

---

## 44_RL_Q_Learning_FrozenLake.ipynb ‚Äì Tabular RL

**Dlaczego?** Jak nauczyƒá robota chodziƒá, nie pokazujƒÖc mu przyk≈Çad√≥w, tylko dajƒÖc kary i nagrody?

**Temat:** Q-Learning. Tworzenie "≈õciƒÖgi" (Tabeli Q), kt√≥ra m√≥wi, jaki ruch jest najlepszy w danej sytuacji.

---

## 45_RL_Deep_Q_Learning_DQN.ipynb ‚Äì Deep RL

**Dlaczego?** W grze takiej jak StarCraft jest za du≈ºo stan√≥w, ≈ºeby zapisaƒá je w tabeli.

**Temat:** DQN. ZastƒÖpienie tabeli sieciƒÖ neuronowƒÖ, kt√≥ra "zgaduje" najlepszy ruch. Replay Buffer i Target Network dla stabilno≈õci.

---

## 46_Transformer_Block_From_Scratch.ipynb ‚Äì Architektura GPT

**Dlaczego?** Attention to za ma≈Ço. Prawdziwy Transformer to kanapka warstw.

**Temat:** Implementacja pe≈Çnego bloku: Attention -> LayerNorm -> FeedForward -> Residual Connection (Add).

---

## 47_SVM_Kernel_Trick.ipynb ‚Äì SVM i Nieliniowo≈õƒá

**Dlaczego?** Jak rozdzieliƒá dane (czerwone w ≈õrodku, niebieskie na zewnƒÖtrz), kt√≥rych nie da siƒô przeciƒÖƒá prostƒÖ kreskƒÖ?

**Temat:** Kernel Trick (RBF). Rzutowanie danych w wy≈ºszy wymiar, gdzie stajƒÖ siƒô separowalne liniowo.

---

## 48_tSNE_vs_PCA_Dimensionality_Reduction.ipynb ‚Äì Wizualizacja Danych

**Dlaczego?** Masz dane 64-wymiarowe. Ekran jest 2D. PCA sp≈Çaszcza dane jak walec (gubiƒÖc strukturƒô), t-SNE je "rozprostowuje".

**Temat:** Nieliniowa redukcja wymiar√≥w. Por√≥wnanie, jak PCA i t-SNE radzƒÖ sobie z klastrowaniem cyfr (MNIST).

---

## 49_Object_Detection_IoU.ipynb ‚Äì Detekcja Obiekt√≥w

**Dlaczego?** Sieƒá narysowa≈Ça ramkƒô wok√≥≈Ç kota. SkƒÖd wiesz, czy trafi≈Ça dobrze?

**Temat:** Intersection over Union (IoU). Matematyka oceniania, jak bardzo dwie ramki na siebie nachodzƒÖ.

---

## 50_UNet_Image_Segmentation.ipynb ‚Äì Segmentacja

**Dlaczego?** Czasem nie wystarczy wiedzieƒá "tu jest rak". Musisz wiedzieƒá dok≈Çadnie, kt√≥ry piksel to rak.

**Temat:** Architektura U-Net. Po≈ÇƒÖczenia skr√≥towe (Skip Connections), kt√≥re pozwalajƒÖ sieci widzieƒá jednocze≈õnie kontekst i precyzyjne detale.

---

## 51_Recommender_Systems_SVD.ipynb ‚Äì Systemy Rekomendacyjne

**Dlaczego?** Masz miliony film√≥w i u≈ºytkownik√≥w. Tabela jest pusta w 99%. Jak zgadnƒÖƒá ocenƒô filmu, kt√≥rego nie widzia≈Çe≈õ?

**Temat:** Faktoryzacja Macierzy (SVD). Rozbicie tabeli na ukryte cechy u≈ºytkownik√≥w i film√≥w.

---

## 52_Genetic_Algorithms_Evolution.ipynb ‚Äì Algorytmy Genetyczne

**Dlaczego?** Gradient nie dzia≈Ça, gdy problem jest poszarpany lub dyskretny (np. co spakowaƒá do plecaka).

**Temat:** Ewolucja. Symulacja populacji, krzy≈ºowania i mutacji w celu znalezienia optymalnego rozwiƒÖzania bez u≈ºycia pochodnych.

---

## 53_Monte_Carlo_Simulation.ipynb ‚Äì Symulacje Ryzyka

**Dlaczego?** W finansach nie pytamy "ile zarobiƒô?", tylko "jaka jest szansa, ≈ºe zbankrutujƒô?". Przesz≈Ço≈õƒá nie gwarantuje przysz≈Ço≈õci.

**Temat:** Generowanie 1000 alternatywnych scenariuszy gie≈Çdowych (Geometryczne Ruchy Browna) i obliczanie VaR (Value at Risk).

---

## 54_FFT_Signal_Processing.ipynb ‚Äì Przetwarzanie Sygna≈Çu

**Dlaczego?** Dane to nie tylko tabelki, to te≈º d≈∫wiƒôk i wibracje. Na wykresie czasowym szumu nie widaƒá.

**Temat:** Szybka Transformata Fouriera (FFT). Zamiana osi czasu na o≈õ czƒôstotliwo≈õci, aby "zobaczyƒá" i wyciƒÖƒá pisk z nagrania.

---

## 55_LoRA_Fine_Tuning_Math.ipynb ‚Äì Fine-Tuning

**Dlaczego?** Douczanie modelu GPT wa≈ºƒÖcego 100GB jest niemo≈ºliwe na laptopie.

**Temat:** LoRA (Low-Rank Adaptation). Matematyczny trik polegajƒÖcy na douczaniu tylko malutkich macierzy-nak≈Çadek (Adapter√≥w).

---

## 56_Positional_Encoding_Transformer.ipynb ‚Äì Czas w Transformerze

**Dlaczego?** Transformer czyta ca≈Çe zdanie naraz (r√≥wnolegle). Nie wie, co by≈Ço wcze≈õniej, a co p√≥≈∫niej.

**Temat:** Positional Encoding. Dodawanie fal sinusoidalnych do wektor√≥w s≈Ç√≥w, aby nadaƒá im "sygnaturƒô czasu".

---

## 57_Metric_Learning_Siamese.ipynb ‚Äì FaceID

**Dlaczego?** Telefon nie ma w bazie milion√≥w twarzy. On sprawdza, czy Twoja twarz jest podobna do tej zapisanej.

**Temat:** Sieci Syjamskie i Triplet Loss. Uczenie sieci mierzenia odleg≈Ço≈õci miƒôdzy obiektami, a nie ich klasyfikacji.

---

## 58_Graph_Neural_Networks_GNN.ipynb ‚Äì Grafy

**Dlaczego?** Social media i chemia to nie tabelki. To relacje (kto zna kogo, jaki atom wiƒÖ≈ºe siƒô z jakim).

**Temat:** Message Passing. Jak wƒôz≈Çy w grafie wymieniajƒÖ siƒô informacjami ze swoimi sƒÖsiadami.

---

## 59_Model_Quantization_INT8.ipynb ‚Äì Kompresja Modeli

**Dlaczego?** Wielkie modele nie mieszczƒÖ siƒô w pamiƒôci telefonu.

**Temat:** Matematyka rzutowania liczb zmiennoprzecinkowych (FP32) na ca≈Çkowite (INT8). Obliczanie Scale i Zero Point.

---

## 60_Bayesian_Optimization_Optuna.ipynb ‚Äì Optymalizacja

**Dlaczego?** Grid Search sprawdza wszystko "na si≈Çƒô" (strata prƒÖdu). Random Search to hazard.

**Temat:** Optuna. Algorytm, kt√≥ry uczy siƒô na b≈Çƒôdach i inteligentnie dobiera parametry modelu, ≈ºeby zmaksymalizowaƒá wynik.

---

## 61_Normalization_Layers_BN_vs_LN.ipynb ‚Äì Stabilizacja

**Dlaczego?** Bez normalizacji g≈Çƒôbokie sieci przestajƒÖ siƒô uczyƒá (wybuchajƒÖce gradienty).

**Temat:** R√≥≈ºnica miƒôdzy BatchNorm (dla obraz√≥w), LayerNorm (dla tekstu/Transformer√≥w) i InstanceNorm.

---

## 62_Time_Series_Decomposition_STL.ipynb ‚Äì Szeregi Czasowe

**Dlaczego?** Szef pyta: "Dlaczego sprzeda≈º spad≈Ça?". Musisz wiedzieƒá, czy to trend (kryzys), czy sezonowo≈õƒá (koniec ≈õwiƒÖt).

**Temat:** Dekompozycja STL. Rozbicie wykresu na trzy sk≈Çadniki: Trend, Sezonowo≈õƒá i Reszty (Szum/Anomalie).

---

## 63_Diffusion_Models_DDPM.ipynb ‚Äì Matematyka Stable Diffusion

**Dlaczego?** GANy (notatnik 41) sƒÖ trudne w treningu. Dzisiaj kr√≥luje Dyfuzja.

**Temat:** Zaimplementujemy od zera proces Forward Diffusion (dodawanie szumu do zdjƒôcia a≈º zniknie) i Reverse Diffusion (nauczenie sieci, jak "odszumiaƒá" bia≈Çy szum, ≈ºeby powsta≈Ç obraz).

---

## 64_Knowledge_Distillation.ipynb ‚Äì Nauczyciel i Ucze≈Ñ

**Dlaczego?** Kwantyzacja (notatnik 59) to nie jedyny spos√≥b na ma≈Çe modele. Mo≈ºemy wziƒÖƒá wielkiego modelu (Teacher) i kazaƒá mu uczyƒá ma≈Çego modelu (Student).

**Temat:** Soft Labels i Temperature. Student nie uczy siƒô tylko "To jest kot", ale uczy siƒô od Nauczyciela, ≈ºe "To jest kot, ale trochƒô przypomina psa".

---

## 65_HNSW_Vector_Database.ipynb ‚Äì Silnik Baz Wektorowych

**Dlaczego?** W RAG (notatnik 20) liczyli≈õmy cosinusy dla wszystkich par. To dzia≈Ça dla 10k plik√≥w. Dla 100 milion√≥w potrzebujesz HNSW.

**Temat:** Zbudujemy (uproszczonƒÖ) strukturƒô grafowƒÖ, kt√≥ra pozwala znale≈∫ƒá najbli≈ºszy wektor w czasie logarytmicznym, skaczƒÖc po "autostradach" miƒôdzy wƒôz≈Çami.

---

## 66_Kalman_Filter.ipynb ‚Äì ≈öledzenie w czasie rzeczywistym

**Dlaczego?** GPS w Twoim telefonie k≈Çamie (ma b≈ÇƒÖd 5m). Jakim cudem nawigacja pokazuje Ciƒô idealnie na drodze?

**Temat:** Algorytm, kt√≥ry ≈ÇƒÖczy Predykcjƒô (gdzie powiniene≈õ byƒá wg fizyki) z Pomiarem (gdzie jeste≈õ wg czujnika) i magicznie usuwa szum. Fundament robotyki.

---

## 67_Contrastive_Learning_SimCLR.ipynb ‚Äì SimCLR / CLIP

**Dlaczego?** Jak nauczyƒá sieƒá rozumieƒá obrazy bez etykiet?

**Temat:** Uczenie kontrastowe. Bierzemy zdjƒôcie psa, robimy mu 2 r√≥≈ºne filtry (obr√≥t, kolor). M√≥wimy sieci: "Te dwa r√≥≈ºne obrazy to to samo. Zbli≈º ich wektory do siebie".

---

## 68_PPO_RLHF.ipynb ‚Äì Jak powsta≈Ç ChatGPT?

**Dlaczego?** Zwyk≈Çy model jƒôzykowy umie tylko doko≈Ñczyƒá zdanie. ≈ªeby by≈Ç asystentem, musi przej≈õƒá RLHF (Reinforcement Learning from Human Feedback).

**Temat:** Zrozumienie mechanizmu PPO ‚Äì jak nagradzaƒá model jƒôzykowy za dobre odpowiedzi, nie psujƒÖc jego zdolno≈õci jƒôzykowych (Kullback-Leibler penalty).

---

## 69_Data_Drift_Detection.ipynb ‚Äì KS-Test & PSI

**Dlaczego?** Wytrenowa≈Çe≈õ model na danych z 2023. Jest 2025. Model dzia≈Ça ≈∫le. Dlaczego? Bo dane siƒô zmieni≈Çy (Drift).

**Temat:** Matematyczne wykrywanie, czy "≈õwiat siƒô zmieni≈Ç". Test Kolmogorova-Smirnova i Population Stability Index. ObowiƒÖzkowe w bankowo≈õci.

---

## 70_Vision_Transformers_ViT.ipynb ‚Äì Koniec ery CNN?

**Dlaczego?** CNN (sploty) patrzƒÖ lokalnie (okno 3x3). Transformery patrzƒÖ globalnie. Od 2021 roku ViT wypiera CNN w rozpoznawaniu obraz√≥w.

**Temat:** Jak pociƒÖƒá zdjƒôcie na "s≈Çowa" (Patches), dodaƒá im pozycjƒô i wrzuciƒá do Transformera, kt√≥rego zbudowali≈õmy w notatniku 46.

---

## 71_Flash_Attention_KV_Cache.ipynb ‚Äì Jak dzia≈ÇajƒÖ szybkie LLM?

**Dlaczego?** Zwyk≈Çe Attention ma z≈Ço≈ºono≈õƒá O(N¬≤). Dla d≈Çugich tekst√≥w pamiƒôƒá GPU wybucha.

**Temat:** Zrozumienie optymalizacji sprzƒôtowej (Tiling) i mechanizmu Key-Value Cache (nie licz tego samego dwa razy), kt√≥ry pozwala ChatGPT odpisywaƒá w czasie rzeczywistym.

---

## 72_Mamba_SSM.ipynb ‚Äì State Space Models

**Dlaczego?** Transformer zatyka siƒô przy d≈Çugich tekstach. Mamba ma z≈Ço≈ºono≈õƒá liniowƒÖ i pamiƒôta niesko≈Ñczenie d≈Çugo.

**Temat:** Architektura SSM. Dyskretyzacja r√≥wna≈Ñ r√≥≈ºniczkowych w sieci neuronowej.

---

## 73_Mixture_of_Experts_MoE.ipynb ‚Äì Architektura GPT-4

**Dlaczego?** GPT-4 to nie jeden wielki model, ale "rada ekspert√≥w".

**Temat:** Budowa warstwy Gating Network (Routera), kt√≥ra decyduje, kt√≥ry ma≈Çy model (Ekspert) ma obs≈Çu≈ºyƒá dane zapytanie.

---

## 74_Liquid_Neural_Networks.ipynb ‚Äì LFC

**Dlaczego?** Sieci neuronowe sƒÖ statyczne po treningu. Liquid Networks zmieniajƒÖ swoje wagi w czasie rzeczywistym, adaptujƒÖc siƒô do otoczenia.

**Temat:** Implementacja sieci opartej na r√≥wnaniach r√≥≈ºniczkowych, kt√≥ra dzia≈Ça ≈õwietnie w robotyce i dronach.

---

## 75_Meta_Learning_MAML.ipynb ‚Äì Model-Agnostic Meta-Learning

**Dlaczego?** Cz≈Çowiek uczy siƒô po jednym przyk≈Çadzie. Sieƒá potrzebuje tysiƒôcy.

**Temat:** Model-Agnostic Meta-Learning. Trenowanie sieci tak, aby jej wagi by≈Çy "≈Çatwe do szybkiego douczenia" na nowym zadaniu w 5 krokach.

---

## 76_LangChain_ReAct.ipynb ‚Äì Reason + Act

**Dlaczego?** Model jƒôzykowy nie ma dostƒôpu do kalkulatora ani Google.

**Temat:** Budowa pƒôtli Agenta: My≈õl -> Wybierz Narzƒôdzie -> Wykonaj Akcjƒô -> Obserwuj Wynik -> Odpowiedz.

---

## 77_Prompt_Engineering_CoT.ipynb ‚Äì Chain of Thought

**Dlaczego?** Proste pytanie "Ile to 23\*45?" czƒôsto myli model.

**Temat:** Implementacja CoT i Tree of Thoughts. Wymuszanie na modelu "my≈õlenia na g≈Ços" przed podaniem odpowiedzi.

---

## 78_RAG_Evaluation_RAGAS.ipynb ‚Äì RAGAS

**Dlaczego?** Zbudowa≈Çe≈õ RAG (notatnik 20). SkƒÖd wiesz, czy jest dobry?

**Temat:** Metryki: Faithfulness (czy odpowied≈∫ wynika z kontekstu?) i Answer Relevance (czy odpowiada na pytanie?).

---

## 79_Product_Quantization_IVF_PQ.ipynb ‚Äì Vector Database

**Dlaczego?** HNSW (notatnik 65) jest szybkie, ale ≈ºre RAM.

**Temat:** Kompresja wektor√≥w (Product Quantization). Jak zmie≈õciƒá miliard wektor√≥w w pamiƒôci, tracƒÖc minimalnie na precyzji (technologia FAISS).

---

## 80_Speculative_Decoding.ipynb ‚Äì Speculative Decoding

**Dlaczego?** Wielki model jest wolny. Ma≈Çy model jest szybki, ale g≈Çupi.

**Temat:** U≈ºycie ma≈Çego modelu do "zgadywania" 5 kolejnych s≈Ç√≥w, a du≈ºego tylko do ich "zatwierdzania". Przyspieszenie generowania tekstu 2-3x.

---

## 81_Audio_Mel_Spectrograms.ipynb ‚Äì Spektrogramy Melowe

**Dlaczego?** Surowe audio to chaos. Modele (Whisper) "patrzƒÖ" na d≈∫wiƒôk.

**Temat:** Zamiana audio na Mel-Spectrogram (skala logarytmiczna, dopasowana do ucha ludzkiego).

---

## 82_ASR_CTC_Loss.ipynb ‚Äì Connectionist Temporal Classification

**Dlaczego?** Plik audio trwa 5 sekund, tekst ma 10 liter. Jak to zmapowaƒá bez wyr√≥wnywania?

**Temat:** Matematyka CTC. Algorytm, kt√≥ry pozwala sieci "pluƒá" literami w odpowiednich momentach i usuwaƒá duplikaty ("heeeelllo" -> "hello").

---

## 83_Text_to_Speech_Tacotron.ipynb ‚Äì Tacotron Concept

**Dlaczego?** Chcesz, ≈ºeby AI do Ciebie m√≥wi≈Ço.

**Temat:** Architektura Encoder-Decoder z mechanizmem Attention, kt√≥ra zamienia litery na spektrogramy audio.

---

## 84_CLIP_Multimodal.ipynb ‚Äì Contrastive Language-Image Pretraining

**Dlaczego?** Jak po≈ÇƒÖczyƒá tekst z obrazem? (Podstawa DALL-E).

**Temat:** Trening modelu, kt√≥ry umie powiedzieƒá, jak bardzo zdjƒôcie psa pasuje do podpisu "Pies na pla≈ºy".

---

## 85_NeRF_Neural_Radiance_Fields.ipynb ‚Äì Neural Radiance Fields

**Dlaczego?** Jak z 5 zdjƒôƒá 2D zrobiƒá model 3D?

**Temat:** Reprezentacja sceny jako funkcji F(x,y,z)‚Üí(Kolor,Gƒôsto≈õƒá). Ray Marching (strzelanie promieniami przez piksele).

---

## 86_PointNet_Point_Clouds.ipynb ‚Äì Chmury Punkt√≥w

**Dlaczego?** Lidary w samochodach autonomicznych widzƒÖ chmury punkt√≥w, a nie zdjƒôcia.

**Temat:** Architektura, kt√≥ra przetwarza zbi√≥r punkt√≥w 3D (niezale≈ºnie od ich kolejno≈õci) i klasyfikuje obiekt (np. "To jest krzes≈Ço").

---

## 87_Graph_Attention_Networks_GAT.ipynb ‚Äì Graph Attention Networks

**Dlaczego?** Zwyk≈Çy GCN (notatnik 58) traktuje wszystkich sƒÖsiad√≥w tak samo.

**Temat:** Dodanie mechanizmu Attention do graf√≥w. Wƒôze≈Ç sam decyduje, kt√≥ry sƒÖsiad jest wa≈ºny.

---

## 88_Temporal_Fusion_Transformer_TFT.ipynb ‚Äì TFT

**Dlaczego?** LSTM jest stary. TFT to obecny SOTA w prognozowaniu (np. zapotrzebowania na prƒÖd).

**Temat:** Transformer, kt√≥ry obs≈Çuguje wiele horyzont√≥w czasowych i zmienne statyczne (np. lokalizacja sklepu) oraz dynamiczne (pogoda).

---

## 89_NeuralProphet.ipynb ‚Äì NeuralProphet

**Dlaczego?** Prophet od Facebooka jest super, ale wolny.

**Temat:** Hybryda klasycznej dekompozycji (Trend/Sezon) z sieciami neuronowymi w PyTorch.

---

## 90_Isolation_Forest.ipynb ‚Äì Isolation Forest

**Dlaczego?** Autoenkoder (notatnik 40) jest super, ale czasem potrzebujesz szybkiego algorytmu drzewiastego do anomalii.

**Temat:** Algorytm, kt√≥ry pr√≥buje "odizolowaƒá" punkt losowymi ciƒôciami. Im szybciej siƒô to uda, tym bardziej punkt jest podejrzany (anomalia).

---

## 91_LIME.ipynb ‚Äì Local Interpretable Model-agnostic Explanations

**Dlaczego?** SHAP jest dok≈Çadny, ale wolny.

**Temat:** Jak wyja≈õniƒá decyzjƒô skomplikowanego modelu, trenujƒÖc wok√≥≈Ç jednej pr√≥bki ma≈Çy, prosty model liniowy.

---

## 92_Counterfactual_Explanations.ipynb ‚Äì Counterfactual Explanations

**Dlaczego?** Klient pyta: "Co muszƒô zmieniƒá, ≈ºeby dostaƒá kredyt?".

**Temat:** Generowanie alternatywnych scenariuszy: "Gdyby≈õ zarabia≈Ç 500 z≈Ç wiƒôcej, dosta≈Çby≈õ kredyt".

---

## 93_Fairness_Bias_Detection.ipynb ‚Äì Fairness & Bias Detection

**Dlaczego?** Tw√≥j model mo≈ºe dyskryminowaƒá.

**Temat:** Metryki sprawiedliwo≈õci (Disparate Impact, Equal Opportunity). Sprawdzanie, czy model traktuje tak samo r√≥≈ºne grupy spo≈Çeczne.

---

## 94_Causal_Inference.ipynb ‚Äì Do-Calculus

**Dlaczego?** Korelacja to nie przyczynowo≈õƒá. Czy reklama zwiƒôkszy≈Ça sprzeda≈º, czy po prostu by≈Çy ≈õwiƒôta?

**Temat:** Drabina Pearla. Odr√≥≈ºnianie obserwacji P(Y‚à£X) od interwencji.

---

## 95_Model_Pruning.ipynb ‚Äì Przycinanie

**Dlaczego?** Wiele wag w sieci jest bliskich zeru. SƒÖ niepotrzebne.

**Temat:** Zerowanie najs≈Çabszych po≈ÇƒÖcze≈Ñ w sieci neuronowej i dotrenowanie reszty. Odchudzanie modelu o 50-90%.

---

## 96_ONNX_Runtime.ipynb ‚Äì ONNX Runtime

**Dlaczego?** PyTorch jest ≈õwietny do treningu, ale ciƒô≈ºki na produkcji.

**Temat:** Eksport modelu do formatu ONNX (Open Neural Network Exchange), kt√≥ry dzia≈Ça na wszystkim (od serwera po przeglƒÖdarkƒô).

---

## 97_Federated_Learning.ipynb ‚Äì Federated Learning

**Dlaczego?** Dane (np. medyczne) nie mogƒÖ opu≈õciƒá telefonu pacjenta.

**Temat:** Trenowanie modelu na tysiƒÖcach urzƒÖdze≈Ñ lokalnie i przesy≈Çanie do serwera tylko aktualizacji wag (a nie danych).

---

## 98_Adversarial_Attacks_FGSM.ipynb ‚Äì FGSM

**Dlaczego?** Jak oszukaƒá sieƒá, zmieniajƒÖc jeden piksel?

**Temat:** Fast Gradient Sign Method. Generowanie szumu, kt√≥ry jest niewidoczny dla oka, a sprawia, ≈ºe AI widzi banana zamiast pistoletu.

---

## 99_Neural_Architecture_Search_NAS.ipynb ‚Äì NAS

**Dlaczego?** Po co projektowaƒá sieƒá rƒôcznie, skoro AI mo≈ºe zaprojektowaƒá AI?

**Temat:** Prosty algorytm, kt√≥ry sam buduje architekturƒô sieci (dobiera liczbƒô warstw i filtr√≥w) metodƒÖ ewolucyjnƒÖ.

---

## 100_World_Models.ipynb ‚Äì Model-Based RL

**Dlaczego?** Agent RL (DQN) uczy siƒô wolno, bo musi "prze≈ºyƒá" grƒô.

**Temat:** Agent, kt√≥ry najpierw uczy siƒô "modelu ≈õwiata" (jak dzia≈Ça fizyka), a potem "≈õni" (symuluje) w g≈Çowie przysz≈Ço≈õƒá, ≈ºeby podjƒÖƒá decyzjƒô.

---

## 101_MLOps_Platform.ipynb ‚Äì The Grand Finale: End-to-End MLOps Platform

**Dlaczego?** Zwie≈Ñczenie wszystkiego.

**Temat:** Projekt spinajƒÖcy wiedzƒô: Trening modelu -> Wersjonowanie (MLflow/DVC concept) -> Testy (PyTest) -> Serwowanie (FastAPI) -> Monitoring (Drift). (W formie kodu Python, symulacja ca≈Çej platformy).
