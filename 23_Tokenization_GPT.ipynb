{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcca121f",
   "metadata": {},
   "source": [
    "# З Tokenizacja: Dlaczego AI nie umie literowa?\n",
    "\n",
    "Dla czowieka zdanie *\"Kocham AI\"* to dwa sowa i 8 liter (plus spacja).\n",
    "Dla komputera to cig liczb.\n",
    "\n",
    "Mamy trzy podejcia do cicia tekstu:\n",
    "1.  **Char-level:** Ka偶da litera to osobna liczba (a=1, b=2).\n",
    "    *   *Wada:* Teksty s strasznie dugie. Model gubi wtek.\n",
    "2.  **Word-level:** Ka偶de sowo to liczba (dom=500, kot=200).\n",
    "    *   *Wada:* Co ze sowami \"domku\", \"domkiem\", \"przydomowy\"? Sownik robi si gigantyczny.\n",
    "3.  **Sub-word (BPE - Byte Pair Encoding):** Zoty rodek.\n",
    "    *   Czste sowa (\"the\", \"and\", \"jest\") s jednym kawakiem.\n",
    "    *   Rzadkie sowa (\"przydomowy\") s sklejane z kawak贸w (\"przy\", \"dom\", \"owy\").\n",
    "\n",
    "To standard w dzisiejszym AI (GPT, Llama, Claude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b08996a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enkoder zaadowany. Zobaczmy, jak GPT-4 widzi wiat.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.2 environment at: venv\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m7 packages\u001b[0m \u001b[2min 772ms\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 942ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 13ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2025.11.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtiktoken\u001b[0m\u001b[2m==0.12.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Najpierw musimy zainstalowa bibliotek od OpenAI\n",
    "# Odkomentuj lini poni偶ej, jeli uruchamiasz to pierwszy raz:\n",
    "!uv pip install tiktoken\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# adujemy \"sownik\" kt贸rego u偶ywa GPT-4\n",
    "# cl100k_base to nazwa kodowania dla GPT-3.5 i GPT-4\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "print(\"Enkoder zaadowany. Zobaczmy, jak GPT-4 widzi wiat.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a17882a",
   "metadata": {},
   "source": [
    "## Eksperyment 1: Sowo vs Token\n",
    "\n",
    "Sprawd藕my proste zdanie. Zobaczysz, 偶e czasem token to cae sowo (wraz ze spacj!), a czasem kawaek sowa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c43ee87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tekst oryginalny: 'Sztuczna inteligencja jest super'\n",
      "Tokeny (ID): [50, 11949, 98667, 3458, 90745, 967, 5697, 13599, 2307]\n",
      "Liczba s贸w: 4\n",
      "Liczba token贸w: 9\n",
      "------------------------------\n",
      "JAK GPT WIDZI TE KAWAKI?\n",
      "ID 50     -> 'S'\n",
      "ID 11949  -> 'zt'\n",
      "ID 98667  -> 'ucz'\n",
      "ID 3458   -> 'na'\n",
      "ID 90745  -> ' intelig'\n",
      "ID 967    -> 'enc'\n",
      "ID 5697   -> 'ja'\n",
      "ID 13599  -> ' jest'\n",
      "ID 2307   -> ' super'\n"
     ]
    }
   ],
   "source": [
    "text = \"Sztuczna inteligencja jest super\"\n",
    "\n",
    "# 1. Zamieniamy tekst na liczby (Tokeny)\n",
    "tokens = encoding.encode(text)\n",
    "\n",
    "print(f\"Tekst oryginalny: '{text}'\")\n",
    "print(f\"Tokeny (ID): {tokens}\")\n",
    "print(f\"Liczba s贸w: {len(text.split())}\")\n",
    "print(f\"Liczba token贸w: {len(tokens)}\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"JAK GPT WIDZI TE KAWAKI?\")\n",
    "\n",
    "# 2. Zamieniamy liczby z powrotem na tekst, kawaek po kawaku\n",
    "for t in tokens:\n",
    "    decoded = encoding.decode([t])\n",
    "    print(f\"ID {t:<6} -> '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331a532a",
   "metadata": {},
   "source": [
    "## Eksperyment 2: Problem \"Strawberry\" (i matematyki)\n",
    "\n",
    "Dlaczego ChatGPT myli si w prostych zadaniach?\n",
    "Sp贸jrzmy na sowo \"Strawberry\" i dziaanie matematyczne.\n",
    "\n",
    "Jeli model widzi token \"berry\" jako jedn cao (jedn liczb), to **nie ma fizycznej mo偶liwoci**, 偶eby wiedzia, ile tam jest liter \"r\", chyba 偶e nauczy si tego na pami. On nie widzi liter w rodku tokena!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45a7bafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ANALIZA TRUDNYCH SW ---\n",
      "Sowo: 'Strawberry'\n",
      "Tokeny: ['Str', 'aw', 'berry']\n",
      "ID:     [2645, 675, 15717]\n",
      "--------------------\n",
      "Sowo: '9.11'\n",
      "Tokeny: ['9', '.', '11']\n",
      "ID:     [24, 13, 806]\n",
      "--------------------\n",
      "Sowo: '9.9'\n",
      "Tokeny: ['9', '.', '9']\n",
      "ID:     [24, 13, 24]\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "tricky_words = [\"Strawberry\", \"9.11\", \"9.9\"]\n",
    "\n",
    "print(\"--- ANALIZA TRUDNYCH SW ---\")\n",
    "\n",
    "for word in tricky_words:\n",
    "    ids = encoding.encode(word)\n",
    "    parts = [encoding.decode([id]) for id in ids]\n",
    "    \n",
    "    print(f\"Sowo: '{word}'\")\n",
    "    print(f\"Tokeny: {parts}\")\n",
    "    print(f\"ID:     {ids}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "# ANALIZA WYNIKW (Dla Twojej informacji):\n",
    "# 1. \"Strawberry\" -> Czsto dzieli si na ['Str', 'aw', 'berry'].\n",
    "#    Model widzi 3 klocki. W klocku 'berry' s dwa 'r', ale model widzi to jako jeden symbol.\n",
    "# 2. \"9.11\" vs \"9.9\" ->\n",
    "#    9.9 to czsto jeden token.\n",
    "#    9.11 to czasem dwa tokeny ['9.', '11'].\n",
    "#    Dlatego modele sabo licz na liczbach zmiennoprzecinkowych - traktuj je jak tekst!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f372fbd4",
   "metadata": {},
   "source": [
    "## Eksperyment 3: Jzyk ma znaczenie (Angielski vs Polski)\n",
    "\n",
    "Tokenizatory s trenowane g贸wnie na angielskim internecie.\n",
    "To sprawia, 偶e angielski jest \"taszy\" (mniej token贸w na zdanie) i model lepiej go rozumie.\n",
    "Polski jest \"dro偶szy\" (sowa s szatkowane na wicej drobnych kawak贸w, bo maj koc贸wki fleksyjne).\n",
    "\n",
    "**Pacisz za tokeny, nie za sowa!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84ebf537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angielski: 'I verify that the calculations are correct.'\n",
      "Liczba token贸w: 8\n",
      "Tokeny: ['I', ' verify', ' that', ' the', ' calculations', ' are', ' correct', '.']\n",
      "\n",
      "\n",
      "Polski: 'Potwierdzam, 偶e obliczenia s poprawne.'\n",
      "Liczba token贸w: 15\n",
      "Tokeny: ['Pot', 'w', 'ier', 'dz', 'am', ',', ' 偶e', ' ob', 'lic', 'zenia', ' s', ' pop', 'raw', 'ne', '.']\n",
      "------------------------------\n",
      "Wniosek: To samo zdanie w jzyku polskim zajmuje 88% wicej token贸w (pamici i pienidzy)!\n"
     ]
    }
   ],
   "source": [
    "text_en = \"I verify that the calculations are correct.\"\n",
    "text_pl = \"Potwierdzam, 偶e obliczenia s poprawne.\"\n",
    "\n",
    "tokens_en = encoding.encode(text_en)\n",
    "tokens_pl = encoding.encode(text_pl)\n",
    "\n",
    "print(f\"Angielski: '{text_en}'\")\n",
    "print(f\"Liczba token贸w: {len(tokens_en)}\")\n",
    "print(f\"Tokeny: {[encoding.decode([t]) for t in tokens_en]}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Polski: '{text_pl}'\")\n",
    "print(f\"Liczba token贸w: {len(tokens_pl)}\")\n",
    "print(f\"Tokeny: {[encoding.decode([t]) for t in tokens_pl]}\")\n",
    "\n",
    "diff = (len(tokens_pl) - len(tokens_en)) / len(tokens_en) * 100\n",
    "print(\"-\" * 30)\n",
    "print(f\"Wniosek: To samo zdanie w jzyku polskim zajmuje {diff:.0f}% wicej token贸w (pamici i pienidzy)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdc315a",
   "metadata": {},
   "source": [
    "##  Podsumowanie: wiat z Klock贸w Lego\n",
    "\n",
    "Co musisz zapamita jako AI Engineer?\n",
    "\n",
    "1.  **Token $\\neq$ Sowo.** rednio 1000 s贸w to okoo 1300 token贸w (w angielskim). W polskim ta r贸偶nica jest jeszcze wiksza.\n",
    "2.  **lepota modelu.** Model widzi tokeny jako niepodzielne klocki. Jeli token `12345` oznacza \"Strawberry\", to model nie wie, z jakich liter si skada, dop贸ki nie zobaczy token贸w literowych `S-t-r-a-w...`.\n",
    "3.  **Optymalizacja koszt贸w.** Piszc prompty, pamitaj, 偶e jzyk polski zjada limit kontekstu szybciej ni偶 angielski. Czasami warto przetumaczy instrukcj systemow na angielski, nawet jeli bot ma odpowiada po polsku.\n",
    "\n",
    "**Tu jest haczyk.**\n",
    "Dlatego GPT-4 jest lepsze w kodowaniu ni偶 w pisaniu wierszy rymowanych. Kod ma jasn struktur token贸w. Rymy wymagaj syszenia brzmienia koc贸wek s贸w, a tokenizator czsto te koc贸wki ucina w dziwnych miejscach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
