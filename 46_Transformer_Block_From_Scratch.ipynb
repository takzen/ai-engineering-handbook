{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caf7090a",
   "metadata": {},
   "source": [
    "# ğŸ—ï¸ Transformer Block: Budujemy klocki GPT\n",
    "\n",
    "W notatniku 24 zrozumieliÅ›my matematykÄ™ *Attention*.\n",
    "Teraz zbudujemy peÅ‚noprawny **Blok Transformera**.\n",
    "\n",
    "Struktura pojedynczego bloku wyglÄ…da tak:\n",
    "$$ x_{new} = x + Attention(Norm(x)) $$\n",
    "$$ x_{final} = x_{new} + MLP(Norm(x_{new})) $$\n",
    "\n",
    "Kluczowe sÄ… tu **PoÅ‚Ä…czenia Rezydualne (Residual Connections)**, czyli to `x + ...`.\n",
    "DziaÅ‚ajÄ… jak autostrada dla gradientÃ³w. DziÄ™ki nim sieÄ‡ moÅ¼e mieÄ‡ 100 warstw i nadal siÄ™ uczyÄ‡ (sygnaÅ‚ nie zanika).\n",
    "\n",
    "ZÅ‚oÅ¼ymy to z 3 klas:\n",
    "1.  `SelfAttention` (Oczy).\n",
    "2.  `FeedForward` (MÃ³zg).\n",
    "3.  `TransformerBlock` (CaÅ‚oÅ›Ä‡)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5086b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WejÅ›cie: torch.Size([1, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Konfiguracja\n",
    "EMBED_DIM = 256   # Rozmiar wektora sÅ‚owa (d_model)\n",
    "HEAD_DIM = 64     # Rozmiar jednej gÅ‚owicy\n",
    "NUM_HEADS = 4     # Ile gÅ‚owic? (4 * 64 = 256, musi siÄ™ zgadzaÄ‡)\n",
    "SEQ_LEN = 10      # DÅ‚ugoÅ›Ä‡ zdania\n",
    "\n",
    "# Dane wejÅ›ciowe (Batch=1, SÅ‚Ã³w=10, Cech=256)\n",
    "x = torch.randn(1, SEQ_LEN, EMBED_DIM)\n",
    "\n",
    "print(f\"WejÅ›cie: {x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c0229f",
   "metadata": {},
   "source": [
    "## 1. ModuÅ‚ Attention (Oczy)\n",
    "\n",
    "Tu dzieje siÄ™ magia z Query, Key, Value.\n",
    "W PyTorch uÅ¼ywamy `nn.Linear` do stworzenia wag $W_q, W_k, W_v$.\n",
    "\n",
    "Dla uproszczenia zrobimy jednÄ… duÅ¼Ä… gÅ‚owicÄ™ (Single Head), Å¼eby kod byÅ‚ czytelny, ale w prawdziwym GPT dzieli siÄ™ to na wiele mniejszych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "026e4113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WyjÅ›cie z Attention: torch.Size([1, 10, 256]) (KsztaÅ‚t zachowany!)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Warstwy generujÄ…ce Q, K, V\n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Warstwa wyjÅ›ciowa (Å‚Ä…czenie wynikÃ³w)\n",
    "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [Batch, Seq_Len, Embed_Dim]\n",
    "        \n",
    "        # 1. Generujemy Q, K, V\n",
    "        Q = self.q_linear(x)\n",
    "        K = self.k_linear(x)\n",
    "        V = self.v_linear(x)\n",
    "        \n",
    "        # 2. Obliczamy Scores (Q * K^T)\n",
    "        # transpose(-2, -1) zamienia dwa ostatnie wymiary (Seq i Dim)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        \n",
    "        # 3. Skalowanie (dzielimy przez pierwiastek z wymiaru)\n",
    "        scores = scores / math.sqrt(self.embed_dim)\n",
    "        \n",
    "        # 4. Softmax (zamiana na prawdopodobieÅ„stwo)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # 5. MnoÅ¼ymy przez V\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # 6. Ostatnie liniowe przeksztaÅ‚cenie\n",
    "        output = self.out_linear(context)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test\n",
    "attn = SelfAttention(EMBED_DIM)\n",
    "out = attn(x)\n",
    "print(f\"WyjÅ›cie z Attention: {out.shape} (KsztaÅ‚t zachowany!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39778b46",
   "metadata": {},
   "source": [
    "## 2. ModuÅ‚ Feed Forward (MÃ³zg)\n",
    "\n",
    "To jest zwykÅ‚a sieÄ‡ MLP wÅ‚oÅ¼ona do Å›rodka.\n",
    "Zazwyczaj rozszerza wymiar 4-krotnie (np. 256 -> 1024), a potem go zwÄ™Å¼a z powrotem.\n",
    "To tutaj model \"przemyÅ›liwuje\" to, co zobaczyÅ‚ w Attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a83c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WyjÅ›cie z FeedForward: torch.Size([1, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * expansion), # Rozszerzamy\n",
    "            nn.ReLU(),                                   # NieliniowoÅ›Ä‡\n",
    "            nn.Linear(embed_dim * expansion, embed_dim)  # ZwÄ™Å¼amy\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Test\n",
    "ff = FeedForward(EMBED_DIM)\n",
    "out_ff = ff(x)\n",
    "print(f\"WyjÅ›cie z FeedForward: {out_ff.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a49611e",
   "metadata": {},
   "source": [
    "## 3. The Transformer Block (CaÅ‚oÅ›Ä‡)\n",
    "\n",
    "Teraz skÅ‚adamy klocki.\n",
    "Dodajemy dwa kluczowe elementy:\n",
    "1.  **LayerNorm:** Normalizacja (Å¼eby liczby nie uciekÅ‚y w nieskoÅ„czonoÅ›Ä‡).\n",
    "2.  **Add (Residual Connection):** Dodajemy oryginaÅ‚ do wyniku.\n",
    "\n",
    "WzÃ³r: `x = x + Layer(Norm(x))` (tzw. Pre-Norm architecture, uÅ¼ywana w GPT-3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eef50521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "WejÅ›cie: torch.Size([1, 10, 256])\n",
      "WyjÅ›cie: torch.Size([1, 10, 256])\n",
      "------------------------------\n",
      "Sukces! Wymiary siÄ™ zgadzajÄ….\n",
      "MoÅ¼emy teraz postawiÄ‡ 96 takich blokÃ³w jeden na drugim i mamy architekturÄ™ GPT-3.\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttention(embed_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.feed_forward = FeedForward(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # KROK 1: Attention z Residualem\n",
    "        # Zapisujemy kopiÄ™ x (residual)\n",
    "        residual = x\n",
    "        # Najpierw normalizacja (Pre-Norm), potem uwaga\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention(x)\n",
    "        # Dodajemy oryginaÅ‚ (Add)\n",
    "        x = x + residual\n",
    "        \n",
    "        # KROK 2: Feed Forward z Residualem\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.feed_forward(x)\n",
    "        # Dodajemy oryginaÅ‚ (Add)\n",
    "        x = x + residual\n",
    "        \n",
    "        return x\n",
    "\n",
    "# WIELKI TEST\n",
    "block = TransformerBlock(EMBED_DIM)\n",
    "final_output = block(x)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"WejÅ›cie: {x.shape}\")\n",
    "print(f\"WyjÅ›cie: {final_output.shape}\")\n",
    "print(\"-\" * 30)\n",
    "print(\"Sukces! Wymiary siÄ™ zgadzajÄ….\")\n",
    "print(\"MoÅ¼emy teraz postawiÄ‡ 96 takich blokÃ³w jeden na drugim i mamy architekturÄ™ GPT-3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65958dff",
   "metadata": {},
   "source": [
    "## ğŸ§  Podsumowanie: Autostrada dla Gradientu\n",
    "\n",
    "Dlaczego ten \"Residual Connection\" (`x + residual`) jest taki waÅ¼ny?\n",
    "\n",
    "WyobraÅº sobie sieÄ‡, ktÃ³ra ma 100 warstw.\n",
    "Podczas treningu (Backpropagation), gradient musi przejÅ›Ä‡ przez mnoÅ¼enie 100 razy.\n",
    "JeÅ›li mnoÅ¼ysz liczby mniejsze od 1 (np. 0.9) przez siebie 100 razy -> wynik to 0.00002. Gradient znika. SieÄ‡ przestaje siÄ™ uczyÄ‡.\n",
    "\n",
    "**Dodawanie (`+`)** dziaÅ‚a jak \"bypass\" (obwodnica).\n",
    "Gradient moÅ¼e popÅ‚ynÄ…Ä‡ \"autostradÄ…\" bezpoÅ›rednio od koÅ„ca do poczÄ…tku sieci, omijajÄ…c skomplikowane obliczenia w Attention/MLP.\n",
    "DziÄ™ki temu moÅ¼emy trenowaÄ‡ **bardzo gÅ‚Ä™bokie** sieci.\n",
    "\n",
    "To jest sekret sukcesu ResNet (obrazy) i TransformerÃ³w (tekst)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
