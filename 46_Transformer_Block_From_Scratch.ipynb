{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n<a href=\"https://colab.research.google.com/github/takzen/ai-engineering-handbook/blob/main/46_Transformer_Block_From_Scratch.ipynb\" target=\"_parent\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf7090a",
   "metadata": {},
   "source": [
    "# \ud83c\udfd7\ufe0f Transformer Block: Budujemy klocki GPT\n",
    "\n",
    "W notatniku 24 zrozumieli\u015bmy matematyk\u0119 *Attention*.\n",
    "Teraz zbudujemy pe\u0142noprawny **Blok Transformera**.\n",
    "\n",
    "Struktura pojedynczego bloku wygl\u0105da tak:\n",
    "$$ x_{new} = x + Attention(Norm(x)) $$\n",
    "$$ x_{final} = x_{new} + MLP(Norm(x_{new})) $$\n",
    "\n",
    "Kluczowe s\u0105 tu **Po\u0142\u0105czenia Rezydualne (Residual Connections)**, czyli to `x + ...`.\n",
    "Dzia\u0142aj\u0105 jak autostrada dla gradient\u00f3w. Dzi\u0119ki nim sie\u0107 mo\u017ce mie\u0107 100 warstw i nadal si\u0119 uczy\u0107 (sygna\u0142 nie zanika).\n",
    "\n",
    "Z\u0142o\u017cymy to z 3 klas:\n",
    "1.  `SelfAttention` (Oczy).\n",
    "2.  `FeedForward` (M\u00f3zg).\n",
    "3.  `TransformerBlock` (Ca\u0142o\u015b\u0107)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5086b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wej\u015bcie: torch.Size([1, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Konfiguracja\n",
    "EMBED_DIM = 256   # Rozmiar wektora s\u0142owa (d_model)\n",
    "HEAD_DIM = 64     # Rozmiar jednej g\u0142owicy\n",
    "NUM_HEADS = 4     # Ile g\u0142owic? (4 * 64 = 256, musi si\u0119 zgadza\u0107)\n",
    "SEQ_LEN = 10      # D\u0142ugo\u015b\u0107 zdania\n",
    "\n",
    "# Dane wej\u015bciowe (Batch=1, S\u0142\u00f3w=10, Cech=256)\n",
    "x = torch.randn(1, SEQ_LEN, EMBED_DIM)\n",
    "\n",
    "print(f\"Wej\u015bcie: {x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c0229f",
   "metadata": {},
   "source": [
    "## 1. Modu\u0142 Attention (Oczy)\n",
    "\n",
    "Tu dzieje si\u0119 magia z Query, Key, Value.\n",
    "W PyTorch u\u017cywamy `nn.Linear` do stworzenia wag $W_q, W_k, W_v$.\n",
    "\n",
    "Dla uproszczenia zrobimy jedn\u0105 du\u017c\u0105 g\u0142owic\u0119 (Single Head), \u017ceby kod by\u0142 czytelny, ale w prawdziwym GPT dzieli si\u0119 to na wiele mniejszych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "026e4113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyj\u015bcie z Attention: torch.Size([1, 10, 256]) (Kszta\u0142t zachowany!)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Warstwy generuj\u0105ce Q, K, V\n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Warstwa wyj\u015bciowa (\u0142\u0105czenie wynik\u00f3w)\n",
    "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [Batch, Seq_Len, Embed_Dim]\n",
    "        \n",
    "        # 1. Generujemy Q, K, V\n",
    "        Q = self.q_linear(x)\n",
    "        K = self.k_linear(x)\n",
    "        V = self.v_linear(x)\n",
    "        \n",
    "        # 2. Obliczamy Scores (Q * K^T)\n",
    "        # transpose(-2, -1) zamienia dwa ostatnie wymiary (Seq i Dim)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        \n",
    "        # 3. Skalowanie (dzielimy przez pierwiastek z wymiaru)\n",
    "        scores = scores / math.sqrt(self.embed_dim)\n",
    "        \n",
    "        # 4. Softmax (zamiana na prawdopodobie\u0144stwo)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # 5. Mno\u017cymy przez V\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # 6. Ostatnie liniowe przekszta\u0142cenie\n",
    "        output = self.out_linear(context)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test\n",
    "attn = SelfAttention(EMBED_DIM)\n",
    "out = attn(x)\n",
    "print(f\"Wyj\u015bcie z Attention: {out.shape} (Kszta\u0142t zachowany!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39778b46",
   "metadata": {},
   "source": [
    "## 2. Modu\u0142 Feed Forward (M\u00f3zg)\n",
    "\n",
    "To jest zwyk\u0142a sie\u0107 MLP w\u0142o\u017cona do \u015brodka.\n",
    "Zazwyczaj rozszerza wymiar 4-krotnie (np. 256 -> 1024), a potem go zw\u0119\u017ca z powrotem.\n",
    "To tutaj model \"przemy\u015bliwuje\" to, co zobaczy\u0142 w Attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a83c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyj\u015bcie z FeedForward: torch.Size([1, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * expansion), # Rozszerzamy\n",
    "            nn.ReLU(),                                   # Nieliniowo\u015b\u0107\n",
    "            nn.Linear(embed_dim * expansion, embed_dim)  # Zw\u0119\u017camy\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Test\n",
    "ff = FeedForward(EMBED_DIM)\n",
    "out_ff = ff(x)\n",
    "print(f\"Wyj\u015bcie z FeedForward: {out_ff.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a49611e",
   "metadata": {},
   "source": [
    "## 3. The Transformer Block (Ca\u0142o\u015b\u0107)\n",
    "\n",
    "Teraz sk\u0142adamy klocki.\n",
    "Dodajemy dwa kluczowe elementy:\n",
    "1.  **LayerNorm:** Normalizacja (\u017ceby liczby nie uciek\u0142y w niesko\u0144czono\u015b\u0107).\n",
    "2.  **Add (Residual Connection):** Dodajemy orygina\u0142 do wyniku.\n",
    "\n",
    "Wz\u00f3r: `x = x + Layer(Norm(x))` (tzw. Pre-Norm architecture, u\u017cywana w GPT-3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eef50521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Wej\u015bcie: torch.Size([1, 10, 256])\n",
      "Wyj\u015bcie: torch.Size([1, 10, 256])\n",
      "------------------------------\n",
      "Sukces! Wymiary si\u0119 zgadzaj\u0105.\n",
      "Mo\u017cemy teraz postawi\u0107 96 takich blok\u00f3w jeden na drugim i mamy architektur\u0119 GPT-3.\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttention(embed_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.feed_forward = FeedForward(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # KROK 1: Attention z Residualem\n",
    "        # Zapisujemy kopi\u0119 x (residual)\n",
    "        residual = x\n",
    "        # Najpierw normalizacja (Pre-Norm), potem uwaga\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention(x)\n",
    "        # Dodajemy orygina\u0142 (Add)\n",
    "        x = x + residual\n",
    "        \n",
    "        # KROK 2: Feed Forward z Residualem\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.feed_forward(x)\n",
    "        # Dodajemy orygina\u0142 (Add)\n",
    "        x = x + residual\n",
    "        \n",
    "        return x\n",
    "\n",
    "# WIELKI TEST\n",
    "block = TransformerBlock(EMBED_DIM)\n",
    "final_output = block(x)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Wej\u015bcie: {x.shape}\")\n",
    "print(f\"Wyj\u015bcie: {final_output.shape}\")\n",
    "print(\"-\" * 30)\n",
    "print(\"Sukces! Wymiary si\u0119 zgadzaj\u0105.\")\n",
    "print(\"Mo\u017cemy teraz postawi\u0107 96 takich blok\u00f3w jeden na drugim i mamy architektur\u0119 GPT-3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65958dff",
   "metadata": {},
   "source": [
    "## \ud83e\udde0 Podsumowanie: Autostrada dla Gradientu\n",
    "\n",
    "Dlaczego ten \"Residual Connection\" (`x + residual`) jest taki wa\u017cny?\n",
    "\n",
    "Wyobra\u017a sobie sie\u0107, kt\u00f3ra ma 100 warstw.\n",
    "Podczas treningu (Backpropagation), gradient musi przej\u015b\u0107 przez mno\u017cenie 100 razy.\n",
    "Je\u015bli mno\u017cysz liczby mniejsze od 1 (np. 0.9) przez siebie 100 razy -> wynik to 0.00002. Gradient znika. Sie\u0107 przestaje si\u0119 uczy\u0107.\n",
    "\n",
    "**Dodawanie (`+`)** dzia\u0142a jak \"bypass\" (obwodnica).\n",
    "Gradient mo\u017ce pop\u0142yn\u0105\u0107 \"autostrad\u0105\" bezpo\u015brednio od ko\u0144ca do pocz\u0105tku sieci, omijaj\u0105c skomplikowane obliczenia w Attention/MLP.\n",
    "Dzi\u0119ki temu mo\u017cemy trenowa\u0107 **bardzo g\u0142\u0119bokie** sieci.\n",
    "\n",
    "To jest sekret sukcesu ResNet (obrazy) i Transformer\u00f3w (tekst)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}