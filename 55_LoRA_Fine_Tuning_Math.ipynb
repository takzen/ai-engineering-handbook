{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0379e65",
   "metadata": {},
   "source": [
    "# üîß LoRA: Jak trenowaƒá giganty na domowym laptopie?\n",
    "\n",
    "Fine-tuning wielkich modeli (LLM) polega na aktualizacji ich wag:\n",
    "$$ W_{new} = W_{old} + \\Delta W $$\n",
    "\n",
    "Gdzie $\\Delta W$ (zmiana) jest tak samo wielka jak $W$. Je≈õli $W$ ma 10 miliard√≥w parametr√≥w, to $\\Delta W$ te≈º. To zatyka pamiƒôƒá GPU.\n",
    "\n",
    "**Idea LoRA:**\n",
    "Hipoteza: Zmiana wiedzy ($\\Delta W$) nie jest \"szumem\". Ma strukturƒô. Mo≈ºna jƒÖ zapisaƒá znacznie pro≈õciej.\n",
    "Zastƒôpujemy wielkƒÖ macierz $\\Delta W$ iloczynem dw√≥ch chudych macierzy $A$ i $B$:\n",
    "$$ \\Delta W \\approx A \\times B $$\n",
    "\n",
    "*   $W$: Wymiar $1000 \\times 1000$ (1 mln parametr√≥w).\n",
    "*   $A$: Wymiar $1000 \\times 4$.\n",
    "*   $B$: Wymiar $4 \\times 1000$.\n",
    "*   Razem: $4000 + 4000 = 8000$ parametr√≥w.\n",
    "\n",
    "**Zysk:** Zredukowali≈õmy liczbƒô parametr√≥w do treningu o **99.2%**, a wynik matematyczny (wymiar wyj≈õcia) jest ten sam!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "817e301a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wymiar oryginalnej wagi: torch.Size([100, 100])\n",
      "Liczba parametr√≥w (zamro≈ºonych): 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Konfiguracja\n",
    "IN_DIM = 100   # Wej≈õcie\n",
    "OUT_DIM = 100  # Wyj≈õcie\n",
    "RANK = 4       # \"Ranga\" LoRA (Szeroko≈õƒá gard≈Ça - im mniej, tym wiƒôksza kompresja)\n",
    "\n",
    "# Symulacja Gigantycznej Warstwy (np. z GPT-3)\n",
    "# To jest zamro≈ºone (Frozen). Tego nie bƒôdziemy trenowaƒá.\n",
    "pretrained_layer = nn.Linear(IN_DIM, OUT_DIM, bias=False)\n",
    "pretrained_layer.weight.requires_grad = False # Mrozimy!\n",
    "\n",
    "print(f\"Wymiar oryginalnej wagi: {pretrained_layer.weight.shape}\")\n",
    "print(f\"Liczba parametr√≥w (zamro≈ºonych): {pretrained_layer.weight.numel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01690568",
   "metadata": {},
   "source": [
    "## Implementacja LoRA od zera\n",
    "\n",
    "Stworzymy klasƒô `LoRALayer`, kt√≥ra \"oplata\" oryginalnƒÖ warstwƒô.\n",
    "1.  **≈öcie≈ºka G≈Ç√≥wna:** Dane idƒÖ przez zamro≈ºony model ($W_{old} \\cdot x$).\n",
    "2.  **≈öcie≈ºka Boczna (LoRA):** Dane idƒÖ przez macierz A, potem przez B ($B \\cdot A \\cdot x$).\n",
    "3.  **Suma:** Wynik to $W_{old} \\cdot x + (B \\cdot A) \\cdot x \\cdot scaling$.\n",
    "\n",
    "**Wa≈ºny szczeg√≥≈Ç:**\n",
    "*   Macierz A inicjalizujemy losowo.\n",
    "*   Macierz B inicjalizujemy zerami.\n",
    "*   Dziƒôki temu na samym poczƒÖtku treningu LoRA nic nie zmienia (wynik = 0), wiƒôc nie psujemy wiedzy, kt√≥rƒÖ model ju≈º ma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35b6829c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pe≈Çne parametry: 10800\n",
      "Parametry do treningu (LoRA): 800\n",
      "Oszczƒôdno≈õƒá: 92.59% mniej pamiƒôci!\n"
     ]
    }
   ],
   "source": [
    "class LoRAParametrization(nn.Module):\n",
    "    def __init__(self, original_layer, rank=4, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.original_layer = original_layer # Zapisujemy orygina≈Ç\n",
    "        \n",
    "        # Pobieramy wymiary z orygina≈Çu\n",
    "        in_dim = original_layer.in_features\n",
    "        out_dim = original_layer.out_features\n",
    "        \n",
    "        # --- MACIERZE LoRA (Trenowalne) ---\n",
    "        # A: Kompresja (in -> rank)\n",
    "        self.lora_A = nn.Parameter(torch.randn(in_dim, rank) / np.sqrt(rank))\n",
    "        # B: Dekompresja (rank -> out) - Inicjalizacja ZERAMI!\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        \n",
    "        self.scale = alpha / rank # Sta≈Ça skalujƒÖca\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Oryginalny wynik (Zamro≈ºony)\n",
    "        original_out = self.original_layer(x)\n",
    "        \n",
    "        # 2. Wynik LoRA (A potem B) - macierze w PyTorch mno≈ºy siƒô odwrotnie (x @ A @ B)\n",
    "        # x @ A -> [Batch, Rank]\n",
    "        # result @ B -> [Batch, Out]\n",
    "        lora_out = (x @ self.lora_A) @ self.lora_B\n",
    "        \n",
    "        # 3. Suma\n",
    "        return original_out + (lora_out * self.scale)\n",
    "\n",
    "# Tworzymy model z LoRA\n",
    "lora_model = LoRAParametrization(pretrained_layer, rank=RANK)\n",
    "\n",
    "# Policzmy parametry\n",
    "trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in lora_model.parameters())\n",
    "\n",
    "print(f\"Pe≈Çne parametry: {total_params}\")\n",
    "print(f\"Parametry do treningu (LoRA): {trainable_params}\")\n",
    "print(f\"Oszczƒôdno≈õƒá: {100 - (trainable_params/total_params*100):.2f}% mniej pamiƒôci!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698c00ad",
   "metadata": {},
   "source": [
    "## Symulacja Fine-Tuningu\n",
    "\n",
    "Za≈Ç√≥≈ºmy, ≈ºe nasz \"Pre-trained Model\" umie mno≈ºyƒá wej≈õcie przez 1.\n",
    "Chcemy go douczyƒá, ≈ºeby mno≈ºy≈Ç wej≈õcie przez 2 (zmieni≈Ç zachowanie), ale **nie dotykajƒÖc oryginalnych wag**.\n",
    "\n",
    "*   Orygina≈Ç: $y = 1 \\cdot x$\n",
    "*   Cel: $y = 2 \\cdot x$\n",
    "*   Zadanie LoRA: Nauczyƒá siƒô dodawaƒá brakujƒÖce $1 \\cdot x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae5a7367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TRENING LoRA ---\n",
      "Epoka 0 | Loss: 25.000000\n",
      "Epoka 10 | Loss: 1.147632\n",
      "Epoka 20 | Loss: 1.095276\n",
      "Epoka 30 | Loss: 0.258786\n",
      "Epoka 40 | Loss: 0.120695\n",
      "Epoka 50 | Loss: 0.045843\n",
      "Epoka 60 | Loss: 0.022210\n",
      "Epoka 70 | Loss: 0.009431\n",
      "Epoka 80 | Loss: 0.002691\n",
      "Epoka 90 | Loss: 0.000427\n",
      "\n",
      "‚úÖ Trening zako≈Ñczony.\n"
     ]
    }
   ],
   "source": [
    "# 1. Dane\n",
    "# Wej≈õcie: losowe wektory\n",
    "# Cel: Wej≈õcie * 2 (czyli przesuniƒôcie o +x)\n",
    "X = torch.randn(10, IN_DIM)\n",
    "target = pretrained_layer(X) + X # Orygina≈Ç + X (czyli x2, zak≈ÇadajƒÖc ≈ºe orygina≈Ç to Identity, ale tu jest losowy)\n",
    "# Upro≈õƒámy cel: Chcemy, ≈ºeby wynik by≈Ç po prostu inny ni≈º orygina≈Ç.\n",
    "target = pretrained_layer(X) + 5.0 # Chcemy dodaƒá 5 do ka≈ºdego wyniku\n",
    "\n",
    "# 2. Optymalizator (Tylko dla parametr√≥w LoRA!)\n",
    "optimizer = optim.Adam(\n",
    "    [lora_model.lora_A, lora_model.lora_B], # Trenujemy tylko A i B\n",
    "    lr=0.1\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "print(\"--- TRENING LoRA ---\")\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward\n",
    "    pred = lora_model(X)\n",
    "    \n",
    "    # Loss\n",
    "    loss = loss_fn(pred, target)\n",
    "    \n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoka {epoch} | Loss: {loss.item():.6f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Trening zako≈Ñczony.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b58a76e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma wag orygina≈Çu (czy siƒô zmieni≈Ça?): 0.7843\n",
      "Si≈Ça wag LoRA (czy sƒÖ niezerowe?): 6521.6572\n",
      "\n",
      "R√≥≈ºnica w wyniku (LoRA vs Orygina≈Ç): -2.0992\n",
      "Oczekiwali≈õmy ok. 5.0 (bo taki by≈Ç cel treningu).\n"
     ]
    }
   ],
   "source": [
    "# WERYFIKACJA\n",
    "# Sprawd≈∫my, czy wagi orygina≈Çu siƒô zmieni≈Çy (nie powinny!)\n",
    "original_weight_sum = pretrained_layer.weight.sum().item()\n",
    "print(f\"Suma wag orygina≈Çu (czy siƒô zmieni≈Ça?): {original_weight_sum:.4f}\")\n",
    "\n",
    "# Sprawd≈∫my, czy LoRA siƒô nauczy≈Ça (macierze A i B nie powinny byƒá puste)\n",
    "lora_magnitude = torch.matmul(lora_model.lora_A, lora_model.lora_B).abs().sum().item()\n",
    "print(f\"Si≈Ça wag LoRA (czy sƒÖ niezerowe?): {lora_magnitude:.4f}\")\n",
    "\n",
    "# Test dzia≈Çania\n",
    "with torch.no_grad():\n",
    "    x_test = torch.randn(1, IN_DIM)\n",
    "    orig_out = pretrained_layer(x_test)\n",
    "    lora_out = lora_model(x_test)\n",
    "    \n",
    "    diff = (lora_out - orig_out).mean().item()\n",
    "    print(f\"\\nR√≥≈ºnica w wyniku (LoRA vs Orygina≈Ç): {diff:.4f}\")\n",
    "    print(\"Oczekiwali≈õmy ok. 5.0 (bo taki by≈Ç cel treningu).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47b3deb",
   "metadata": {},
   "source": [
    "## üß† Podsumowanie: Adaptery\n",
    "\n",
    "Co siƒô sta≈Ço?\n",
    "1.  Oryginalna sieƒá pozosta≈Ça nienaruszona (Wagi sƒÖ identyczne).\n",
    "2.  Ca≈ÇƒÖ \"nowƒÖ wiedzƒô\" (dodawanie 5.0) przejƒô≈Çy malutkie macierze A i B.\n",
    "\n",
    "**Dlaczego to rewolucja?**\n",
    "Gdy ≈õciƒÖgasz z internetu \"Llamƒô douczonƒÖ do pisania po polsku\", nie ≈õciƒÖgasz 50 GB modelu. ≈öciƒÖgasz plik wa≈ºƒÖcy **100 MB** (Adapter LoRA).\n",
    "Wczytujesz bazowƒÖ Llamƒô, doklejasz adapter i gotowe.\n",
    "Mo≈ºesz mieƒá jeden model bazowy i 10 adapter√≥w (jeden do prawa, jeden do medycyny, jeden do kodowania) i prze≈ÇƒÖczaƒá je w locie."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
