{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n<a href=\"https://colab.research.google.com/github/takzen/ai-engineering-handbook/blob/main/55_LoRA_Fine_Tuning_Math.ipynb\" target=\"_parent\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0379e65",
   "metadata": {},
   "source": [
    "# \ud83d\udd27 LoRA: Jak trenowa\u0107 giganty na domowym laptopie?\n",
    "\n",
    "Fine-tuning wielkich modeli (LLM) polega na aktualizacji ich wag:\n",
    "$$ W_{new} = W_{old} + \\Delta W $$\n",
    "\n",
    "Gdzie $\\Delta W$ (zmiana) jest tak samo wielka jak $W$. Je\u015bli $W$ ma 10 miliard\u00f3w parametr\u00f3w, to $\\Delta W$ te\u017c. To zatyka pami\u0119\u0107 GPU.\n",
    "\n",
    "**Idea LoRA:**\n",
    "Hipoteza: Zmiana wiedzy ($\\Delta W$) nie jest \"szumem\". Ma struktur\u0119. Mo\u017cna j\u0105 zapisa\u0107 znacznie pro\u015bciej.\n",
    "Zast\u0119pujemy wielk\u0105 macierz $\\Delta W$ iloczynem dw\u00f3ch chudych macierzy $A$ i $B$:\n",
    "$$ \\Delta W \\approx A \\times B $$\n",
    "\n",
    "*   $W$: Wymiar $1000 \\times 1000$ (1 mln parametr\u00f3w).\n",
    "*   $A$: Wymiar $1000 \\times 4$.\n",
    "*   $B$: Wymiar $4 \\times 1000$.\n",
    "*   Razem: $4000 + 4000 = 8000$ parametr\u00f3w.\n",
    "\n",
    "**Zysk:** Zredukowali\u015bmy liczb\u0119 parametr\u00f3w do treningu o **99.2%**, a wynik matematyczny (wymiar wyj\u015bcia) jest ten sam!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "817e301a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wymiar oryginalnej wagi: torch.Size([100, 100])\n",
      "Liczba parametr\u00f3w (zamro\u017conych): 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Konfiguracja\n",
    "IN_DIM = 100   # Wej\u015bcie\n",
    "OUT_DIM = 100  # Wyj\u015bcie\n",
    "RANK = 4       # \"Ranga\" LoRA (Szeroko\u015b\u0107 gard\u0142a - im mniej, tym wi\u0119ksza kompresja)\n",
    "\n",
    "# Symulacja Gigantycznej Warstwy (np. z GPT-3)\n",
    "# To jest zamro\u017cone (Frozen). Tego nie b\u0119dziemy trenowa\u0107.\n",
    "pretrained_layer = nn.Linear(IN_DIM, OUT_DIM, bias=False)\n",
    "pretrained_layer.weight.requires_grad = False # Mrozimy!\n",
    "\n",
    "print(f\"Wymiar oryginalnej wagi: {pretrained_layer.weight.shape}\")\n",
    "print(f\"Liczba parametr\u00f3w (zamro\u017conych): {pretrained_layer.weight.numel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01690568",
   "metadata": {},
   "source": [
    "## Implementacja LoRA od zera\n",
    "\n",
    "Stworzymy klas\u0119 `LoRALayer`, kt\u00f3ra \"oplata\" oryginaln\u0105 warstw\u0119.\n",
    "1.  **\u015acie\u017cka G\u0142\u00f3wna:** Dane id\u0105 przez zamro\u017cony model ($W_{old} \\cdot x$).\n",
    "2.  **\u015acie\u017cka Boczna (LoRA):** Dane id\u0105 przez macierz A, potem przez B ($B \\cdot A \\cdot x$).\n",
    "3.  **Suma:** Wynik to $W_{old} \\cdot x + (B \\cdot A) \\cdot x \\cdot scaling$.\n",
    "\n",
    "**Wa\u017cny szczeg\u00f3\u0142:**\n",
    "*   Macierz A inicjalizujemy losowo.\n",
    "*   Macierz B inicjalizujemy zerami.\n",
    "*   Dzi\u0119ki temu na samym pocz\u0105tku treningu LoRA nic nie zmienia (wynik = 0), wi\u0119c nie psujemy wiedzy, kt\u00f3r\u0105 model ju\u017c ma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35b6829c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pe\u0142ne parametry: 10800\n",
      "Parametry do treningu (LoRA): 800\n",
      "Oszcz\u0119dno\u015b\u0107: 92.59% mniej pami\u0119ci!\n"
     ]
    }
   ],
   "source": [
    "class LoRAParametrization(nn.Module):\n",
    "    def __init__(self, original_layer, rank=4, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.original_layer = original_layer # Zapisujemy orygina\u0142\n",
    "        \n",
    "        # Pobieramy wymiary z orygina\u0142u\n",
    "        in_dim = original_layer.in_features\n",
    "        out_dim = original_layer.out_features\n",
    "        \n",
    "        # --- MACIERZE LoRA (Trenowalne) ---\n",
    "        # A: Kompresja (in -> rank)\n",
    "        self.lora_A = nn.Parameter(torch.randn(in_dim, rank) / np.sqrt(rank))\n",
    "        # B: Dekompresja (rank -> out) - Inicjalizacja ZERAMI!\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        \n",
    "        self.scale = alpha / rank # Sta\u0142a skaluj\u0105ca\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Oryginalny wynik (Zamro\u017cony)\n",
    "        original_out = self.original_layer(x)\n",
    "        \n",
    "        # 2. Wynik LoRA (A potem B) - macierze w PyTorch mno\u017cy si\u0119 odwrotnie (x @ A @ B)\n",
    "        # x @ A -> [Batch, Rank]\n",
    "        # result @ B -> [Batch, Out]\n",
    "        lora_out = (x @ self.lora_A) @ self.lora_B\n",
    "        \n",
    "        # 3. Suma\n",
    "        return original_out + (lora_out * self.scale)\n",
    "\n",
    "# Tworzymy model z LoRA\n",
    "lora_model = LoRAParametrization(pretrained_layer, rank=RANK)\n",
    "\n",
    "# Policzmy parametry\n",
    "trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in lora_model.parameters())\n",
    "\n",
    "print(f\"Pe\u0142ne parametry: {total_params}\")\n",
    "print(f\"Parametry do treningu (LoRA): {trainable_params}\")\n",
    "print(f\"Oszcz\u0119dno\u015b\u0107: {100 - (trainable_params/total_params*100):.2f}% mniej pami\u0119ci!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698c00ad",
   "metadata": {},
   "source": [
    "## Symulacja Fine-Tuningu\n",
    "\n",
    "Za\u0142\u00f3\u017cmy, \u017ce nasz \"Pre-trained Model\" umie mno\u017cy\u0107 wej\u015bcie przez 1.\n",
    "Chcemy go douczy\u0107, \u017ceby mno\u017cy\u0142 wej\u015bcie przez 2 (zmieni\u0142 zachowanie), ale **nie dotykaj\u0105c oryginalnych wag**.\n",
    "\n",
    "*   Orygina\u0142: $y = 1 \\cdot x$\n",
    "*   Cel: $y = 2 \\cdot x$\n",
    "*   Zadanie LoRA: Nauczy\u0107 si\u0119 dodawa\u0107 brakuj\u0105ce $1 \\cdot x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae5a7367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TRENING LoRA ---\n",
      "Epoka 0 | Loss: 25.000000\n",
      "Epoka 10 | Loss: 1.147632\n",
      "Epoka 20 | Loss: 1.095276\n",
      "Epoka 30 | Loss: 0.258786\n",
      "Epoka 40 | Loss: 0.120695\n",
      "Epoka 50 | Loss: 0.045843\n",
      "Epoka 60 | Loss: 0.022210\n",
      "Epoka 70 | Loss: 0.009431\n",
      "Epoka 80 | Loss: 0.002691\n",
      "Epoka 90 | Loss: 0.000427\n",
      "\n",
      "\u2705 Trening zako\u0144czony.\n"
     ]
    }
   ],
   "source": [
    "# 1. Dane\n",
    "# Wej\u015bcie: losowe wektory\n",
    "# Cel: Wej\u015bcie * 2 (czyli przesuni\u0119cie o +x)\n",
    "X = torch.randn(10, IN_DIM)\n",
    "target = pretrained_layer(X) + X # Orygina\u0142 + X (czyli x2, zak\u0142adaj\u0105c \u017ce orygina\u0142 to Identity, ale tu jest losowy)\n",
    "# Upro\u015b\u0107my cel: Chcemy, \u017ceby wynik by\u0142 po prostu inny ni\u017c orygina\u0142.\n",
    "target = pretrained_layer(X) + 5.0 # Chcemy doda\u0107 5 do ka\u017cdego wyniku\n",
    "\n",
    "# 2. Optymalizator (Tylko dla parametr\u00f3w LoRA!)\n",
    "optimizer = optim.Adam(\n",
    "    [lora_model.lora_A, lora_model.lora_B], # Trenujemy tylko A i B\n",
    "    lr=0.1\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "print(\"--- TRENING LoRA ---\")\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward\n",
    "    pred = lora_model(X)\n",
    "    \n",
    "    # Loss\n",
    "    loss = loss_fn(pred, target)\n",
    "    \n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoka {epoch} | Loss: {loss.item():.6f}\")\n",
    "\n",
    "print(\"\\n\u2705 Trening zako\u0144czony.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b58a76e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma wag orygina\u0142u (czy si\u0119 zmieni\u0142a?): 0.7843\n",
      "Si\u0142a wag LoRA (czy s\u0105 niezerowe?): 6521.6572\n",
      "\n",
      "R\u00f3\u017cnica w wyniku (LoRA vs Orygina\u0142): -2.0992\n",
      "Oczekiwali\u015bmy ok. 5.0 (bo taki by\u0142 cel treningu).\n"
     ]
    }
   ],
   "source": [
    "# WERYFIKACJA\n",
    "# Sprawd\u017amy, czy wagi orygina\u0142u si\u0119 zmieni\u0142y (nie powinny!)\n",
    "original_weight_sum = pretrained_layer.weight.sum().item()\n",
    "print(f\"Suma wag orygina\u0142u (czy si\u0119 zmieni\u0142a?): {original_weight_sum:.4f}\")\n",
    "\n",
    "# Sprawd\u017amy, czy LoRA si\u0119 nauczy\u0142a (macierze A i B nie powinny by\u0107 puste)\n",
    "lora_magnitude = torch.matmul(lora_model.lora_A, lora_model.lora_B).abs().sum().item()\n",
    "print(f\"Si\u0142a wag LoRA (czy s\u0105 niezerowe?): {lora_magnitude:.4f}\")\n",
    "\n",
    "# Test dzia\u0142ania\n",
    "with torch.no_grad():\n",
    "    x_test = torch.randn(1, IN_DIM)\n",
    "    orig_out = pretrained_layer(x_test)\n",
    "    lora_out = lora_model(x_test)\n",
    "    \n",
    "    diff = (lora_out - orig_out).mean().item()\n",
    "    print(f\"\\nR\u00f3\u017cnica w wyniku (LoRA vs Orygina\u0142): {diff:.4f}\")\n",
    "    print(\"Oczekiwali\u015bmy ok. 5.0 (bo taki by\u0142 cel treningu).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47b3deb",
   "metadata": {},
   "source": [
    "## \ud83e\udde0 Podsumowanie: Adaptery\n",
    "\n",
    "Co si\u0119 sta\u0142o?\n",
    "1.  Oryginalna sie\u0107 pozosta\u0142a nienaruszona (Wagi s\u0105 identyczne).\n",
    "2.  Ca\u0142\u0105 \"now\u0105 wiedz\u0119\" (dodawanie 5.0) przej\u0119\u0142y malutkie macierze A i B.\n",
    "\n",
    "**Dlaczego to rewolucja?**\n",
    "Gdy \u015bci\u0105gasz z internetu \"Llam\u0119 douczon\u0105 do pisania po polsku\", nie \u015bci\u0105gasz 50 GB modelu. \u015aci\u0105gasz plik wa\u017c\u0105cy **100 MB** (Adapter LoRA).\n",
    "Wczytujesz bazow\u0105 Llam\u0119, doklejasz adapter i gotowe.\n",
    "Mo\u017cesz mie\u0107 jeden model bazowy i 10 adapter\u00f3w (jeden do prawa, jeden do medycyny, jeden do kodowania) i prze\u0142\u0105cza\u0107 je w locie."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}