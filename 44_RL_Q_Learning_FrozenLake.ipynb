{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9970424",
   "metadata": {},
   "source": [
    "# ü§ñ Q-Learning: Jak nauczyƒá robota chodziƒá po lodzie?\n",
    "\n",
    "Reinforcement Learning (RL) to nauka metodƒÖ pr√≥b i b≈Çƒôd√≥w.\n",
    "Mamy:\n",
    "1.  **Agenta:** Nasz robot.\n",
    "2.  **≈örodowisko:** Gra (np. FrozenLake - ≈õliskie jezioro z dziurami).\n",
    "3.  **Akcje:** G√≥ra, D√≥≈Ç, Lewo, Prawo.\n",
    "4.  **Nagrody:** +1 za dotarcie do prezentu, 0 za resztƒô (i ≈õmierƒá w dziurze).\n",
    "\n",
    "**Q-Table (Tabela Jako≈õci):**\n",
    "Agent nie ma m√≥zgu (sieci neuronowej). Ma **≈õciƒÖgƒô** (Tabelƒô Excela).\n",
    "*   Wiersze = Gdzie jestem? (Stan).\n",
    "*   Kolumny = Co zrobiƒá? (Akcja).\n",
    "*   Warto≈õƒá = Jak bardzo to siƒô op≈Çaca? (Q-Value).\n",
    "\n",
    "Na poczƒÖtku tabela jest pusta. Agent chodzi losowo. Gdy wpadnie do dziury -> zapisuje \"To by≈Ço g≈Çupie\". Gdy znajdzie prezent -> zapisuje \"To by≈Ço super\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b430c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "≈örodowisko gotowe.\n",
      "Liczba stan√≥w (p√≥l na planszy): 16\n",
      "Liczba akcji (ruchy): 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.2 environment at: venv\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m5 packages\u001b[0m \u001b[2min 590ms\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 2.41s\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 57ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfarama-notifications\u001b[0m\u001b[2m==0.0.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgymnasium\u001b[0m\u001b[2m==1.2.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Instalacja ≈õrodowiska Gym (standard w RL)\n",
    "!uv pip install gymnasium\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Tworzymy ≈õrodowisko FrozenLake\n",
    "# is_slippery=False u≈Çatwia sprawƒô (l√≥d nie jest ≈õliski, robot idzie tam gdzie chce)\n",
    "# render_mode=\"rgb_array\" pozwala nam podglƒÖdaƒá, co siƒô dzieje\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")\n",
    "\n",
    "print(\"≈örodowisko gotowe.\")\n",
    "print(f\"Liczba stan√≥w (p√≥l na planszy): {env.observation_space.n}\")\n",
    "print(f\"Liczba akcji (ruchy): {env.action_space.n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5244e8b1",
   "metadata": {},
   "source": [
    "## Inicjalizacja Q-Table\n",
    "\n",
    "Tworzymy tabelƒô rozmiaru `16 x 4` (16 p√≥l na planszy, 4 mo≈ºliwe ruchy).\n",
    "Wype≈Çniamy jƒÖ zerami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "497e79fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pusta Q-Table ---\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "\n",
    "# Q-Table: [16 wierszy, 4 kolumny]\n",
    "q_table = np.zeros((state_space, action_space))\n",
    "\n",
    "print(\"--- Pusta Q-Table ---\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caa46d3",
   "metadata": {},
   "source": [
    "## Algorytm Q-Learning (R√≥wnanie Bellmana)\n",
    "\n",
    "To jest serce RL. Wz√≥r na aktualizacjƒô wiedzy:\n",
    "\n",
    "$$ Q(s, a) = Q(s, a) + \\alpha [R + \\gamma \\max Q(s', a') - Q(s, a)] $$\n",
    "\n",
    "Po ludzku:\n",
    "Nowa Wiedza = Stara Wiedza + Nauka * (Nagroda + Przewidywana Przysz≈Ço≈õƒá - Stara Wiedza).\n",
    "\n",
    "Parametry:\n",
    "*   **Alpha ($\\alpha$):** Szybko≈õƒá uczenia (0.1 - 0.9).\n",
    "*   **Gamma ($\\gamma$):** Jak bardzo dbamy o przysz≈Ço≈õƒá? (0.9 = bardzo, 0.1 = ≈ºyjemy chwilƒÖ).\n",
    "*   **Epsilon ($\\epsilon$):** Chƒôƒá eksploracji. Na poczƒÖtku (1.0) robimy losowe ruchy, ≈ºeby poznaƒá ≈õwiat. Z czasem zmniejszamy epsilon, ≈ºeby wykorzystywaƒá wiedzƒô."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "171630cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trening zako≈Ñczony!\n"
     ]
    }
   ],
   "source": [
    "# Hiperparametry\n",
    "num_episodes = 2000       # Ile razy gramy?\n",
    "max_steps = 100           # Maksymalna liczba krok√≥w w jednej grze\n",
    "learning_rate = 0.8       # Alpha\n",
    "gamma = 0.95              # Discount factor (przysz≈Ço≈õƒá jest wa≈ºna)\n",
    "\n",
    "# Parametry Eksploracji (Epsilon Greedy)\n",
    "epsilon = 1.0             # Na poczƒÖtku 100% losowo≈õci\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.005        # Jak szybko przestajemy losowaƒá\n",
    "\n",
    "rewards_all_episodes = []\n",
    "\n",
    "# --- G≈Å√ìWNA PƒòTLA TRENINGOWA ---\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 1. Decyzja: Eksploracja (Los) czy Eksploatacja (Wiedza)?\n",
    "        tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        if tradeoff > epsilon:\n",
    "            action = np.argmax(q_table[state, :]) # Wybierz najlepszy znany ruch\n",
    "        else:\n",
    "            action = env.action_space.sample() # Wybierz losowy ruch\n",
    "            \n",
    "        # 2. Wykonaj ruch\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # 3. Aktualizacja Q-Table (R√≥wnanie Bellmana)\n",
    "        # Q(s,a) = Q(s,a) + lr * [R + gamma * max(Q(s',a')) - Q(s,a)]\n",
    "        q_table[state, action] = q_table[state, action] + learning_rate * (\n",
    "            reward + gamma * np.max(q_table[new_state, :]) - q_table[state, action]\n",
    "        )\n",
    "        \n",
    "        state = new_state\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    # Zmniejszamy Epsilon (coraz mniej losowo≈õci)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "print(\"‚úÖ Trening zako≈Ñczony!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2105ed3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ≈örednia nagroda na 100 gier ---\n",
      "100: 0.00\n",
      "200: 0.00\n",
      "300: 0.00\n",
      "400: 0.00\n",
      "500: 0.00\n",
      "600: 0.00\n",
      "700: 0.00\n",
      "800: 0.00\n",
      "900: 0.00\n",
      "1000: 0.00\n",
      "1100: 0.00\n",
      "1200: 0.00\n",
      "1300: 0.00\n",
      "1400: 0.00\n",
      "1500: 0.00\n",
      "1600: 0.00\n",
      "1700: 0.00\n",
      "1800: 0.00\n",
      "1900: 0.00\n",
      "2000: 0.00\n",
      "\n",
      "--- NAUCZONA Q-TABLE ---\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "(Widzisz liczby inne ni≈º zero? To ≈õcie≈ºka do nagrody!)\n"
     ]
    }
   ],
   "source": [
    "# Sprawd≈∫my wyniki\n",
    "# Dzielimy trening na paczki po 100 gier i liczymy ≈õredniƒÖ nagrodƒô\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/100)\n",
    "count = 100\n",
    "\n",
    "print(\"--- ≈örednia nagroda na 100 gier ---\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(f\"{count}: {sum(r)/100 :.2f}\")\n",
    "    count += 100\n",
    "\n",
    "print(\"\\n--- NAUCZONA Q-TABLE ---\")\n",
    "print(q_table)\n",
    "print(\"(Widzisz liczby inne ni≈º zero? To ≈õcie≈ºka do nagrody!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d53841",
   "metadata": {},
   "source": [
    "## Gra w Czasie Rzeczywistym\n",
    "\n",
    "Teraz, gdy agent ma wype≈ÇnionƒÖ tabelƒô, pu≈õcimy go na planszƒô bez losowo≈õci (`epsilon=0`).\n",
    "Powinien i≈õƒá prosto do celu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec1b096f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start gry!\n",
      "Liczba krok√≥w: 20\n"
     ]
    }
   ],
   "source": [
    "# Funkcja do wizualizacji gry\n",
    "def play_game(env, q_table):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    print(\"Start gry!\")\n",
    "    path = []\n",
    "    \n",
    "    for step in range(20):\n",
    "        # Zawsze wybieramy najlepszy ruch (argmax)\n",
    "        action = np.argmax(q_table[state, :])\n",
    "        \n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        path.append(state)\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            if reward == 1:\n",
    "                print(\"üèÜ SUKCES! Agent dotar≈Ç do prezentu.\")\n",
    "            else:\n",
    "                print(\"‚ò†Ô∏è PORA≈ªKA! Agent wpad≈Ç do dziury.\")\n",
    "            break\n",
    "            \n",
    "    print(f\"Liczba krok√≥w: {step+1}\")\n",
    "\n",
    "# Zagrajmy!\n",
    "play_game(env, q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b4864",
   "metadata": {},
   "source": [
    "## üß† Podsumowanie: Gdzie jest haczyk?\n",
    "\n",
    "To zadzia≈Ça≈Ço ≈õwietnie, bo plansza ma 16 p√≥l.\n",
    "Tabela Q ma rozmiar $16 \\times 4 = 64$ kom√≥rki. To ma≈Ço.\n",
    "\n",
    "**Problem:**\n",
    "Wyobra≈∫ sobie grƒô w szachy albo StarCrafta. Liczba stan√≥w jest wiƒôksza ni≈º liczba atom√≥w we wszech≈õwiecie.\n",
    "Nie da siƒô stworzyƒá Q-Tabeli dla szach√≥w. Zjad≈Çaby ca≈ÇƒÖ pamiƒôƒá RAM ≈õwiata.\n",
    "\n",
    "**RozwiƒÖzanie:**\n",
    "Zamiast Tabeli (Excela), u≈ºyjemy **Sieci Neuronowej** (Funkcji), kt√≥ra bƒôdzie *zgadywaƒá* warto≈õci Q dla ka≈ºdego stanu.\n",
    "To siƒô nazywa **Deep Q-Network (DQN)** i tym zajmiemy siƒô w nastƒôpnym notatniku."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
