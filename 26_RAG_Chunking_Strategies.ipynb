{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d36c214d",
   "metadata": {},
   "source": [
    "# 锔 RAG Chunking: Sztuka krojenia tekstu\n",
    "\n",
    "Kiedy budujesz system RAG, nie wrzucasz caej ksi偶ki do bazy wektorowej. Dzielisz j na fragmenty.\n",
    "\n",
    "**Dlaczego to trudne?**\n",
    "Wyobra藕 sobie zdanie: *\"Kluczem do sejfu jest... [CICIE] ...1234.\"*\n",
    "*   Chunk 1: \"Kluczem do sejfu jest...\" (AI nie wie co).\n",
    "*   Chunk 2: \"...1234.\" (AI nie wie, do czego to kod).\n",
    "\n",
    "Oba kawaki staj si bezu偶yteczne.\n",
    "\n",
    "**Strategie:**\n",
    "1.  **Fixed Size:** Tniemy r贸wno co 100 znak贸w (Ryzykowne).\n",
    "2.  **Sliding Window (Overlap):** Tniemy co 100 znak贸w, ale cofamy si o 20. (Tworzymy zakadk).\n",
    "3.  **Recursive Split:** Najpierw tniemy akapity. Jak za du偶e -> tniemy zdania. Jak za du偶e -> tniemy sowa. (Standard w LangChain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ef104e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dugo tekstu: 629 znak贸w.\n"
     ]
    }
   ],
   "source": [
    "# 1. PRZYGOTOWANIE DANYCH\n",
    "# Dugi tekst (symulacja artykuu)\n",
    "\n",
    "long_text = \"\"\"\n",
    "ROZDZIA 1: WSTP DO AI\n",
    "Sztuczna inteligencja zmienia wiat w tempie wykadniczym. Modele jzykowe, takie jak GPT-4, potrafi pisa kod, wiersze i analizowa prawo.\n",
    "Jednak偶e, maj one swoje ograniczenia, zwane halucynacjami.\n",
    "\n",
    "ROZDZIA 2: PROBLEM PAMICI\n",
    "Modele nie maj pamici dugotrwaej. Kiedy zamykasz okno czatu, model zapomina, o czym rozmawialicie.\n",
    "Dlatego stosujemy techniki takie jak RAG (Retrieval Augmented Generation), aby dostarczy im kontekst.\n",
    "\n",
    "ROZDZIA 3: PRZYSZO\n",
    "W przyszoci agenci AI bd dziaa autonomicznie. Bd rezerwowa loty i robi zakupy.\n",
    "Wa偶ne jest jednak bezpieczestwo i etyka tych system贸w.\n",
    "\"\"\"\n",
    "\n",
    "# Usuwamy zbdne puste linie dla czytelnoci\n",
    "long_text = long_text.strip()\n",
    "print(f\"Dugo tekstu: {len(long_text)} znak贸w.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f562d",
   "metadata": {},
   "source": [
    "## Metoda 1: Naive Chunking (Sztywne cicie)\n",
    "\n",
    "Tniemy tekst r贸wno co `chunk_size` znak贸w.\n",
    "To najprostsza metoda, ale niszczy sens, jeli cicie wypadnie w rodku wa偶nego sowa lub zdania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d648eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METODA NAIWNA ---\n",
      "CHUNK 0: [ROZDZIA 1: WSTP DO AI\n",
      "Sztuczna inteligencja zmienia wiat w tempie wykadniczym. Modele jzykowe, takie jak GPT-4, pot]\n",
      "CHUNK 1: [rafi pisa kod, wiersze i analizowa prawo.\n",
      "Jednak偶e, maj one swoje ograniczenia, zwane halucynacjami.\n",
      "\n",
      "ROZDZIA 2: PR]\n",
      "CHUNK 2: [OBLEM PAMICI\n",
      "Modele nie maj pamici dugotrwaej. Kiedy zamykasz okno czatu, model zapomina, o czym rozmawialicie.\n",
      "Dl]\n",
      "CHUNK 3: [atego stosujemy techniki takie jak RAG (Retrieval Augmented Generation), aby dostarczy im kontekst.\n",
      "\n",
      "ROZDZIA 3: PRZYSZ]\n",
      "CHUNK 4: [O\n",
      "W przyszoci agenci AI bd dziaa autonomicznie. Bd rezerwowa loty i robi zakupy.\n",
      "Wa偶ne jest jednak bezpiecz]\n",
      "CHUNK 5: [estwo i etyka tych system贸w.]\n",
      "\n",
      "ZAUWA呕 PROBLEM:\n",
      "Chunk 0 koczy si w poowie zdania. Chunk 1 zaczyna si od rodka.\n",
      "Jeli zapytasz AI o 'halucynacje' (koniec Chunk 0), mo偶e nie zrozumie kontekstu z pocztku Chunk 1.\n"
     ]
    }
   ],
   "source": [
    "def naive_chunking(text, chunk_size=100):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunk = text[i : i + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# Testujemy\n",
    "chunks_naive = naive_chunking(long_text, chunk_size=120)\n",
    "\n",
    "print(\"--- METODA NAIWNA ---\")\n",
    "for i, chunk in enumerate(chunks_naive):\n",
    "    print(f\"CHUNK {i}: [{chunk}]\")\n",
    "\n",
    "print(\"\\nZAUWA呕 PROBLEM:\")\n",
    "print(\"Chunk 0 koczy si w poowie zdania. Chunk 1 zaczyna si od rodka.\")\n",
    "print(\"Jeli zapytasz AI o 'halucynacje' (koniec Chunk 0), mo偶e nie zrozumie kontekstu z pocztku Chunk 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bab15b",
   "metadata": {},
   "source": [
    "## Metoda 2: Overlap (Zakadka)\n",
    "\n",
    "Naprawiamy problem.\n",
    "Tniemy kawaek o dugoci 120, ale nastpny zaczynamy nie od 121, tylko cofamy si o 20 znak贸w (Overlap).\n",
    "Dziki temu informacje z granicy cicia pojawiaj si w **obu** kawakach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e147df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METODA Z ZAKADK (OVERLAP) ---\n",
      "CHUNK 0: [ROZDZIA 1: WSTP DO AI\n",
      "Sztuczna inteligencja zmienia wiat w tempie wykadniczym. Modele jzykowe, takie jak GPT-4, pot]\n",
      "CHUNK 1: [jzykowe, takie jak GPT-4, potrafi pisa kod, wiersze i analizowa prawo.\n",
      "Jednak偶e, maj one swoje ograniczenia, zwane ]\n",
      "CHUNK 2: [one swoje ograniczenia, zwane halucynacjami.\n",
      "\n",
      "ROZDZIA 2: PROBLEM PAMICI\n",
      "Modele nie maj pamici dugotrwaej. Kiedy za]\n",
      "CHUNK 3: [pamici dugotrwaej. Kiedy zamykasz okno czatu, model zapomina, o czym rozmawialicie.\n",
      "Dlatego stosujemy techniki takie]\n",
      "CHUNK 4: [atego stosujemy techniki takie jak RAG (Retrieval Augmented Generation), aby dostarczy im kontekst.\n",
      "\n",
      "ROZDZIA 3: PRZYSZ]\n",
      "CHUNK 5: [ kontekst.\n",
      "\n",
      "ROZDZIA 3: PRZYSZO\n",
      "W przyszoci agenci AI bd dziaa autonomicznie. Bd rezerwowa loty i robi zaku]\n",
      "CHUNK 6: [ rezerwowa loty i robi zakupy.\n",
      "Wa偶ne jest jednak bezpieczestwo i etyka tych system贸w.]\n",
      "\n",
      "ZALETA:\n",
      "Sp贸jrz na koniec Chunk 0 i pocztek Chunk 1.\n",
      "Zdanie o halucynacjach jest powt贸rzone. Dziki temu nie tracimy wtku!\n"
     ]
    }
   ],
   "source": [
    "def overlapping_chunking(text, chunk_size=120, overlap=30):\n",
    "    chunks = []\n",
    "    step = chunk_size - overlap  # O tyle si przesuwamy\n",
    "    \n",
    "    for i in range(0, len(text), step):\n",
    "        chunk = text[i : i + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "        # Jeli doszlimy do koca, przerywamy\n",
    "        if i + chunk_size >= len(text):\n",
    "            break\n",
    "            \n",
    "    return chunks\n",
    "\n",
    "chunks_overlap = overlapping_chunking(long_text, chunk_size=120, overlap=30)\n",
    "\n",
    "print(\"--- METODA Z ZAKADK (OVERLAP) ---\")\n",
    "for i, chunk in enumerate(chunks_overlap):\n",
    "    print(f\"CHUNK {i}: [{chunk}]\")\n",
    "    \n",
    "print(\"\\nZALETA:\")\n",
    "print(\"Sp贸jrz na koniec Chunk 0 i pocztek Chunk 1.\")\n",
    "print(\"Zdanie o halucynacjach jest powt贸rzone. Dziki temu nie tracimy wtku!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0028aabb",
   "metadata": {},
   "source": [
    "## Metoda 3: Recursive Splitting (Metoda Profesjonalna)\n",
    "\n",
    "Tak dziaaj biblioteki typu **LangChain** czy **LlamaIndex**.\n",
    "Algorytm nie tnie na lepo po znakach.\n",
    "Pr贸buje ci w \"logicznych\" miejscach:\n",
    "1.  Najpierw szuka podw贸jnych enter贸w `\\n\\n` (Akapity).\n",
    "2.  Jak akapit jest wci偶 za du偶y, szuka pojedynczych enter贸w `\\n` (Linie).\n",
    "3.  Jak linia jest za du偶a, szuka spacji ` ` (Sowa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9dc5699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METODA REKURENCYJNA (Logiczna) ---\n",
      "CHUNK 0:\n",
      "ROZDZIA 1: WSTP DO AI\n",
      "Sztuczna inteligencja zmienia wiat w tempie wykadniczym. Modele jzykowe, takie jak GPT-4, potrafi pisa kod, wiersze i analizowa prawo.\n",
      "Jednak偶e, maj one swoje ograniczenia, zwane halucynacjami.\n",
      "--------------------\n",
      "CHUNK 1:\n",
      "ROZDZIA 2: PROBLEM PAMICI\n",
      "Modele nie maj pamici dugotrwaej. Kiedy zamykasz okno czatu, model zapomina, o czym rozmawialicie.\n",
      "Dlatego stosujemy techniki takie jak RAG (Retrieval Augmented Generation), aby dostarczy im kontekst.\n",
      "--------------------\n",
      "CHUNK 2:\n",
      "ROZDZIA 3: PRZYSZO\n",
      "W przyszoci agenci AI bd dziaa autonomicznie. Bd rezerwowa loty i robi zakupy.\n",
      "Wa偶ne jest jednak bezpieczestwo i etyka tych system贸w.\n",
      "--------------------\n",
      "WYNIK:\n",
      "Ka偶dy chunk to peny, logiczny rozdzia. Nie ucilimy 偶adnego zdania w poowie!\n"
     ]
    }
   ],
   "source": [
    "def recursive_split(text, max_chunk_size=120):\n",
    "    chunks = []\n",
    "    \n",
    "    # KROK 1: Podziel na akapity (Logiczne caoci)\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    \n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        # Sprawdzamy, czy jak dodamy ten akapit, to przekroczymy limit?\n",
    "        if len(current_chunk) + len(para) <= max_chunk_size:\n",
    "            # Mieci si -> Doklejamy\n",
    "            current_chunk += para + \"\\n\\n\"\n",
    "        else:\n",
    "            # Nie mieci si -> Zapisujemy stary chunk i zaczynamy nowy\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = para + \"\\n\\n\"\n",
    "            \n",
    "            # (Tutaj w prawdziwym LangChain nastpioby dalsze cicie akapitu na zdania,\n",
    "            #  jeli sam akapit byby wikszy ni偶 max_chunk_size. Dla uproszczenia to pomijamy).\n",
    "    \n",
    "    # Dodajemy ostatni kawaek\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "chunks_recursive = recursive_split(long_text, max_chunk_size=150)\n",
    "\n",
    "print(\"--- METODA REKURENCYJNA (Logiczna) ---\")\n",
    "for i, chunk in enumerate(chunks_recursive):\n",
    "    print(f\"CHUNK {i}:\\n{chunk}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "print(\"WYNIK:\")\n",
    "print(\"Ka偶dy chunk to peny, logiczny rozdzia. Nie ucilimy 偶adnego zdania w poowie!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efd2583",
   "metadata": {},
   "source": [
    "##  Podsumowanie: Jak nie zepsu RAG-a?\n",
    "\n",
    "Wyb贸r strategii zale偶y od danych:\n",
    "\n",
    "1.  **Kod 藕r贸dowy (Python, JS):** Tniemy w oparciu o klasy i funkcje. (Overlap mao wa偶ny).\n",
    "2.  **Dokumenty prawne/umowy:** Bardzo du偶y **Overlap** (nawet 50%), bo kontekst jednego paragrafu mo偶e zale偶e od poprzedniego zdania.\n",
    "3.  **Wiki/Artykuy:** Recursive Split (zachowanie akapit贸w) jest najlepszy.\n",
    "\n",
    "**Tu jest haczyk (Embedding Model).**\n",
    "Pamitaj, 偶e ka偶dy model Embeddings ma limit (np. 512 lub 8192 token贸w). Tw贸j Chunk **musi** by mniejszy ni偶 ten limit. Inaczej model utnie koc贸wk i nawet o tym nie bdziesz wiedzia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
