{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95e262d8",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/takzen/ai-engineering-handbook/blob/main/70_Vision_Transformer_ViT.ipynb\" target=\"_parent\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aad70b",
   "metadata": {},
   "source": [
    "# üëÅÔ∏è Vision Transformer (ViT): Obraz jest wart 16x16 s≈Ç√≥w\n",
    "\n",
    "CNN ma tzw. **Inductive Bias** (zak≈Çada z g√≥ry, ≈ºe lokalno≈õƒá jest wa≈ºna).\n",
    "Transformer nie zak≈Çada niczego. Uczy siƒô relacji miƒôdzy pikselem w lewym g√≥rnym rogu, a pikselem w prawym dolnym rogu od razu (Global Attention).\n",
    "\n",
    "**Algorytm ViT:**\n",
    "1.  **Patching:** Podziel obrazek na kwadraty (np. 16x16 pikseli).\n",
    "2.  **Flatten:** Sp≈Çaszcz ka≈ºdy kwadrat do wektora. To sƒÖ nasze \"s≈Çowa\".\n",
    "3.  **Position Embedding:** Dodaj informacjƒô, gdzie ten kwadrat by≈Ç na obrazku (u≈ºyjemy trenowalnych parametr√≥w, a nie sinusa).\n",
    "4.  **CLS Token:** Dodaj jeden specjalny, pusty wektor na poczƒÖtku. Po przej≈õciu przez sieƒá, to on bƒôdzie zawiera≈Ç informacjƒô \"Co jest na obrazku?\".\n",
    "5.  **Transformer Encoder:** Standardowe bloki (Attention + MLP).\n",
    "\n",
    "U≈ºyjemy zbioru CIFAR-10 (obrazki 32x32). Podzielimy je na Patche 4x4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf7d7b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obrazek: 32x32\n",
      "Patch: 4x4\n",
      "Liczba Patchy: 64 (To bƒôdzie d≈Çugo≈õƒá naszego 'zdania')\n",
      "Wymiar jednego Patcha (sp≈Çaszczony): 48\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Konfiguracja\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "IMG_SIZE = 32\n",
    "PATCH_SIZE = 4\n",
    "EMBED_DIM = 128  # Rozmiar wektora, w jaki zamienimy patch\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 4\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Sprawd≈∫my matematykƒô\n",
    "num_patches = (IMG_SIZE // PATCH_SIZE) ** 2\n",
    "input_dim = 3 * PATCH_SIZE * PATCH_SIZE # 3 kana≈Çy RGB * 4 * 4 piksele\n",
    "\n",
    "print(f\"Obrazek: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"Patch: {PATCH_SIZE}x{PATCH_SIZE}\")\n",
    "print(f\"Liczba Patchy: {num_patches} (To bƒôdzie d≈Çugo≈õƒá naszego 'zdania')\")\n",
    "print(f\"Wymiar jednego Patcha (sp≈Çaszczony): {input_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adabdcd3",
   "metadata": {},
   "source": [
    "## Krok 1: Patch Embedding Layer\n",
    "\n",
    "Musimy zamieniƒá obrazek (3D) na sekwencjƒô wektor√≥w (2D).\n",
    "Mogliby≈õmy u≈ºyƒá pƒôtli i ciƒÖƒá obrazek, ale jest sprytniejszy spos√≥b.\n",
    "\n",
    "**In≈ºynierski Trik:**\n",
    "U≈ºycie warstwy `Conv2d` z rozmiarem kernela r√≥wnym `patch_size` i krokiem (`stride`) r√≥wnym `patch_size` robi dok≈Çadnie to samo! Tnie obrazek na kawa≈Çki i od razu rzutuje je na wymiar `EMBED_DIM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12845878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wej≈õcie: torch.Size([1, 3, 32, 32])\n",
      "Wyj≈õcie (Sekwencja): torch.Size([1, 64, 128]) -> [Batch, Liczba Patchy, Wymiar]\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=3, patch_size=4, emb_size=128, img_size=32):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        # Trik: Conv2d jako tokenizer\n",
    "        self.projection = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [Batch, C, H, W] -> [Batch, Emb_Size, H/P, W/P]\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        # Sp≈Çaszczamy: [Batch, Emb_Size, Num_Patches]\n",
    "        x = x.flatten(2)\n",
    "        \n",
    "        # Transpozycja: [Batch, Num_Patches, Emb_Size] (Tak lubiƒÖ Transformery)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "# Test\n",
    "dummy_img = torch.randn(1, 3, 32, 32)\n",
    "pe = PatchEmbedding(patch_size=PATCH_SIZE, emb_size=EMBED_DIM)\n",
    "out = pe(dummy_img)\n",
    "print(f\"Wej≈õcie: {dummy_img.shape}\")\n",
    "print(f\"Wyj≈õcie (Sekwencja): {out.shape} -> [Batch, Liczba Patchy, Wymiar]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4966a18c",
   "metadata": {},
   "source": [
    "## Krok 2: The ViT (Sk≈Çadamy ca≈Ço≈õƒá)\n",
    "\n",
    "Tutaj dzieje siƒô magia.\n",
    "1.  Tworzymy **CLS Token** (learnable parameter). Doklejamy go na poczƒÖtek sekwencji.\n",
    "2.  Tworzymy **Position Embeddings** (te≈º learnable). Dodajemy je do sekwencji.\n",
    "3.  Przepuszczamy przez **Transformer Encoder** (PyTorch ma gotowy modu≈Ç `nn.TransformerEncoder`, ale to jest dok≈Çadnie to, co budowali≈õmy w notatniku 46).\n",
    "4.  Na ko≈Ñcu bierzemy TYLKO pierwszy token (CLS) i na jego podstawie klasyfikujemy obraz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae206631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT gotowy. Liczba parametr√≥w: 2,388,362\n"
     ]
    }
   ],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads, n_layers, num_classes, patch_size, img_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Tokenizacja (Patching)\n",
    "        self.patch_embed = PatchEmbedding(patch_size=patch_size, emb_size=embed_dim, img_size=img_size)\n",
    "        \n",
    "        # Liczba patchy\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # 2. Token CLS (Klasyfikacyjny) - parametryzowany wektor\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        \n",
    "        # 3. Pozycje (Learnable) - dla wszystkich patchy + 1 (CLS)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 1 + self.num_patches, embed_dim))\n",
    "        \n",
    "        # 4. Transformer Encoder (Stack blok√≥w)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=n_heads, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # 5. G≈Çowica klasyfikujƒÖca (MLP Head)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, 3, 32, 32]\n",
    "        \n",
    "        # Embeddings\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Doklejamy CLS Token\n",
    "        batch_size = x.shape[0]\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1) # Kopiujemy dla ca≈Çego batcha\n",
    "        x = torch.cat((cls_tokens, x), dim=1) # [Batch, N+1, Dim]\n",
    "        \n",
    "        # Dodajemy pozycje\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Transformer\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Klasyfikacja: Bierzemy tylko token 0 (CLS)\n",
    "        cls_output = x[:, 0]\n",
    "        \n",
    "        return self.mlp_head(cls_output)\n",
    "\n",
    "# Inicjalizacja modelu\n",
    "model = VisionTransformer(\n",
    "    embed_dim=EMBED_DIM,\n",
    "    n_heads=NUM_HEADS,\n",
    "    n_layers=NUM_LAYERS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    img_size=IMG_SIZE\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"ViT gotowy. Liczba parametr√≥w: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14c98b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start treningu ViT...\n",
      "Epoka 0, Batch 0, Loss: 2.4581\n",
      "Epoka 0, Batch 100, Loss: 2.1943\n",
      "Epoka 0, Batch 200, Loss: 1.9984\n",
      "Epoka 0, Batch 300, Loss: 2.1356\n",
      "Epoka 0, Batch 400, Loss: 2.0505\n",
      "Epoka 0, Batch 500, Loss: 2.1221\n",
      "Epoka 0, Batch 600, Loss: 1.7942\n",
      "Epoka 0, Batch 700, Loss: 1.8145\n",
      "Epoka 1, Batch 0, Loss: 1.8641\n",
      "Epoka 1, Batch 100, Loss: 2.0023\n",
      "Epoka 1, Batch 200, Loss: 1.9983\n",
      "Epoka 1, Batch 300, Loss: 1.8797\n",
      "Epoka 1, Batch 400, Loss: 1.9642\n",
      "Epoka 1, Batch 500, Loss: 2.0154\n",
      "Epoka 1, Batch 600, Loss: 1.9771\n",
      "Epoka 1, Batch 700, Loss: 1.8348\n",
      "‚úÖ Trening zako≈Ñczony.\n"
     ]
    }
   ],
   "source": [
    "# POBIERANIE DANYCH (CIFAR-10)\n",
    "# Obrazki: Samolot, Auto, Ptak, Kot...\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# TRENING (Szybki test - 2 epoki)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Start treningu ViT...\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(2): # Tylko 2 epoki, bo Transformer jest wolny na CPU\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoka {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"‚úÖ Trening zako≈Ñczony.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab91906",
   "metadata": {},
   "source": [
    "## üß† Podsumowanie: ViT vs CNN\n",
    "\n",
    "Co zauwa≈ºysz?\n",
    "ViT prawdopodobnie uczy siƒô **wolniej** lub daje gorsze wyniki na ma≈Çym zbiorze (CIFAR-10) ni≈º proste CNN.\n",
    "\n",
    "**Dlaczego?**\n",
    "*   **CNN** ma \"wrodzonƒÖ wiedzƒô\" (Inductive Bias): wie, ≈ºe piksele obok siebie sƒÖ wa≈ºne. To pomaga przy ma≈Çych danych.\n",
    "*   **ViT** musi siƒô wszystkiego nauczyƒá od zera (nawet tego, co to znaczy \"byƒá obok siebie\").\n",
    "\n",
    "**Ale...**\n",
    "Gdy dasz ViT-owi 300 milion√≥w zdjƒôƒá (zbi√≥r JFT-300M), ViT mia≈ºd≈ºy CNN. CNN dochodzi do szklanego sufitu, a ViT skaluje siƒô w niesko≈Ñczono≈õƒá. Dlatego GPT-4V, Gemini i DALL-E u≈ºywajƒÖ Transformer√≥w do obrazu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-handbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
